{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "991209b5-c8db-4fef-87c4-bf128cc83f6a",
   "metadata": {},
   "source": [
    "# CONFIGURACION DELTA SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6168c6b-856f-4138-a631-5a8b09741b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "def ensure_directories_exist(warehouse_dir, metastore_db_path):\n",
    "    \"\"\"\n",
    "    Ensures the necessary directories for the warehouse and metastore exist.\n",
    "\n",
    "    Parameters:\n",
    "        warehouse_dir (str): Path to the warehouse directory (Spark catalog).\n",
    "        metastore_db_path (str): Path to the metastore database.\n",
    "    \"\"\"\n",
    "    os.makedirs(warehouse_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(metastore_db_path), exist_ok=True)\n",
    "\n",
    "def create_spark_session(app_name=\"DeltaCatalog\", warehouse_dir=\"./warehouse-spark/spark_catalog\", \n",
    "                         metastore_db_path=\"./warehouse-spark/metastore_db\"):\n",
    "    \"\"\"\n",
    "    Creates and initializes a SparkSession with Delta Lake support and persistent metastore.\n",
    "\n",
    "    Parameters:\n",
    "        app_name (str): Name of the Spark application.\n",
    "        warehouse_dir (str): Path to the Spark catalog warehouse directory.\n",
    "        metastore_db_path (str): Path to the persistent metastore database (Derby).\n",
    "\n",
    "    Returns:\n",
    "        SparkSession: Configured SparkSession instance.\n",
    "    \"\"\"\n",
    "    # Ensure required directories exist\n",
    "    ensure_directories_exist(warehouse_dir, metastore_db_path)\n",
    "    \n",
    "    # Configure SparkSession with Delta Lake\n",
    "    builder = SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"gzip\") \\\n",
    "        .config(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\") \\\n",
    "        .config(\"spark.databricks.delta.autoCompact.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.warehouse.dir\", os.path.abspath(warehouse_dir)) \\\n",
    "        .config(\"javax.jdo.option.ConnectionURL\", f\"jdbc:derby:{os.path.abspath(metastore_db_path)};create=true\") \\\n",
    "        .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "        .enableHiveSupport()\n",
    "    \n",
    "    # Initialize Spark with Delta\n",
    "    spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "    \n",
    "    print(f\"SparkSession created with Delta and persistent metastore at: {warehouse_dir}\")\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3cb250-b191-4a9d-ad0d-b6c6849cd9f9",
   "metadata": {},
   "source": [
    "# SESION DE SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ced18e24-a9c9-409d-87ad-789a68c37e44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: javax.jdo.option.ConnectionURL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f9258edd-d8b9-4842-b9b1-c275b1a5a874;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.2.1 in central\n",
      "\tfound io.delta#delta-storage;3.2.1 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 157ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.2.1 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.1 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f9258edd-d8b9-4842-b9b1-c275b1a5a874\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/5ms)\n",
      "24/11/27 22:15:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/27 22:15:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created with Delta and persistent metastore at: ./warehouse-spark/spark_catalog\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "app_name = 'Delta Spark'\n",
    "# Ruta para el directorio del metastore\n",
    "warehouse_dir = \"./warehouse-spark/spark_catalog\"\n",
    "metastore_db_path = \"./warehouse-spark/metastore_db\"\n",
    "\n",
    "spark_session = create_spark_session(\n",
    "    app_name=app_name,\n",
    "    warehouse_dir=warehouse_dir,\n",
    "    metastore_db_path=metastore_db_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9262f2-7660-4e17-99d2-da0cb8c03959",
   "metadata": {},
   "source": [
    "# [SPARK USER INTERFACE](http://localhost:4040/)\n",
    "\n",
    "[CLICK HERE](http://localhost:4040/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004afaca-6ec0-4a33-8a94-a635cd252a8e",
   "metadata": {},
   "source": [
    "# CREAR DATABASE AND DELTA TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b6ad86e-679a-4469-992c-8c8eb75944f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database 'delta_spark_database' created or already exists.\n",
      "\n",
      "The following databases are present in the Spark catalog:\n",
      "default\n",
      "delta_spark_database\n",
      "delta_spark_schema\n"
     ]
    }
   ],
   "source": [
    "def create_database_schema(spark_session, database_name):\n",
    "    \"\"\"\n",
    "    Creates a database if it does not already exist.\n",
    "\n",
    "    Parameters:\n",
    "        spark_session (SparkSession): The active SparkSession.\n",
    "        database_name (str): The name of the database to be created.\n",
    "    \"\"\"\n",
    "    spark_session.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "    print(f\"Database '{database_name}' created or already exists.\")\n",
    "\n",
    "def list_databases(spark_session):\n",
    "    \"\"\"\n",
    "    Lists all the databases in the Spark catalog.\n",
    "\n",
    "    Parameters:\n",
    "        spark_session (SparkSession): The active Spark session.\n",
    "    \"\"\"\n",
    "    databases = spark_session.catalog.listDatabases()\n",
    "    print('The following databases are present in the Spark catalog:')\n",
    "\n",
    "    for db in databases:\n",
    "        print(db.name)\n",
    "\n",
    "# Ejemplo de uso\n",
    "database_name = 'delta_spark_database'\n",
    "create_database(spark_session, database_name)\n",
    "\n",
    "print()\n",
    "\n",
    "list_databases(spark_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74adea2-94a6-4105-8bd8-3682f3f440c0",
   "metadata": {},
   "source": [
    "# CREATE SPARK DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3facfb3c-8b76-4a47-98a2-2347e3d72855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+\n",
      "| id|     name|age|\n",
      "+---+---------+---+\n",
      "|  1| Nathalie|  0|\n",
      "|  2|     Cora|  3|\n",
      "|  3|     Gaby| 10|\n",
      "|  4|   Muneca| 42|\n",
      "|  5| Principe| 46|\n",
      "|  6|      Ana| 25|\n",
      "|  7|  Cecilia| 30|\n",
      "|  8|    Lucia| 18|\n",
      "|  9|     Zeus|  5|\n",
      "| 10|Guadalupe| 15|\n",
      "| 11|  Augusto| 28|\n",
      "| 12|  Muiscas| 23|\n",
      "| 13|    Jorge| 31|\n",
      "+---+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_dataframe_from_list_dict_using_alphabetical_order_from_columns(spark_session, list_data_dict):\n",
    "    \"\"\"\n",
    "    Creates a Spark DataFrame from a list of dictionaries, reordering columns in alphabetical order.\n",
    "\n",
    "    Parameters:\n",
    "        spark_session (SparkSession): The active Spark session.\n",
    "        list_data_dict (list): A list of dictionaries, where each dictionary represents a row.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The created Spark DataFrame with columns in alphabetical order.\n",
    "    \"\"\"\n",
    "    if list_data_dict:\n",
    "        df = spark_session.createDataFrame(list_data_dict)\n",
    "        return df\n",
    "    else:\n",
    "        raise ValueError(\"The input list is empty.\")\n",
    "\n",
    "def create_dataframe_from_list_dict(spark_session, list_data_dict):\n",
    "    \"\"\"\n",
    "    Creates a Spark DataFrame from a list of dictionaries, preserving the order of the keys.\n",
    "\n",
    "    Parameters:\n",
    "        spark_session (SparkSession): The active Spark session.\n",
    "        list_data_dict (list): A list of dictionaries, where each dictionary represents a row.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The created Spark DataFrame with columns in the order of the keys.\n",
    "    \"\"\"\n",
    "    if not list_data_dict:\n",
    "        raise ValueError(\"The input list is empty.\")\n",
    "\n",
    "    # Get the order of keys from the first dictionary\n",
    "    columns_order = list(list_data_dict[0].keys())\n",
    "\n",
    "    # Create the DataFrame and reorder columns\n",
    "    df = spark_session.createDataFrame(list_data_dict)\n",
    "    df = df.select(*columns_order)  # Reorder columns explicitly\n",
    "\n",
    "    return df\n",
    "\n",
    "def split_spark_dataframe(spark_dataframe, num_parts):\n",
    "    \"\"\"\n",
    "    Splits a Spark DataFrame into the specified number of parts, ensuring each part has at least one row.\n",
    "    If the requested number of parts exceeds the total rows, it creates as many balanced parts as possible.\n",
    "\n",
    "    Parameters:\n",
    "        spark_dataframe (DataFrame): The Spark DataFrame to be split.\n",
    "        num_parts (int): spark_dataframeThe desired number of parts to split the DataFrame into.\n",
    "\n",
    "    Returns:\n",
    "        List[DataFrame]: A list containing the split DataFrames.\n",
    "    \"\"\"\n",
    "    total_rows = spark_dataframe.count()\n",
    "\n",
    "    if total_rows == 0:\n",
    "        print(\"The DataFrame is empty. No parts created.\")\n",
    "        return []\n",
    "\n",
    "    # Adjust number of parts if more parts are requested than rows\n",
    "    actual_parts = min(num_parts, total_rows)\n",
    "\n",
    "    # Calculate base rows per part and distribute remaining rows\n",
    "    rows_per_part = total_rows // actual_parts\n",
    "    extra_rows = total_rows % actual_parts\n",
    "\n",
    "    split_dataframes = []\n",
    "    start_row = 0\n",
    "\n",
    "    for i in range(actual_parts):\n",
    "        # Calculate rows for the current part\n",
    "        rows_in_this_part = rows_per_part + (1 if i < extra_rows else 0)\n",
    "        end_row = start_row + rows_in_this_part\n",
    "\n",
    "        # Select the rows for the current part\n",
    "        split_dataframes.append(\n",
    "            spark_dataframe.limit(end_row).subtract(spark_dataframe.limit(start_row))\n",
    "        )\n",
    "        start_row = end_row  # Update start row for the next part\n",
    "\n",
    "    print(f\"Successfully created {len(split_dataframes)} DataFrames.\")\n",
    "    return split_dataframes\n",
    "\n",
    "# Example of usage\n",
    "list_data_dict = [\n",
    "    {'id': 1, 'name': 'Nathalie', 'age': 0},\n",
    "    {'id': 2, 'name': 'Cora', 'age': 3},\n",
    "    {'id': 3, 'name': 'Gaby', 'age': 10},\n",
    "    {'id': 4, 'name': 'Muneca', 'age': 42},\n",
    "    {'id': 5, 'name': 'Principe', 'age': 46},\n",
    "    {'id': 6, 'name': 'Ana', 'age': 25},\n",
    "    {'id': 7, 'name': 'Cecilia', 'age': 30},\n",
    "    {'id': 8, 'name': 'Lucia', 'age': 18},\n",
    "    {'id': 9, 'name': 'Zeus', 'age': 5},\n",
    "    {'id': 10, 'name': 'Guadalupe', 'age': 15},\n",
    "    {'id': 11, 'name': 'Augusto', 'age': 28},\n",
    "    {'id': 12, 'name': 'Muiscas', 'age': 23},\n",
    "    {'id': 13, 'name': 'Jorge', 'age': 31}\n",
    "]\n",
    "\n",
    "# crea el datafarme\n",
    "spark_dataframe = create_dataframe_from_list_dict(spark_session, list_data_dict)\n",
    "\n",
    "spark_dataframe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28634048-2ea4-4e25-b011-3653c22365bb",
   "metadata": {},
   "source": [
    "# DIVIDIR DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69c75ed3-4230-4bba-a7b9-31e00f96c5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 2 DataFrames.\n",
      "+---+--------+---+\n",
      "| id|    name|age|\n",
      "+---+--------+---+\n",
      "|  1|Nathalie|  0|\n",
      "|  2|    Cora|  3|\n",
      "|  3|    Gaby| 10|\n",
      "|  4|  Muneca| 42|\n",
      "|  5|Principe| 46|\n",
      "|  6|     Ana| 25|\n",
      "|  7| Cecilia| 30|\n",
      "+---+--------+---+\n",
      "\n",
      "+---+---------+---+\n",
      "| id|     name|age|\n",
      "+---+---------+---+\n",
      "|  8|    Lucia| 18|\n",
      "|  9|     Zeus|  5|\n",
      "| 10|Guadalupe| 15|\n",
      "| 11|  Augusto| 28|\n",
      "| 12|  Muiscas| 23|\n",
      "| 13|    Jorge| 31|\n",
      "+---+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# divide el dataframe\n",
    "num_parts = 2\n",
    "df_sql, df_delta = split_spark_dataframe(spark_dataframe, num_parts)\n",
    "\n",
    "df_sql.show()\n",
    "\n",
    "df_delta.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd09869-9346-4d85-8eba-489041cdb220",
   "metadata": {},
   "source": [
    "# CREAR TABLAS DELTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62fadf89-9aef-437a-a4f7-cff7df2e0adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_delta_table_with_spark_dataframe(spark_session, database_name, table_name, spark_dataframe, warehouse_dir):\n",
    "    \"\"\"\n",
    "    Creates a Delta table in a specified database from a Spark DataFrame if it does not already exist.\n",
    "\n",
    "    Parameters:\n",
    "        spark_session (SparkSession): The active SparkSession.\n",
    "        database_name (str): The name of the database where the table will be created.\n",
    "        table_name (str): The name of the table to be created.\n",
    "        spark_dataframe (DataFrame): The Spark DataFrame from which the schema is derived.\n",
    "        warehouse_dir (str): The root directory for the warehouse.\n",
    "    \"\"\"\n",
    "    # Validate that the database exists\n",
    "    available_databases = [db.name for db in spark_session.catalog.listDatabases()]\n",
    "    if database_name not in available_databases:\n",
    "        raise ValueError(f\"The database '{database_name}' does not exist. Available databases: {available_databases}\")\n",
    "    \n",
    "    # Set the active database\n",
    "    spark_session.sql(f\"USE {database_name}\")\n",
    "    \n",
    "    # Define the table path based on the database and table name\n",
    "    table_path = f\"{warehouse_dir}/{database_name}/{table_name}\"\n",
    "    \n",
    "    # Get the schema of the DataFrame\n",
    "    table_schema = \", \".join([f\"{field.name} {field.dataType.simpleString()}\" for field in spark_dataframe.schema.fields])\n",
    "    \n",
    "    # Create the Delta table\n",
    "    spark_session.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} ({table_schema})\n",
    "        USING DELTA\n",
    "        LOCATION '{table_path}'\n",
    "    \"\"\")\n",
    "    \n",
    "    print(f\"Table '{table_name}' created or already exists at '{table_path}' in database '{database_name}'.\")\n",
    "\n",
    "def save_dataframe_as_parquet(spark_dataframe, file_name, file_path):\n",
    "    \"\"\"\n",
    "    Saves a Spark DataFrame to the specified path in Parquet format.\n",
    "\n",
    "    Parameters:\n",
    "        spark_dataframe (DataFrame): The Spark DataFrame to be saved.\n",
    "        file_name (str): The name of the file (or dataset) to be created.\n",
    "        file_path (str): The location where the file will be stored.\n",
    "    \"\"\"\n",
    "    # Save the Spark DataFrame as a Parquet file at the specified location\n",
    "    spark_dataframe.write.format(\"parquet\").mode(\"overwrite\").save(file_path)\n",
    "    \n",
    "    print(f\"DataFrame saved as Parquet at '{file_path}/{file_name}.parquet'.\")\n",
    "\n",
    "def create_delta_table_in_database(spark_session, database_name, table_name, spark_dataframe, warehouse_dir):\n",
    "    \"\"\"\n",
    "    Creates a Delta table in a specified database from a Spark DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        spark_session (SparkSession): The active SparkSession.\n",
    "        database_name (str): The name of the database where the table will be created.\n",
    "        table_name (str): The name of the table to be created.\n",
    "        spark_dataframe (DataFrame): The Spark DataFrame to be written as a table.\n",
    "        warehouse_dir (str): The root directory for the warehouse.\n",
    "    \"\"\"\n",
    "    # Validate that the database exists\n",
    "    available_databases = [db.name for db in spark_session.catalog.listDatabases()]\n",
    "    if database_name not in available_databases:\n",
    "        raise ValueError(f\"The database '{database_name}' does not exist. Available databases: {available_databases}\")\n",
    "    \n",
    "    # Set the active database\n",
    "    spark_session.sql(f\"USE {database_name}\")\n",
    "    \n",
    "    # Define the full table name and table path\n",
    "    full_table_name = f\"{database_name}.{table_name}\"\n",
    "    table_path = f\"{warehouse_dir}/{database_name}/{table_name}\"\n",
    "    \n",
    "    # Save the DataFrame to the Delta table in the catalog\n",
    "    spark_dataframe.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_table_name)\n",
    "    \n",
    "    print(f\"Table '{table_name}' created or already exists in database '{database_name}' at '{table_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c4e8fb-7197-4437-abbe-bc835667f80c",
   "metadata": {},
   "source": [
    "# GUARDAR DELTA TABLE CON SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c8fec6f-a1e8-4659-bfe8-28470cc04848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/27 22:44:09 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`default`.`delta_sql` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "24/11/27 22:44:09 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "24/11/27 22:44:09 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/11/27 22:44:09 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/11/27 22:44:09 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'delta_sql' created or already exists at './warehouse-spark/spark_catalog/default/delta_sql' in database 'default'.\n"
     ]
    }
   ],
   "source": [
    "database_sql = 'default'\n",
    "table_sql = 'delta_sql'\n",
    "dataframe_sql = df_sql\n",
    "sql_warehouse_dir = warehouse_dir\n",
    "\n",
    "create_delta_table_with_spark_dataframe(spark_session, database_sql, table_sql, dataframe_sql, sql_warehouse_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14797621-b5e3-4285-8c1c-09c604573eee",
   "metadata": {},
   "source": [
    "# GUARDAR DELTA TABLE CON DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bf9c8d5-528e-4479-8ca1-eabd3918135e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                24/11/27 22:47:18 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                24/11/27 22:47:21 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`delta_spark_database`.`delta_dataframe` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'delta_dataframe' created or already exists in database 'delta_spark_database' at './warehouse-spark/spark_catalog/delta_spark_database/delta_dataframe'.\n"
     ]
    }
   ],
   "source": [
    "database_delta = 'delta_spark_database'\n",
    "table_delta = 'delta_dataframe'\n",
    "dataframe_delta = df_delta\n",
    "delta_warehouse_dir = warehouse_dir\n",
    "\n",
    "create_delta_table_in_database(spark_session, database_delta, table_delta, dataframe_delta, delta_warehouse_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce3423e-a585-4575-97d1-849eb51ef433",
   "metadata": {},
   "source": [
    "# GUARDAR DATAFRAME COMO PARQUET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52cd9ef6-5194-487a-86b9-8b2d4e5940e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved as Parquet at './spark_files/spark_dataframe.parquet.parquet'.\n"
     ]
    }
   ],
   "source": [
    "parquet_dataframe = spark_dataframe\n",
    "file_name = 'spark_dataframe.parquet'\n",
    "file_path = './spark_files'\n",
    "\n",
    "save_dataframe_as_parquet(spark_dataframe, file_name, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669d26cd-3d2c-42de-b825-2b81df4af324",
   "metadata": {},
   "source": [
    "# LISTAR LAS TABLAS DE LA BASE DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb2920ee-05c4-485a-a9b6-475e7fb31c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database : default, table : delta_sql\n",
      "Database : delta_spark_database, table : delta_dataframe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['delta_dataframe']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_tables_in_database(spark_session, database_name):\n",
    "    \"\"\"\n",
    "    Lists all tables in the specified database in Spark.\n",
    "\n",
    "    Parameters:\n",
    "        spark_session (SparkSession): The active SparkSession.\n",
    "        database_name (str): The name of the database whose tables will be listed.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of table names in the specified database.\n",
    "    \"\"\"\n",
    "    # Set the current database to the specified database\n",
    "    spark_session.sql(f\"USE {database_name}\")\n",
    "    \n",
    "    # List all tables in the database\n",
    "    tables = spark_session.catalog.listTables(database_name)\n",
    "    \n",
    "    # Extract table names from the list of table objects\n",
    "    table_names = [table.name for table in tables]\n",
    "\n",
    "    for table in table_names:\n",
    "        print(f'Database : {database_name}, table : {table}')\n",
    "    \n",
    "    return table_names\n",
    "\n",
    "list_tables_in_database(spark_session, database_sql)\n",
    "\n",
    "list_tables_in_database(spark_session, database_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b509671c-c384-460f-81de-f3b3ec3b4b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python - ML - Data Science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
