{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8dc8402-9c6d-40cb-8650-72bc1cc599ec",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9d6250-476c-4b3f-982f-e94a60ab5309",
   "metadata": {},
   "source": [
    "# START ZOOKEPER SERVICE\n",
    "### RUN THIS COMMAND IN A TERMINAL\n",
    "```\n",
    "/usr/local/kafka/bin/zookeeper-server-start.sh /usr/local/kafka/config/zookeeper.properties\n",
    "\n",
    "```\n",
    "# START KAFKA BROKERS\n",
    "### RUN THIS COMMAND ON A DIFFERENT TERMINAL FOR EACH LINE\n",
    "```\n",
    "/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server1.properties\n",
    "/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server2.properties\n",
    "/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server3.properties\n",
    "/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server4.properties\n",
    "```\n",
    "# LIST AVALABLE BROKERS\n",
    "```\n",
    "/usr/local/kafka/bin/zookeeper-shell.sh localhost:2181 ls /brokers/ids\n",
    "```\n",
    "\n",
    "# CREATE GROUPS AND TOPICS AVALABLE GROUPS\n",
    "```\n",
    "/usr/local/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --group jorge-cardona-kafka-batch --topic animals-topic-batch\n",
    "/usr/local/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --group jorge-cardona-kafka-streaming --topic animals-topic-streaming\n",
    "```\n",
    "\n",
    "# LIST AVALABLE GROUPS\n",
    "```\n",
    "/usr/local/kafka/bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list\n",
    "```\n",
    "\n",
    "# LISTS TOPICS\n",
    "```\n",
    "/usr/local/kafka/bin/kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "```\n",
    "\n",
    "# LISTS CONSUMERS ASOCIATE TO GROUPS\n",
    "```\n",
    "/usr/local/kafka/bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group jorge-cardona-kafka-batch\n",
    "/usr/local/kafka/bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group jorge-cardona-kafka-streaming\n",
    "```\n",
    "\n",
    "# DELETE CONSUMER GROUP\n",
    "```\n",
    "/usr/local/kafka/bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --delete --group jorge-cardona-kafka-batch\n",
    "/usr/local/kafka/bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --delete --group jorge-cardona-kafka-streaming\n",
    "```\n",
    "\n",
    "# DELETE TOPICS\n",
    "```\n",
    "# all topics at same time\n",
    "/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --delete --topic animals-topic-batch,animals-topic-streaming\n",
    "\n",
    "# or one by one topic\n",
    "/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --delete --topic animals-topic-batch\n",
    "/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --delete --topic animals-topic-streaming\n",
    "```\n",
    "\n",
    "# DELETE TOPICS\n",
    "```\n",
    "# all groups at same time\n",
    "/usr/local/kafka/bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --delete --group jorge-cardona-kafka-batch,jorge-cardona-kafka-streaming\n",
    "\n",
    "\n",
    "/usr/local/kafka/bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --delete --group jorge-cardona-kafka-batch\n",
    "/usr/local/kafka/bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --delete --group jorge-cardona-kafka-streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c1d103-cffb-4485-b0f8-63941bd64e97",
   "metadata": {},
   "source": [
    "# <center> PYTHON LIBRARY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dea906c-f40a-4513-97e2-e674ca6381c0",
   "metadata": {},
   "source": [
    "# EXECUTE COMMANDS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbae095-c157-49ca-b60c-0a452319a60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "\n",
    "def execute_commands(commands):\n",
    "    for command in commands:\n",
    "        print(f\"Executing: {command}\")\n",
    "        try:\n",
    "            subprocess.Popen(command, shell=True)\n",
    "            print()\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing command: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8a1ee5-b8c6-4e52-b4e6-120d6f191164",
   "metadata": {},
   "source": [
    "# START ZOOKEEPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b170d7-92cf-425b-9508-d9055cb96328",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commands = [\n",
    "    \"/usr/local/kafka/bin/zookeeper-server-start.sh /usr/local/kafka/config/zookeeper.properties\"\n",
    "]\n",
    "\n",
    "execute_commands(commands=commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46934a18-8902-4764-a789-36761bbb3871",
   "metadata": {},
   "source": [
    "# START SECOND BROKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cefd55-ad64-45b1-a84e-6591c1950f7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commands = [\n",
    "    \"/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server2.properties\"\n",
    "]\n",
    "\n",
    "execute_commands(commands=commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522cc9a0-0b48-4423-a8d5-5a4289337692",
   "metadata": {},
   "source": [
    "# PRODUCE MESSAGES CLASIC WAY\n",
    "### COPY, PASTE, AND RUN THE FOLLOWING CODE IN ANOTHER NOTEBOOK TO OBSERVE STREAMING READING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ce26a2-ffbf-48e8-ac7e-237e8ddae262",
   "metadata": {},
   "source": [
    "```python\n",
    "from confluent_kafka import Producer\n",
    "import json\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import random\n",
    "\n",
    "def get_dataset():\n",
    "    # Create a sample list with animal data\n",
    "    dataset = [(\"lion\", \"mammal\"), (\"elephant\", \"mammal\"), (\"tiger\", \"feline\"), (\"whale\", \"mammal\"), (\"penguin\", \"bird\"), (\"gorilla\", \"primate\"), (\"leopard\", \"feline\"), (\"crocodile\", \"reptile\"), (\"rhinoceros\", \"mammal\"), \n",
    "            (\"zebra\", \"mammal\"), (\"hippopotamus\", \"mammal\"), (\"eagle\", \"bird\"), (\"orangutan\", \"primate\"), (\"grizzly bear\", \"mammal\"), (\"owl\", \"bird\"), (\"polar bear\", \"mammal\"), (\"python\", \"reptile\"), (\"hawk\", \"bird\"), \n",
    "            (\"wolf\", \"mammal\"), (\"tortoise\", \"reptile\"), (\"swan\", \"bird\"), (\"cheetah\", \"feline\"), (\"seagull\", \"bird\"), (\"giraffe\", \"mammal\"), (\"deer\", \"mammal\"), (\"giraffe\", \"mammal\"), (\"lizard\", \"reptile\"), (\"flamingo\", \"bird\"),\n",
    "            (\"chimp\", \"primate\"), (\"buffalo\", \"mammal\"), (\"vulture\", \"bird\"), (\"bear\", \"mammal\"), (\"anaconda\", \"reptile\"), (\"pigeon\", \"bird\"), (\"coyote\", \"mammal\"), (\"chameleon\", \"reptile\"), (\"ostrich\", \"bird\"), (\"jaguar\", \"feline\"), \n",
    "            (\"owl\", \"bird\"), (\"beetle\", \"insect\"), (\"snail\", \"invertebrate\"), (\"octopus\", \"cephalopod\"), (\"lobster\", \"crustacean\"), (\"koala\", \"marsupial\"), (\"crane\", \"bird\"), (\"iguana\", \"reptile\"), (\"lemur\", \"primate\"), (\"sloth\", \"mammal\"), \n",
    "            (\"gazelle\", \"mammal\"), (\"wombat\", \"marsupial\"), (\"hummingbird\", \"bird\"), (\"porcupine\", \"mammal\"), (\"macaw\", \"bird\"), (\"hyena\", \"mammal\"), (\"dolphin\", \"mammal\"), (\"seahorse\", \"fish\"), (\"orca\", \"mammal\"), (\"kangaroo\", \"marsupial\"), \n",
    "            (\"shark\", \"fish\"), (\"beaver\", \"mammal\"), (\"platypus\", \"mammal\"), (\"armadillo\", \"mammal\"), (\"rabbit\", \"mammal\"), (\"camel\", \"mammal\"), (\"squirrel\", \"mammal\"), (\"peacock\", \"bird\"), (\"crow\", \"bird\"), (\"frog\", \"amphibian\"), \n",
    "            (\"toad\", \"amphibian\"), (\"newt\", \"amphibian\"), (\"axolotl\", \"amphibian\"), (\"butterfly\", \"insect\"), (\"dragonfly\", \"insect\"), (\"grasshopper\", \"insect\"), (\"mantis\", \"insect\"), (\"beetle\", \"insect\"), (\"ant\", \"insect\"), (\"termite\", \"insect\"),\n",
    "            (\"spider\", \"arachnid\"), (\"scorpion\", \"arachnid\"), (\"tick\", \"arachnid\"), (\"bee\", \"insect\"), (\"wasp\", \"insect\"), (\"hornet\", \"insect\"), (\"fly\", \"insect\"), (\"mosquito\", \"insect\"), (\"cockroach\", \"insect\"), (\"ladybug\", \"insect\"), \n",
    "            (\"firefly\", \"insect\"), (\"millipede\", \"arthropod\"), (\"centipede\", \"arthropod\"), (\"crab\", \"crustacean\"), (\"shrimp\", \"crustacean\"), (\"barnacle\", \"crustacean\"), (\"clam\", \"mollusk\"), (\"oyster\", \"mollusk\"), (\"mussel\", \"mollusk\"), \n",
    "            (\"snail\", \"mollusk\"), (\"slug\", \"mollusk\"), (\"squid\", \"cephalopod\"), (\"cuttlefish\", \"cephalopod\"), (\"nautilus\", \"cephalopod\"), (\"jellyfish\", \"cnidarian\"), (\"coral\", \"cnidarian\"), (\"hydra\", \"cnidarian\"), (\"anemone\", \"cnidarian\"), \n",
    "            (\"sponge\", \"porifera\"), (\"sea cucumber\", \"echinoderm\"), (\"starfish\", \"echinoderm\"), (\"sand dollar\", \"echinoderm\"), (\"sea urchin\", \"echinoderm\"), (\"brittle star\", \"echinoderm\"), (\"sea star\", \"echinoderm\"), (\"sea lily\", \"echinoderm\"), \n",
    "            (\"feather star\", \"echinoderm\"), (\"black widow\", \"arachnid\"), (\"brown recluse\", \"arachnid\"), (\"tarantula\", \"arachnid\"), (\"daddy longlegs\", \"arachnid\"), (\"wolf spider\", \"arachnid\"), (\"jumping spider\", \"arachnid\"), \n",
    "            (\"huntsman spider\", \"arachnid\"), (\"tarantula hawk\", \"insect\"), (\"assassin bug\", \"insect\"), (\"lacewing\", \"insect\"), (\"stink bug\", \"insect\"), (\"cicada\", \"insect\"), (\"walking stick\", \"insect\"), (\"scorpionfly\", \"insect\"), \n",
    "            (\"flower mantis\", \"insect\"), (\"praying mantis\", \"insect\"), (\"earwig\", \"insect\"), (\"flea\", \"insect\"), (\"leaf insect\", \"insect\"), (\"planthopper\", \"insect\"), (\"scale insect\", \"insect\"), (\"aphid\", \"insect\"), (\"mealybug\", \"insect\"), \n",
    "            (\"thrips\", \"insect\"), (\"whitefly\", \"insect\"), (\"beetle\", \"insect\"), (\"antlion\", \"insect\"), (\"snakefly\", \"insect\"), (\"dobsonfly\", \"insect\"), (\"webspinner\", \"insect\"), (\"mayfly\", \"insect\"), (\"stonefly\", \"insect\"), \n",
    "            (\"silverfish\", \"insect\"), (\"firebrat\", \"insect\"), (\"bristletail\", \"insect\"), (\"thysanuran\", \"insect\"), (\"dragonfly\", \"insect\"), (\"damselfly\", \"insect\"), (\"bluet\", \"insect\"), (\"darner\", \"insect\"), (\"adder\", \"insect\"), \n",
    "            (\"basker\", \"insect\"), (\"biter\", \"insect\"), (\"blister beetle\", \"insect\"), (\"bomber\", \"insect\"), (\"bristle beetle\", \"insect\"), (\"burrower\", \"insect\"), (\"carrier\", \"insect\"), (\"caterpillar\", \"insect\"), (\"chafers\", \"insect\"), \n",
    "            (\"chewer\", \"insect\"), (\"click beetle\", \"insect\"), (\"cobblers\", \"insect\"), (\"cobweb spider\", \"arachnid\"), (\"cockroaches\", \"insect\"), (\"coil worm\", \"insect\"), (\"creeper\", \"insect\"), (\"cuckoo wasp\", \"insect\"), (\"cutworm\", \"insect\"), \n",
    "            (\"digger\", \"insect\"), (\"dor beetle\", \"insect\"), (\"earthworms\", \"insect\"), (\"eggfly\", \"insect\"), (\"elaters\", \"insect\"), (\"emperor\", \"insect\"), (\"gadfly\", \"insect\"), (\"gnat\", \"insect\"), (\"grasshopper\", \"insect\"), (\"grazer\", \"insect\"), \n",
    "            (\"ground beetle\", \"insect\"), (\"harvester\", \"insect\"), (\"hornet\", \"insect\"), (\"hornworm\", \"insect\"), (\"humblebee\", \"insect\"), (\"humpbacked fly\", \"insect\"), (\"hoverfly\", \"insect\"), (\"hunter\", \"insect\"), (\"jumper\", \"insect\"), \n",
    "            (\"katydid\", \"insect\"), (\"lacewing\", \"insect\"), (\"leafcutter\", \"insect\"), (\"leafhopper\", \"insect\"), (\"leafroller\", \"insect\"), (\"louse\", \"insect\"), (\"maggot\", \"insect\"), (\"mantisfly\", \"insect\"), (\"marsh fly\", \"insect\"), \n",
    "            (\"marsh beetle\", \"insect\"), (\"mason wasp\", \"insect\"), (\"mealybug\", \"insect\"), (\"miner\", \"insect\"), (\"mite\", \"insect\"), (\"mole cricket\", \"insect\"), (\"moth\", \"insect\"), (\"nemesis\", \"insect\"), (\"net-winged insect\", \"insect\"), \n",
    "            (\"nightcrawler\", \"insect\"), (\"nit\", \"insect\"), (\"nymph\", \"insect\"), (\"odorous ant\", \"insect\"), (\"oracle\", \"insect\"), (\"orb weaver\", \"arachnid\"), (\"orcus\", \"insect\"), (\"ostracod\", \"insect\"), (\"outlaw\", \"insect\"), \n",
    "            (\"peacock butterfly\", \"insect\"), (\"pharaoh ant\", \"insect\"), (\"pillbug\", \"insect\"), (\"plankton\", \"insect\"), (\"pollinator\", \"insect\"), (\"potter wasp\", \"insect\"), (\"praying mantis\", \"insect\"), (\"predator\", \"insect\"),\n",
    "            (\"proboscis\", \"insect\"), (\"prophet\", \"insect\"), (\"pruner\", \"insect\"), (\"pseudoscorpion\", \"arachnid\"), (\"psycho\", \"insect\"), (\"psycho fly\", \"insect\"), (\"psychodid\", \"insect\"), (\"pupa\", \"insect\"), (\"purple martin\", \"insect\"), \n",
    "            (\"putter\", \"insect\"), (\"ranger\", \"insect\"), (\"recluse\", \"insect\"), (\"reducer\", \"insect\"), (\"repeater\", \"insect\"), (\"riffle bug\", \"insect\"), (\"robber fly\", \"insect\"), (\"rootworm\", \"insect\"), (\"rover\", \"insect\"), (\"saber wasp\", \"insect\"),\n",
    "            (\"sawfly\", \"insect\"), (\"scarab\", \"insect\"), (\"scorpionfly\", \"insect\"), (\"scourge\", \"insect\"), (\"scout\", \"insect\"), (\"scuttle fly\", \"insect\"), (\"silk spinner\", \"insect\"), (\"silverfish\", \"insect\"), (\"skipper butterfly\", \"insect\"), \n",
    "            (\"snout butterfly\", \"insect\"), (\"snout beetle\", \"insect\"), (\"snout moth\", \"insect\"), (\"sow bug\", \"insect\"),(\"spider mite\", \"insect\"), (\"spider wasp\", \"insect\"), (\"sphinx moth\", \"insect\"), (\"spider\", \"arachnid\"), (\"spinner\", \"insect\"),\n",
    "            (\"spittlebug\", \"insect\"), (\"spook\", \"insect\"), (\"springtail\", \"insect\"), (\"stag beetle\", \"insect\"), (\"stealer\", \"insect\"), (\"stinger\", \"insect\"), (\"stink bug\", \"insect\"), (\"stinging ant\", \"insect\"), (\"stonefly\", \"insect\"), \n",
    "            (\"strangler\", \"insect\"), (\"sucking louse\", \"insect\"), (\"sweat bee\", \"insect\"), (\"tailor\", \"insect\"), (\"tanglefoot\", \"insect\"), (\"tarantula\", \"arachnid\"), (\"tarantula hawk\", \"insect\"), (\"tick\", \"arachnid\"), (\"tiger beetle\", \"insect\"), \n",
    "            (\"tiger moth\", \"insect\"), (\"tiphiid wasp\", \"insect\"), (\"titan beetle\", \"insect\"), (\"toad bug\", \"insect\"), (\"torchbearer\", \"insect\"), (\"torpedo bug\", \"insect\"), (\"tortoise beetle\", \"insect\"), (\"trapper\", \"insect\"), \n",
    "            (\"tree cricket\", \"insect\"), (\"trilobite beetle\", \"insect\"), (\"trogonoptera\", \"insect\"), (\"twig borer\", \"insect\"), (\"vampire\", \"insect\"), (\"victorious\", \"insect\"), (\"vinegar fly\", \"insect\"), (\"vine (weevil\", \"insect\"), \n",
    "            (\"wanderer\", \"insect\"), (\"wasps\", \"insect\"), (\"weaver\", \"insect\"), (\"webworm moth\", \"insect\"), (\"weta\", \"insect\"), (\"whirligig beetle\", \"insect\"), (\"whisperer\", \"insect\"), (\"whitefly\", \"insect\"), (\"widow spider\", \"arachnid\"),\n",
    "            (\"willow fly\", \"insect\"), (\"winged ant\", \"insect\"), (\"wood wasp\", \"insect\"), (\"woodworm\", \"insect\"), (\"woolly bear\", \"insect\"), (\"worm\", \"insect\"),(\"wrestler\", \"insect\"), (\"yucca moth\", \"insect\"), (\"zebra butterfly\", \"insect\"),(\"zebra\", \"mammal\"), \n",
    "            (\"koala\", \"marsupial\"), (\"cheetah\", \"feline\"),(\"dolphin\", \"mammal\"),(\"parrot\", \"bird\"), (\"rhino\", \"mammal\"), (\"panda\", \"mammal\"), (\"kangaroo\", \"marsupial\"),(\"panther\", \"feline\"), (\"chimpanzee\", \"primate\"), (\"hippo\", \"mammal\"), (\"eagle\", \"bird\"),\n",
    "            (\"orangutan\", \"primate\"), (\"bear\", \"mammal\"), (\"owl\", \"bird\"), (\"polar bear\", \"mammal\"),(\"snake\", \"reptile\"), (\"hawk\", \"bird\"), (\"fox\", \"mammal\"), (\"turtle\", \"reptile\"), (\"swan\", \"bird\"), (\"jaguar\", \"feline\"), (\"seagull\", \"bird\"), (\"gazelle\", \"mammal\")]\n",
    "    return dataset\n",
    "    \n",
    "def produce_messages(kafka_bootstrap_servers, topic, dataset, iterations=1, empty=0, random_sample=20):\n",
    "    \"\"\"\n",
    "    Generate sample animal data and write it to a Kafka topic.\n",
    "\n",
    "    Args:\n",
    "        kafka_bootstrap_servers (str): Kafka bootstrap servers in the format \"host:port\".\n",
    "        topic (str): Kafka topic to which the data will be written.\n",
    "        iterations (int, optional): Number of iterations to write data. Defaults to 1.\n",
    "        empty (int, optional): Flag to indicate whether to write empty data. Defaults to 0.\n",
    "    \"\"\"\n",
    " \n",
    "    # Kafka Producer configuration\n",
    "    conf = {\n",
    "        'bootstrap.servers': kafka_bootstrap_servers,\n",
    "    }\n",
    "\n",
    "    # Create Kafka Producer\n",
    "    producer = Producer(conf)\n",
    "\n",
    "    try:\n",
    "        for iteration in range(iterations):\n",
    "            # Select random elements from the data list\n",
    "            selected_data = random.sample(dataset, random_sample)\n",
    "            \n",
    "            # Write data to Kafka\n",
    "            local_timezone = pytz.timezone('America/New_York')  # Set the local timezone\n",
    "            date_created = datetime.now(local_timezone).strftime('%Y-%m-%d %H:%M:%S %Z')  # Get current timestamp in the desired format\n",
    "            for name, animal_type in selected_data:\n",
    "                animal_data = {'name': name, 'type': animal_type, 'iteration': iteration + 1, 'date_created': date_created}\n",
    "                producer.produce(topic, key='Animal', value=json.dumps(animal_data))  # Eliminé .encode('utf-8')\n",
    "            # Print a message indicating that the data has been written to the Kafka topic\n",
    "            print(f\"Iteration {iteration + 1} , Data written to Kafka topic ({topic}).\")    \n",
    "            # delaying seconds for next batch\n",
    "            sleep(iteration + 1)  # Fixed iteration + 1 seconds wait between iterations\n",
    "            \n",
    "        # Flush any remaining messages\n",
    "        producer.flush()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        # Close the producer\n",
    "        producer.flush()  # Flush any remaining messages before closing\n",
    "        producer.poll(0)  # Poll to handle any message delivery callbacks\n",
    "        del producer\n",
    "\n",
    "# Example usage:\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "topic = \"animals-topic-batch\"\n",
    "dataset = get_dataset()\n",
    "produce_messages(kafka_bootstrap_servers=kafka_bootstrap_servers, dataset=dataset, topic=topic, iterations=5, random_sample = 27)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da53e17-dd68-442c-80aa-6a723baafea7",
   "metadata": {},
   "source": [
    "# CONSUME MESSAGES CLASIC WAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9e5dd5-a233-42e7-afec-ec2a3ab466fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from math import ceil\n",
    "\n",
    "import pandas as pd\n",
    "from confluent_kafka import Consumer, KafkaError\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "\n",
    "def consume_messages(kafka_bootstrap_servers, topic, batch_size=20, timeout=10):\n",
    "    \"\"\"\n",
    "    Consume messages from a Kafka topic in batches and return them as a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        kafka_bootstrap_servers (str): Kafka bootstrap servers in the format \"host:port\".\n",
    "        topic (str): Kafka topic from which to consume messages.\n",
    "        batch_size (int): Size of each batch of messages to return as a DataFrame. Defaults to 20.\n",
    "        timeout (int): Maximum time to wait for new messages (in seconds) before returning the current batch. Defaults to 10 seconds.\n",
    "\n",
    "    Yields:\n",
    "        pandas.DataFrame: DataFrame containing the consumed batch of messages.\n",
    "\n",
    "    Raises:\n",
    "        KeyboardInterrupt: Raised when the consumer is stopped by the user (e.g., through keyboard interrupt).\n",
    "    \"\"\"\n",
    "    # Consumer configuration\n",
    "    conf = {\n",
    "        \"bootstrap.servers\": kafka_bootstrap_servers,  # Kafka bootstrap servers\n",
    "        \"group.id\": \"my_consumer_group\",  # Consumer group ID\n",
    "        \"auto.offset.reset\": \"earliest\",  # Set the starting offset to the earliest available\n",
    "    }\n",
    "\n",
    "    # Create consumer\n",
    "    consumer = Consumer(conf)\n",
    "\n",
    "    # Subscribe to the topic\n",
    "    consumer.subscribe([topic])\n",
    "\n",
    "    # List to store messages for each batch\n",
    "    messages = []\n",
    "\n",
    "    # Time tracking for timeout\n",
    "    last_message_time = time.time()\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Wait for messages\n",
    "            msg = consumer.poll(1.0)\n",
    "\n",
    "            # Check if the timeout has been reached\n",
    "            time_limit = time.time() - last_message_time\n",
    "\n",
    "            # If no message received within the poll timeout\n",
    "            if msg is None:\n",
    "                print(\n",
    "                    f\"Producer will disconnect due to inactivity in {ceil(timeout - time_limit)} Seconds.\"\n",
    "                )\n",
    "                # If timeout exceeded, return the current batch\n",
    "                if time_limit > timeout:\n",
    "                    print(\n",
    "                        f\"No new messages received in the last {timeout} seconds. Returning the current batch.\"\n",
    "                    )\n",
    "                    if messages:\n",
    "                        # Convert the list of messages into a pandas DataFrame\n",
    "                        df = pd.DataFrame(messages)\n",
    "                        # Yield the DataFrame\n",
    "                        yield df\n",
    "                        # Clear the messages list for the next batch\n",
    "                        messages = []\n",
    "                    # If no messages received on timeout, break the loop\n",
    "                    else:\n",
    "                        break\n",
    "                continue\n",
    "\n",
    "            # Update last message time\n",
    "            last_message_time = time.time()\n",
    "\n",
    "            # Handle Kafka errors\n",
    "            if msg.error():\n",
    "                # If end of partition, continue with the next one\n",
    "                if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                    continue\n",
    "                else:\n",
    "                    # Otherwise, print the error and break the loop\n",
    "                    print(f\"Error receiving message: {msg.error()}\")\n",
    "                    break\n",
    "\n",
    "            # Decode the received value as JSON\n",
    "            try:\n",
    "                message = {\n",
    "                    \"Key\": msg.key().decode(\"utf-8\"),\n",
    "                    \"Value\": msg.value().decode(\"utf-8\"),\n",
    "                    \"Topic\": msg.topic(),\n",
    "                    \"Partition\": msg.partition(),\n",
    "                    \"Offset\": msg.offset(),\n",
    "                    \"Timestamp\": datetime.utcfromtimestamp(\n",
    "                        msg.timestamp()[1] / 1000\n",
    "                    ).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                }\n",
    "                messages.append(message)\n",
    "            except Exception as e:\n",
    "                # Print error if decoding fails\n",
    "                print(f\"Error processing message: {e}\")\n",
    "\n",
    "            # Check if the batch size has been reached\n",
    "            if len(messages) == batch_size:\n",
    "                # Convert the list of messages into a pandas DataFrame\n",
    "                df = pd.DataFrame(messages)\n",
    "                # Yield the DataFrame\n",
    "                yield df\n",
    "                # Clear the messages list for the next batch\n",
    "                messages = []\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        # If KeyboardInterrupt occurs, print message and stop the consumer\n",
    "        print(\"Stopping the consumer...\")\n",
    "\n",
    "    finally:\n",
    "        # Close the consumer\n",
    "        consumer.close()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "topic = \"animals-topic-batch\"\n",
    "\n",
    "for index, batch_df in enumerate(\n",
    "    consume_messages(kafka_bootstrap_servers, topic, batch_size=15)\n",
    "):\n",
    "    print(f\"Batch index:\", index)\n",
    "    display(batch_df)  # Display the batch DataFrame\n",
    "    # Perform operations on the batch DataFrame\n",
    "    print(f\"End Preprocessing batch {index}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b043b562-ce26-4f1b-a291-4ef32c295555",
   "metadata": {},
   "source": [
    "# <center> SPARK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072390a7-8284-4012-a43a-54d48ea249c3",
   "metadata": {},
   "source": [
    "# ANIMALS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f7b6c4-b8d0-44c2-b1cf-5ba74cb78a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    # Create a sample list with animal data\n",
    "    dataset = [\n",
    "        (\"lion\", \"mammal\"),\n",
    "        (\"elephant\", \"mammal\"),\n",
    "        (\"tiger\", \"feline\"),\n",
    "        (\"whale\", \"mammal\"),\n",
    "        (\"penguin\", \"bird\"),\n",
    "        (\"gorilla\", \"primate\"),\n",
    "        (\"leopard\", \"feline\"),\n",
    "        (\"crocodile\", \"reptile\"),\n",
    "        (\"rhinoceros\", \"mammal\"),\n",
    "        (\"zebra\", \"mammal\"),\n",
    "        (\"hippopotamus\", \"mammal\"),\n",
    "        (\"eagle\", \"bird\"),\n",
    "        (\"orangutan\", \"primate\"),\n",
    "        (\"grizzly bear\", \"mammal\"),\n",
    "        (\"owl\", \"bird\"),\n",
    "        (\"polar bear\", \"mammal\"),\n",
    "        (\"python\", \"reptile\"),\n",
    "        (\"hawk\", \"bird\"),\n",
    "        (\"wolf\", \"mammal\"),\n",
    "        (\"tortoise\", \"reptile\"),\n",
    "        (\"swan\", \"bird\"),\n",
    "        (\"cheetah\", \"feline\"),\n",
    "        (\"seagull\", \"bird\"),\n",
    "        (\"giraffe\", \"mammal\"),\n",
    "        (\"deer\", \"mammal\"),\n",
    "        (\"giraffe\", \"mammal\"),\n",
    "        (\"lizard\", \"reptile\"),\n",
    "        (\"flamingo\", \"bird\"),\n",
    "        (\"chimp\", \"primate\"),\n",
    "        (\"buffalo\", \"mammal\"),\n",
    "        (\"vulture\", \"bird\"),\n",
    "        (\"bear\", \"mammal\"),\n",
    "        (\"anaconda\", \"reptile\"),\n",
    "        (\"pigeon\", \"bird\"),\n",
    "        (\"coyote\", \"mammal\"),\n",
    "        (\"chameleon\", \"reptile\"),\n",
    "        (\"ostrich\", \"bird\"),\n",
    "        (\"jaguar\", \"feline\"),\n",
    "        (\"owl\", \"bird\"),\n",
    "        (\"beetle\", \"insect\"),\n",
    "        (\"snail\", \"invertebrate\"),\n",
    "        (\"octopus\", \"cephalopod\"),\n",
    "        (\"lobster\", \"crustacean\"),\n",
    "        (\"koala\", \"marsupial\"),\n",
    "        (\"crane\", \"bird\"),\n",
    "        (\"iguana\", \"reptile\"),\n",
    "        (\"lemur\", \"primate\"),\n",
    "        (\"sloth\", \"mammal\"),\n",
    "        (\"gazelle\", \"mammal\"),\n",
    "        (\"wombat\", \"marsupial\"),\n",
    "        (\"hummingbird\", \"bird\"),\n",
    "        (\"porcupine\", \"mammal\"),\n",
    "        (\"macaw\", \"bird\"),\n",
    "        (\"hyena\", \"mammal\"),\n",
    "        (\"dolphin\", \"mammal\"),\n",
    "        (\"seahorse\", \"fish\"),\n",
    "        (\"orca\", \"mammal\"),\n",
    "        (\"kangaroo\", \"marsupial\"),\n",
    "        (\"shark\", \"fish\"),\n",
    "        (\"beaver\", \"mammal\"),\n",
    "        (\"platypus\", \"mammal\"),\n",
    "        (\"armadillo\", \"mammal\"),\n",
    "        (\"rabbit\", \"mammal\"),\n",
    "        (\"camel\", \"mammal\"),\n",
    "        (\"squirrel\", \"mammal\"),\n",
    "        (\"peacock\", \"bird\"),\n",
    "        (\"crow\", \"bird\"),\n",
    "        (\"frog\", \"amphibian\"),\n",
    "        (\"toad\", \"amphibian\"),\n",
    "        (\"newt\", \"amphibian\"),\n",
    "        (\"axolotl\", \"amphibian\"),\n",
    "        (\"butterfly\", \"insect\"),\n",
    "        (\"dragonfly\", \"insect\"),\n",
    "        (\"grasshopper\", \"insect\"),\n",
    "        (\"mantis\", \"insect\"),\n",
    "        (\"beetle\", \"insect\"),\n",
    "        (\"ant\", \"insect\"),\n",
    "        (\"termite\", \"insect\"),\n",
    "        (\"spider\", \"arachnid\"),\n",
    "        (\"scorpion\", \"arachnid\"),\n",
    "        (\"tick\", \"arachnid\"),\n",
    "        (\"bee\", \"insect\"),\n",
    "        (\"wasp\", \"insect\"),\n",
    "        (\"hornet\", \"insect\"),\n",
    "        (\"fly\", \"insect\"),\n",
    "        (\"mosquito\", \"insect\"),\n",
    "        (\"cockroach\", \"insect\"),\n",
    "        (\"ladybug\", \"insect\"),\n",
    "        (\"firefly\", \"insect\"),\n",
    "        (\"millipede\", \"arthropod\"),\n",
    "        (\"centipede\", \"arthropod\"),\n",
    "        (\"crab\", \"crustacean\"),\n",
    "        (\"shrimp\", \"crustacean\"),\n",
    "        (\"barnacle\", \"crustacean\"),\n",
    "        (\"clam\", \"mollusk\"),\n",
    "        (\"oyster\", \"mollusk\"),\n",
    "        (\"mussel\", \"mollusk\"),\n",
    "        (\"snail\", \"mollusk\"),\n",
    "        (\"slug\", \"mollusk\"),\n",
    "        (\"squid\", \"cephalopod\"),\n",
    "        (\"cuttlefish\", \"cephalopod\"),\n",
    "        (\"nautilus\", \"cephalopod\"),\n",
    "        (\"jellyfish\", \"cnidarian\"),\n",
    "        (\"coral\", \"cnidarian\"),\n",
    "        (\"hydra\", \"cnidarian\"),\n",
    "        (\"anemone\", \"cnidarian\"),\n",
    "        (\"sponge\", \"porifera\"),\n",
    "        (\"sea cucumber\", \"echinoderm\"),\n",
    "        (\"starfish\", \"echinoderm\"),\n",
    "        (\"sand dollar\", \"echinoderm\"),\n",
    "        (\"sea urchin\", \"echinoderm\"),\n",
    "        (\"brittle star\", \"echinoderm\"),\n",
    "        (\"sea star\", \"echinoderm\"),\n",
    "        (\"sea lily\", \"echinoderm\"),\n",
    "        (\"feather star\", \"echinoderm\"),\n",
    "        (\"black widow\", \"arachnid\"),\n",
    "        (\"brown recluse\", \"arachnid\"),\n",
    "        (\"tarantula\", \"arachnid\"),\n",
    "        (\"daddy longlegs\", \"arachnid\"),\n",
    "        (\"wolf spider\", \"arachnid\"),\n",
    "        (\"jumping spider\", \"arachnid\"),\n",
    "        (\"huntsman spider\", \"arachnid\"),\n",
    "        (\"tarantula hawk\", \"insect\"),\n",
    "        (\"assassin bug\", \"insect\"),\n",
    "        (\"lacewing\", \"insect\"),\n",
    "        (\"stink bug\", \"insect\"),\n",
    "        (\"cicada\", \"insect\"),\n",
    "        (\"walking stick\", \"insect\"),\n",
    "        (\"scorpionfly\", \"insect\"),\n",
    "        (\"flower mantis\", \"insect\"),\n",
    "        (\"praying mantis\", \"insect\"),\n",
    "        (\"earwig\", \"insect\"),\n",
    "        (\"flea\", \"insect\"),\n",
    "        (\"leaf insect\", \"insect\"),\n",
    "        (\"planthopper\", \"insect\"),\n",
    "        (\"scale insect\", \"insect\"),\n",
    "        (\"aphid\", \"insect\"),\n",
    "        (\"mealybug\", \"insect\"),\n",
    "        (\"thrips\", \"insect\"),\n",
    "        (\"whitefly\", \"insect\"),\n",
    "        (\"beetle\", \"insect\"),\n",
    "        (\"antlion\", \"insect\"),\n",
    "        (\"snakefly\", \"insect\"),\n",
    "        (\"dobsonfly\", \"insect\"),\n",
    "        (\"webspinner\", \"insect\"),\n",
    "        (\"mayfly\", \"insect\"),\n",
    "        (\"stonefly\", \"insect\"),\n",
    "        (\"silverfish\", \"insect\"),\n",
    "        (\"firebrat\", \"insect\"),\n",
    "        (\"bristletail\", \"insect\"),\n",
    "        (\"thysanuran\", \"insect\"),\n",
    "        (\"dragonfly\", \"insect\"),\n",
    "        (\"damselfly\", \"insect\"),\n",
    "        (\"bluet\", \"insect\"),\n",
    "        (\"darner\", \"insect\"),\n",
    "        (\"adder\", \"insect\"),\n",
    "        (\"basker\", \"insect\"),\n",
    "        (\"biter\", \"insect\"),\n",
    "        (\"blister beetle\", \"insect\"),\n",
    "        (\"bomber\", \"insect\"),\n",
    "        (\"bristle beetle\", \"insect\"),\n",
    "        (\"burrower\", \"insect\"),\n",
    "        (\"carrier\", \"insect\"),\n",
    "        (\"caterpillar\", \"insect\"),\n",
    "        (\"chafers\", \"insect\"),\n",
    "        (\"chewer\", \"insect\"),\n",
    "        (\"click beetle\", \"insect\"),\n",
    "        (\"cobblers\", \"insect\"),\n",
    "        (\"cobweb spider\", \"arachnid\"),\n",
    "        (\"cockroaches\", \"insect\"),\n",
    "        (\"coil worm\", \"insect\"),\n",
    "        (\"creeper\", \"insect\"),\n",
    "        (\"cuckoo wasp\", \"insect\"),\n",
    "        (\"cutworm\", \"insect\"),\n",
    "        (\"digger\", \"insect\"),\n",
    "        (\"dor beetle\", \"insect\"),\n",
    "        (\"earthworms\", \"insect\"),\n",
    "        (\"eggfly\", \"insect\"),\n",
    "        (\"elaters\", \"insect\"),\n",
    "        (\"emperor\", \"insect\"),\n",
    "        (\"gadfly\", \"insect\"),\n",
    "        (\"gnat\", \"insect\"),\n",
    "        (\"grasshopper\", \"insect\"),\n",
    "        (\"grazer\", \"insect\"),\n",
    "        (\"ground beetle\", \"insect\"),\n",
    "        (\"harvester\", \"insect\"),\n",
    "        (\"hornet\", \"insect\"),\n",
    "        (\"hornworm\", \"insect\"),\n",
    "        (\"humblebee\", \"insect\"),\n",
    "        (\"humpbacked fly\", \"insect\"),\n",
    "        (\"hoverfly\", \"insect\"),\n",
    "        (\"hunter\", \"insect\"),\n",
    "        (\"jumper\", \"insect\"),\n",
    "        (\"katydid\", \"insect\"),\n",
    "        (\"lacewing\", \"insect\"),\n",
    "        (\"leafcutter\", \"insect\"),\n",
    "        (\"leafhopper\", \"insect\"),\n",
    "        (\"leafroller\", \"insect\"),\n",
    "        (\"louse\", \"insect\"),\n",
    "        (\"maggot\", \"insect\"),\n",
    "        (\"mantisfly\", \"insect\"),\n",
    "        (\"marsh fly\", \"insect\"),\n",
    "        (\"marsh beetle\", \"insect\"),\n",
    "        (\"mason wasp\", \"insect\"),\n",
    "        (\"mealybug\", \"insect\"),\n",
    "        (\"miner\", \"insect\"),\n",
    "        (\"mite\", \"insect\"),\n",
    "        (\"mole cricket\", \"insect\"),\n",
    "        (\"moth\", \"insect\"),\n",
    "        (\"nemesis\", \"insect\"),\n",
    "        (\"net-winged insect\", \"insect\"),\n",
    "        (\"nightcrawler\", \"insect\"),\n",
    "        (\"nit\", \"insect\"),\n",
    "        (\"nymph\", \"insect\"),\n",
    "        (\"odorous ant\", \"insect\"),\n",
    "        (\"oracle\", \"insect\"),\n",
    "        (\"orb weaver\", \"arachnid\"),\n",
    "        (\"orcus\", \"insect\"),\n",
    "        (\"ostracod\", \"insect\"),\n",
    "        (\"outlaw\", \"insect\"),\n",
    "        (\"peacock butterfly\", \"insect\"),\n",
    "        (\"pharaoh ant\", \"insect\"),\n",
    "        (\"pillbug\", \"insect\"),\n",
    "        (\"plankton\", \"insect\"),\n",
    "        (\"pollinator\", \"insect\"),\n",
    "        (\"potter wasp\", \"insect\"),\n",
    "        (\"praying mantis\", \"insect\"),\n",
    "        (\"predator\", \"insect\"),\n",
    "        (\"proboscis\", \"insect\"),\n",
    "        (\"prophet\", \"insect\"),\n",
    "        (\"pruner\", \"insect\"),\n",
    "        (\"pseudoscorpion\", \"arachnid\"),\n",
    "        (\"psycho\", \"insect\"),\n",
    "        (\"psycho fly\", \"insect\"),\n",
    "        (\"psychodid\", \"insect\"),\n",
    "        (\"pupa\", \"insect\"),\n",
    "        (\"purple martin\", \"insect\"),\n",
    "        (\"putter\", \"insect\"),\n",
    "        (\"ranger\", \"insect\"),\n",
    "        (\"recluse\", \"insect\"),\n",
    "        (\"reducer\", \"insect\"),\n",
    "        (\"repeater\", \"insect\"),\n",
    "        (\"riffle bug\", \"insect\"),\n",
    "        (\"robber fly\", \"insect\"),\n",
    "        (\"rootworm\", \"insect\"),\n",
    "        (\"rover\", \"insect\"),\n",
    "        (\"saber wasp\", \"insect\"),\n",
    "        (\"sawfly\", \"insect\"),\n",
    "        (\"scarab\", \"insect\"),\n",
    "        (\"scorpionfly\", \"insect\"),\n",
    "        (\"scourge\", \"insect\"),\n",
    "        (\"scout\", \"insect\"),\n",
    "        (\"scuttle fly\", \"insect\"),\n",
    "        (\"silk spinner\", \"insect\"),\n",
    "        (\"silverfish\", \"insect\"),\n",
    "        (\"skipper butterfly\", \"insect\"),\n",
    "        (\"snout butterfly\", \"insect\"),\n",
    "        (\"snout beetle\", \"insect\"),\n",
    "        (\"snout moth\", \"insect\"),\n",
    "        (\"sow bug\", \"insect\"),\n",
    "        (\"spider mite\", \"insect\"),\n",
    "        (\"spider wasp\", \"insect\"),\n",
    "        (\"sphinx moth\", \"insect\"),\n",
    "        (\"spider\", \"arachnid\"),\n",
    "        (\"spinner\", \"insect\"),\n",
    "        (\"spittlebug\", \"insect\"),\n",
    "        (\"spook\", \"insect\"),\n",
    "        (\"springtail\", \"insect\"),\n",
    "        (\"stag beetle\", \"insect\"),\n",
    "        (\"stealer\", \"insect\"),\n",
    "        (\"stinger\", \"insect\"),\n",
    "        (\"stink bug\", \"insect\"),\n",
    "        (\"stinging ant\", \"insect\"),\n",
    "        (\"stonefly\", \"insect\"),\n",
    "        (\"strangler\", \"insect\"),\n",
    "        (\"sucking louse\", \"insect\"),\n",
    "        (\"sweat bee\", \"insect\"),\n",
    "        (\"tailor\", \"insect\"),\n",
    "        (\"tanglefoot\", \"insect\"),\n",
    "        (\"tarantula\", \"arachnid\"),\n",
    "        (\"tarantula hawk\", \"insect\"),\n",
    "        (\"tick\", \"arachnid\"),\n",
    "        (\"tiger beetle\", \"insect\"),\n",
    "        (\"tiger moth\", \"insect\"),\n",
    "        (\"tiphiid wasp\", \"insect\"),\n",
    "        (\"titan beetle\", \"insect\"),\n",
    "        (\"toad bug\", \"insect\"),\n",
    "        (\"torchbearer\", \"insect\"),\n",
    "        (\"torpedo bug\", \"insect\"),\n",
    "        (\"tortoise beetle\", \"insect\"),\n",
    "        (\"trapper\", \"insect\"),\n",
    "        (\"tree cricket\", \"insect\"),\n",
    "        (\"trilobite beetle\", \"insect\"),\n",
    "        (\"trogonoptera\", \"insect\"),\n",
    "        (\"twig borer\", \"insect\"),\n",
    "        (\"vampire\", \"insect\"),\n",
    "        (\"victorious\", \"insect\"),\n",
    "        (\"vinegar fly\", \"insect\"),\n",
    "        (\"vine (weevil\", \"insect\"),\n",
    "        (\"wanderer\", \"insect\"),\n",
    "        (\"wasps\", \"insect\"),\n",
    "        (\"weaver\", \"insect\"),\n",
    "        (\"webworm moth\", \"insect\"),\n",
    "        (\"weta\", \"insect\"),\n",
    "        (\"whirligig beetle\", \"insect\"),\n",
    "        (\"whisperer\", \"insect\"),\n",
    "        (\"whitefly\", \"insect\"),\n",
    "        (\"widow spider\", \"arachnid\"),\n",
    "        (\"willow fly\", \"insect\"),\n",
    "        (\"winged ant\", \"insect\"),\n",
    "        (\"wood wasp\", \"insect\"),\n",
    "        (\"woodworm\", \"insect\"),\n",
    "        (\"woolly bear\", \"insect\"),\n",
    "        (\"worm\", \"insect\"),\n",
    "        (\"wrestler\", \"insect\"),\n",
    "        (\"yucca moth\", \"insect\"),\n",
    "        (\"zebra butterfly\", \"insect\"),\n",
    "        (\"zebra\", \"mammal\"),\n",
    "        (\"koala\", \"marsupial\"),\n",
    "        (\"cheetah\", \"feline\"),\n",
    "        (\"dolphin\", \"mammal\"),\n",
    "        (\"parrot\", \"bird\"),\n",
    "        (\"rhino\", \"mammal\"),\n",
    "        (\"panda\", \"mammal\"),\n",
    "        (\"kangaroo\", \"marsupial\"),\n",
    "        (\"panther\", \"feline\"),\n",
    "        (\"chimpanzee\", \"primate\"),\n",
    "        (\"hippo\", \"mammal\"),\n",
    "        (\"eagle\", \"bird\"),\n",
    "        (\"orangutan\", \"primate\"),\n",
    "        (\"bear\", \"mammal\"),\n",
    "        (\"owl\", \"bird\"),\n",
    "        (\"polar bear\", \"mammal\"),\n",
    "        (\"snake\", \"reptile\"),\n",
    "        (\"hawk\", \"bird\"),\n",
    "        (\"fox\", \"mammal\"),\n",
    "        (\"turtle\", \"reptile\"),\n",
    "        (\"swan\", \"bird\"),\n",
    "        (\"jaguar\", \"feline\"),\n",
    "        (\"seagull\", \"bird\"),\n",
    "        (\"gazelle\", \"mammal\"),\n",
    "    ]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cfb762-517f-4751-9d95-470531a78e84",
   "metadata": {},
   "source": [
    "# DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cf8395-7cce-4af4-8f47-4aa0e30f4cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Directory where JARs are located\n",
    "jars_directory = \"/usr/local/spark/jars/\"\n",
    "\n",
    "# List of JAR filenames\n",
    "jar_files = [\n",
    "    \"commons-pool2-2.12.0.jar\",\n",
    "    \"kafka-clients-3.7.0.jar\",\n",
    "    \"spark-sql-kafka-0-10_2.12-3.4.3.jar\",\n",
    "    \"spark-token-provider-kafka-0-10_2.12-3.4.3.jar\",\n",
    "]\n",
    "\n",
    "dependencies = \",\".join([os.path.join(jars_directory, jar) for jar in jar_files])\n",
    "\n",
    "# Configure Kafka connection\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "topic = \"animals-topic-batch\"\n",
    "\n",
    "# Create Spark session and add JARs\n",
    "spark_session = (\n",
    "    SparkSession.builder.appName(\"WriteKafkaAnimals\")\n",
    "    .config(\"spark.jars\", dependencies)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e05947-be49-4ba6-a986-3f20fe11babe",
   "metadata": {},
   "source": [
    "# BATCH WRITING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f14306-a6e2-48c6-8fe8-7926efd1ff2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "\n",
    "import pytz\n",
    "\n",
    "\n",
    "# Function to save batch data to a Kafka topic\n",
    "def save_batch_data(\n",
    "    spark_session,\n",
    "    kafka_bootstrap_servers,\n",
    "    dataset,\n",
    "    topic,\n",
    "    iterations=1,\n",
    "    empty=0,\n",
    "    random_sample=20,\n",
    "):\n",
    "    \"\"\"\n",
    "    Save batch data to a Kafka topic.\n",
    "\n",
    "    Args:\n",
    "        spark_session (SparkSession): Spark session object.\n",
    "        kafka_bootstrap_servers (str): Kafka bootstrap servers in the format \"host:port\".\n",
    "        dataset (list): List of tuples containing data to be written to Kafka.\n",
    "        topic (str): Kafka topic to which the data will be written.\n",
    "        iterations (int, optional): Number of iterations to run. Defaults to 1.\n",
    "        empty (int, optional): Placeholder for future use. Defaults to 0.\n",
    "        random_sample (int, optional): Number of random samples to select from the dataset for each iteration. Defaults to 20.\n",
    "    \"\"\"\n",
    "    # Define the columns for the DataFrame\n",
    "    columns = [\"topic_name\", \"name\", \"animal_type\", \"iteration\", \"date_created\"]\n",
    "\n",
    "    # Iterate through the specified number of iterations\n",
    "    for iteration in range(iterations):\n",
    "        # Select random elements from the dataset\n",
    "        selected_data = random.sample(dataset, random_sample)\n",
    "\n",
    "        # Write data to Kafka\n",
    "        local_timezone = pytz.timezone(\"America/New_York\")  # Set the local timezone\n",
    "        date_created = datetime.now(local_timezone).strftime(\n",
    "            \"%Y-%m-%d %H:%M:%S %Z\"\n",
    "        )  # Get the current timestamp in the desired format\n",
    "\n",
    "        values = list()\n",
    "\n",
    "        # Create a list of tuples with animal data and other metadata\n",
    "        for name, animal_type in selected_data:\n",
    "            animal_data = (\"Animal\", name, animal_type, iteration + 1, date_created)\n",
    "            values.append(animal_data)\n",
    "\n",
    "        # Create a Spark DataFrame from the values and columns\n",
    "        df_animals = spark_session.createDataFrame(values, columns)\n",
    "\n",
    "        # Write the DataFrame to the Kafka topic\n",
    "        df_animals.selectExpr(\n",
    "            \"topic_name as key\",\n",
    "            \"to_json(struct(name, animal_type, iteration, date_created)) as value\",\n",
    "        ).write.format(\"kafka\").option(\n",
    "            \"kafka.bootstrap.servers\", kafka_bootstrap_servers\n",
    "        ).option(\n",
    "            \"topic\", topic\n",
    "        ).save()\n",
    "\n",
    "        # Print a message indicating that the data has been written to the Kafka topic\n",
    "        print(f\"Iteration {iteration + 1} , Data written to Kafka topic ({topic}).\")\n",
    "\n",
    "\n",
    "# Configure Kafka connection\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "topic = \"animals-topic-batch\"\n",
    "dataset = get_dataset()\n",
    "save_batch_data(\n",
    "    spark_session=spark_session,\n",
    "    kafka_bootstrap_servers=kafka_bootstrap_servers,\n",
    "    dataset=dataset,\n",
    "    topic=topic,\n",
    "    iterations=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18600232-19bf-42b8-a191-a096ba535b08",
   "metadata": {},
   "source": [
    "# BATCH READING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc3e9cb-e24c-4366-a0da-27cb057aba45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "# Define the hexadecimal decoding function\n",
    "@udf(returnType=StringType())\n",
    "def decode_hex(value):\n",
    "    try:\n",
    "        if isinstance(value, str):\n",
    "            return bytes.fromhex(value).decode(\"utf-8\")\n",
    "        elif isinstance(value, bytearray):\n",
    "            return bytes(value).decode(\"utf-8\")\n",
    "        else:\n",
    "            return str(value)\n",
    "    except (ValueError, UnicodeDecodeError):\n",
    "        return str(value)\n",
    "\n",
    "\n",
    "def read_batch_data(spark_session, kafka_bootstrap_servers, topic):\n",
    "\n",
    "    # Try to read data from Kafka\n",
    "    try:\n",
    "        # Read data from Kafka\n",
    "        df_kafka = (\n",
    "            spark_session.read.format(\"kafka\")\n",
    "            .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers)\n",
    "            .option(\"subscribe\", topic)\n",
    "            .load()\n",
    "        )\n",
    "\n",
    "        # Decode hexadecimal values\n",
    "        df_decoded = df_kafka.withColumn(\"key\", decode_hex(\"key\")).withColumn(\n",
    "            \"value\", decode_hex(\"value\")\n",
    "        )\n",
    "\n",
    "        # Show the DataFrame with decoded data\n",
    "        df_decoded.show(truncate=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        if \"UnknownTopicOrPartitionException\" in str(e):\n",
    "            print(f\"The topic '{topic}' does not exist in the Kafka cluster.\")\n",
    "        else:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Stop the Spark session\n",
    "        None\n",
    "\n",
    "\n",
    "topic = \"animals-topic-batch\"\n",
    "read_batch_data(spark_session, kafka_bootstrap_servers, topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a48a04-8cb8-460b-b9dd-d1b1e601dff6",
   "metadata": {},
   "source": [
    "# STREAMING WRITING\n",
    "## MASSIVE DATA INSERTION TO KAFKA FOR STREAMING READ\n",
    "### COPY, PASTE, AND RUN THE FOLLOWING CODE IN ANOTHER NOTEBOOK TO OBSERVE STREAMING REDIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c759bab-bb0c-4279-8602-b24a5a6281c6",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "```python\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from time import sleep\n",
    "\n",
    "# Directory where JARs are located\n",
    "jars_directory = \"/usr/local/spark/jars/\"\n",
    "\n",
    "# List of JAR filenames\n",
    "jar_files = [\n",
    "    \"commons-pool2-2.12.0.jar\",\n",
    "    \"kafka-clients-3.7.0.jar\",\n",
    "    \"spark-sql-kafka-0-10_2.12-3.4.3.jar\",\n",
    "    \"spark-token-provider-kafka-0-10_2.12-3.4.3.jar\"\n",
    "]\n",
    "\n",
    "dependencies = \",\".join([os.path.join(jars_directory, jar) for jar in jar_files])\n",
    "\n",
    "# Configure Kafka connection\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "topic = \"animals-topic-batch\"\n",
    "\n",
    "\n",
    "# Create Spark session and add JARs\n",
    "spark_session = SparkSession.builder \\\n",
    "    .appName(\"WriteKafkaAnimals\") \\\n",
    "    .config(\"spark.jars\", dependencies) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "def save_batch_data(spark_session, kafka_bootstrap_servers, topic, iterations=1):\n",
    "\n",
    "    # Create a sample DataFrame with animal data\n",
    "    data = [(\"zebra\", \"mammal\"), (\"koala\", \"marsupial\"), (\"cheetah\", \"feline\"),(\"dolphin\", \"mammal\"),\n",
    "            (\"parrot\", \"bird\"), (\"rhino\", \"mammal\"), (\"panda\", \"mammal\"), (\"kangaroo\", \"marsupial\"), \n",
    "            (\"panther\", \"feline\"), (\"chimpanzee\", \"primate\"), (\"hippo\", \"mammal\"), (\"eagle\", \"bird\"), \n",
    "            (\"orangutan\", \"primate\"), (\"bear\", \"mammal\"), (\"owl\", \"bird\"), (\"polar bear\", \"mammal\"), \n",
    "            (\"snake\", \"reptile\"), (\"hawk\", \"bird\"), (\"fox\", \"mammal\"), (\"turtle\", \"reptile\"), \n",
    "            (\"swan\", \"bird\"), (\"jaguar\", \"feline\"), (\"seagull\", \"bird\"), (\"gazelle\", \"mammal\")]\n",
    "    \n",
    "    columns = [\"name\", \"type\"]\n",
    "    \n",
    "    df_animals = spark_session.createDataFrame(data, columns)\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        sleep(0.2)\n",
    "        # Write the DataFrame to Kafka topic\n",
    "        df_animals.selectExpr(\"name as key\", \"type as value\") \\\n",
    "            .write \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "            .option(\"topic\", topic) \\\n",
    "            .save()\n",
    "        print (f'Iteration {iteration}, completed!!!')\n",
    "    \n",
    "    # Print a message indicating that the data has been written to the Kafka topic\n",
    "    print(f\"{iterations} Iterations, Data written to Kafka topic ({topic}).\")\n",
    "\n",
    "     # Finally, stop the Spark session\n",
    "    spark_session.stop()\n",
    "\n",
    "save_batch_data(spark_session, kafka_bootstrap_servers, topic, iterations=13)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3268fa8-9fa5-49d3-87fd-d7db3c28bf07",
   "metadata": {},
   "source": [
    "# STREAMING READING\n",
    "## IT WILL BE LISTENING TO THE \"animals-topic-batch\" TOPIC\n",
    "## WHEN NEW DATA ARRIVES, IT READS AND STORES THEM IN THE \"animals-topic-streaming\" TOPIC\n",
    "## MAKE THE TRANSFORMATIONS\n",
    "### UPPER CASE, PROCCESS ROW BY ROW, USE UDF FUNCTION AND SEND PARAMETERS, CREATE PANDAS DATAFRAME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050f2e11-cef4-4896-a5f4-ae75d574690a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from pyspark.sql.functions import col, expr, udf\n",
    "\n",
    "\n",
    "def read_streaming_data(\n",
    "    spark_session,\n",
    "    kafka_bootstrap_servers,\n",
    "    input_topic,\n",
    "    output_topic,\n",
    "    checkpoint_location,\n",
    "    files_directory,\n",
    "):\n",
    "\n",
    "    # Read from Kafka in streaming mode\n",
    "    kafkaStream = (\n",
    "        spark_session.readStream.format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers)\n",
    "        .option(\"subscribe\", input_topic)\n",
    "        .option(\"startingOffsets\", \"earliest\")\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "    # Perform some transformation on the data (here we are simply renaming the columns and uppercase the values)\n",
    "    kafkaTransformedStream = kafkaStream.selectExpr(\n",
    "        \"CAST(key AS STRING) as key\", \"UPPER(CAST(value AS STRING)) as value\"\n",
    "    )\n",
    "\n",
    "    # Configure a streaming write query to write data to Parquet format\n",
    "    kafkaStreamSaveToParquet = (\n",
    "        kafkaTransformedStream.writeStream.outputMode(\"append\")\n",
    "        .format(\"parquet\")\n",
    "        .option(\"path\", files_directory)\n",
    "        .option(\"checkpointLocation\", checkpoint_location)\n",
    "        .trigger(processingTime=\"1 minute\")\n",
    "        .start()\n",
    "    )\n",
    "\n",
    "    def process_batch2(batch_df, batch_id):\n",
    "        print(f\"Batch ID: {batch_id}\")\n",
    "        batch_df.show(truncate=False)\n",
    "\n",
    "    # Define a function to process each batch and print it without truncating\n",
    "    def process_batch(\n",
    "        batch_df,\n",
    "        batch_id,\n",
    "        kafka_bootstrap_servers,\n",
    "        input_topic,\n",
    "        output_topic,\n",
    "        checkpoint_location,\n",
    "        files_directory,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Processes each batch of the streaming DataFrame and displays it as a Pandas DataFrame.\n",
    "\n",
    "        Args:\n",
    "        - batch_df (DataFrame): The DataFrame containing the batch data.\n",
    "        - batch_id (int): The ID of the batch being processed.\n",
    "        - kafka_bootstrap_servers (str): The Kafka bootstrap servers.\n",
    "        - input_topic (str): The input Kafka topic.\n",
    "        - output_topic (str): The output Kafka topic.\n",
    "        - checkpoint_location (str): The location for checkpointing.\n",
    "        - files_directory (str): The directory path for saving files.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        # Print the batch ID for identification\n",
    "        print(f\"Batch ID : {batch_id}\")\n",
    "\n",
    "        # Collect key-value pairs from the batch DataFrame\n",
    "        key_values = batch_df.select(col(\"key\"), col(\"value\")).collect()\n",
    "\n",
    "        # Initialize an empty list to store data for the Pandas DataFrame\n",
    "        data = []\n",
    "\n",
    "        # Iterate through key-value pairs\n",
    "        for row in key_values:\n",
    "            # Extract the key and create a dictionary with the key\n",
    "            row_data = {\"key\": row[\"key\"]}\n",
    "\n",
    "            # Parse the JSON value and update the dictionary\n",
    "            value_dict = json.loads(row[\"value\"])\n",
    "            row_data.update(value_dict)\n",
    "\n",
    "            # Append Kafka metadata to the dictionary\n",
    "            row_data.update(\n",
    "                {\n",
    "                    \"input_topic\": input_topic,  # Use input_topic as the Kafka topic\n",
    "                    \"output_topic\": input_topic,  # Use output_topic as the Kafka topic\n",
    "                    \"checkpoint_location\": checkpoint_location,  # Use checkpoint_location as the Kafka topic\n",
    "                    \"files_directory\": files_directory,  # Use files_directory as the Kafka topic\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Append the updated dictionary to the data list\n",
    "            data.append(row_data)\n",
    "\n",
    "        # Create a Pandas DataFrame from the collected data\n",
    "        df_pandas = pd.DataFrame(data)\n",
    "\n",
    "        # Display the Pandas DataFrame\n",
    "        display(df_pandas)\n",
    "\n",
    "    from functools import partial\n",
    "\n",
    "    # Create a partial function with the additional parameters\n",
    "    partial_process_batch = partial(\n",
    "        process_batch,\n",
    "        kafka_bootstrap_servers=kafka_bootstrap_servers,\n",
    "        input_topic=input_topic,\n",
    "        output_topic=output_topic,\n",
    "        checkpoint_location=checkpoint_location,\n",
    "        files_directory=files_directory,\n",
    "    )\n",
    "\n",
    "    # Configure a streaming write query to send data to Kafka, and use a function into foreach for show content whitout truncate\n",
    "    kafkaStreamWriter = (\n",
    "        kafkaTransformedStream.writeStream.option(\"failOnDataLoss\", \"false\")\n",
    "        .outputMode(\"append\")\n",
    "        .format(\"kafka\")\n",
    "        .option(\"truncate\", \"false\")\n",
    "        .option(\"checkpointLocation\", checkpoint_location)\n",
    "        .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers)\n",
    "        .option(\"topic\", output_topic)\n",
    "        .foreachBatch(partial_process_batch)\n",
    "        .start()\n",
    "    )\n",
    "\n",
    "    print(\"kafkaStream\", type(kafkaStream))\n",
    "    print(\"transformedStream\", type(kafkaTransformedStream))\n",
    "    print(\"query\", type(kafkaStreamWriter))\n",
    "\n",
    "\n",
    "# Configure Kafka connection\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "input_topic = \"animals-topic-batch\"\n",
    "output_topic = \"animals-topic-streaming\"\n",
    "# Checkpoint directory within the Kafka directory\n",
    "checkpoint_location = \"/usr/local/kafka/data/checkpoint\"\n",
    "files_directory = \"/usr/local/kafka/data/files\"\n",
    "\n",
    "read_streaming_data(\n",
    "    spark_session,\n",
    "    kafka_bootstrap_servers,\n",
    "    input_topic,\n",
    "    output_topic,\n",
    "    checkpoint_location,\n",
    "    files_directory,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec0bf92-d064-4727-a24e-1f6b06f02ca2",
   "metadata": {},
   "source": [
    "# STREAMING FILES USING FROM DIRECTORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a87ecc-d707-4e67-a9a5-eaabca9461f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, expr, lit\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    FloatType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "\n",
    "\n",
    "def process_batch(directory_to_save_files, df, epoch_id):\n",
    "\n",
    "    # Generate a timestamp\n",
    "    timestamp_str = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "    # Add epoch_id and timestamp_str as new columns\n",
    "    df = df.withColumn(\"batch_id\", lit(epoch_id)).withColumn(\n",
    "        \"timestamp\", lit(timestamp_str)\n",
    "    )\n",
    "\n",
    "    # Print information about the new batch\n",
    "    print(f\"Processing microbatch {epoch_id} at {timestamp_str}\")\n",
    "\n",
    "    # Group by specified columns and calculate the average ticket price\n",
    "    grouped_df = df.groupBy(\n",
    "        \"batch_id\",\n",
    "        \"timestamp\",\n",
    "        \"passenger_nationality\",\n",
    "        \"passenger_gender\",\n",
    "        \"passenger_age\",\n",
    "    ).agg(\n",
    "        F.avg(\"ticket_price\").alias(\"avg_ticket_price\"),\n",
    "        F.sum(\"ticket_price\").alias(\"total_ticket_price\"),\n",
    "        F.count(\"passenger_name\").alias(\"total_passengers\"),\n",
    "    )\n",
    "\n",
    "    # Filter out groups where the total number of passengers is greater than 1\n",
    "    # filtered_df = grouped_df.filter(grouped_df.total_passengers > 1)\n",
    "    # Multiple filter conditions\n",
    "    filtered_df = grouped_df.filter(\n",
    "        (col(\"total_passengers\") > 2) & (col(\"total_passengers\") <= 7)\n",
    "    )\n",
    "\n",
    "    # Sort the DataFrame by passenger_nationality and passenger_age\n",
    "    sorted_df = filtered_df.orderBy(\n",
    "        \"total_passengers\", \"passenger_nationality\", \"passenger_gender\"\n",
    "    )\n",
    "\n",
    "    # Show the sorted DataFrame\n",
    "    sorted_df.show(truncate=False)\n",
    "\n",
    "    # Coalesce to a single partition and write the DataFrame as Parquet with timestamp\n",
    "    maximum_parquet_files_per_batch = 3\n",
    "    sorted_df.coalesce(maximum_parquet_files_per_batch).write.parquet(\n",
    "        f\"{directory_to_save_files}/microbatch_{epoch_id}_{timestamp_str}\"\n",
    "    )\n",
    "\n",
    "    # Show the original DataFrame\n",
    "    df.show()\n",
    "\n",
    "\n",
    "def read_file_like_streaming(\n",
    "    spark_session, customSchema, format, checkpoint_location, directory_to_save_files\n",
    "):\n",
    "\n",
    "    # Read the DataFrame as a continuous stream\n",
    "    streaming_df = (\n",
    "        spark_session.readStream.schema(customSchema)\n",
    "        .format(format)\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(folder_files_path)\n",
    "    )\n",
    "\n",
    "    # Display the DataFrame in the console and count the records per microbatch\n",
    "    # .trigger(processingTime='5 seconds') specifies that Spark Structured Streaming should process micro-batches of data from the specified directory every 5 seconds.\n",
    "    query = (\n",
    "        streaming_df.writeStream.outputMode(\"append\")\n",
    "        .trigger(processingTime=\"5 seconds\")\n",
    "        .option(\"checkpointLocation\", checkpoint_location)\n",
    "        .option(\"basePath\", directory_to_save_files)\n",
    "        .foreachBatch(\n",
    "            lambda df, epoch_id: process_batch(directory_to_save_files, df, epoch_id)\n",
    "        )\n",
    "        .start()\n",
    "    )\n",
    "\n",
    "    # Wait for the streaming to complete\n",
    "    # query.awaitTermination()\n",
    "\n",
    "\n",
    "# Define the schema\n",
    "customSchema = StructType(\n",
    "    [\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"secure_code\", StringType(), True),\n",
    "        StructField(\"airline\", StringType(), True),\n",
    "        StructField(\"departure_city\", StringType(), True),\n",
    "        StructField(\"departure_date\", StringType(), True),\n",
    "        StructField(\"arrival_airport\", StringType(), True),\n",
    "        StructField(\"arrival_city\", StringType(), True),\n",
    "        StructField(\"arrival_time\", StringType(), True),\n",
    "        StructField(\"passenger_name\", StringType(), True),\n",
    "        StructField(\"passenger_gender\", StringType(), True),\n",
    "        StructField(\"seat_number\", StringType(), True),\n",
    "        StructField(\"currency\", StringType(), True),\n",
    "        StructField(\"departure_gate\", StringType(), True),\n",
    "        StructField(\"flight_status\", StringType(), True),\n",
    "        StructField(\"co_pilot_name\", StringType(), True),\n",
    "        StructField(\"aircraft_type\", StringType(), True),\n",
    "        StructField(\"fuel_consumption\", DoubleType(), True),\n",
    "        StructField(\"flight_id\", IntegerType(), True),\n",
    "        StructField(\"flight_number\", IntegerType(), True),\n",
    "        StructField(\"departure_airport\", StringType(), True),\n",
    "        StructField(\"departure_country\", StringType(), True),\n",
    "        StructField(\"departure_time\", StringType(), True),\n",
    "        StructField(\"arrival_country\", StringType(), True),\n",
    "        StructField(\"arrival_date\", StringType(), True),\n",
    "        StructField(\"flight_duration\", DoubleType(), True),\n",
    "        StructField(\"passenger_age\", IntegerType(), True),\n",
    "        StructField(\"passenger_nationality\", StringType(), True),\n",
    "        StructField(\"ticket_price\", DoubleType(), True),\n",
    "        StructField(\"baggage_weight\", DoubleType(), True),\n",
    "        StructField(\"arrival_gate\", StringType(), True),\n",
    "        StructField(\"pilot_name\", StringType(), True),\n",
    "        StructField(\"cabin_crew_count\", IntegerType(), True),\n",
    "        StructField(\"aircraft_registration\", StringType(), True),\n",
    "        StructField(\"flight_distance\", DoubleType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "format = \"csv\"\n",
    "folder_files_path = \"/notebooks\"\n",
    "directory_to_save_files = \"/usr/local/kafka/data/files/batch\"\n",
    "shutil.rmtree(directory_to_save_files, ignore_errors=True)\n",
    "\n",
    "# Delete the checkpoint directory\n",
    "checkpoint_location = \"/usr/local/kafka/data/batch\"\n",
    "shutil.rmtree(checkpoint_location, ignore_errors=True)\n",
    "\n",
    "read_file_like_streaming(\n",
    "    spark_session=spark_session,\n",
    "    customSchema=customSchema,\n",
    "    format=format,\n",
    "    checkpoint_location=checkpoint_location,\n",
    "    directory_to_save_files=directory_to_save_files,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730cf897-dcf0-40ca-a082-d446b80f44e4",
   "metadata": {},
   "source": [
    "# COPY, PASTE, AND RUN THE FOLLOWING CODE IN ANOTHER NOTEBOOK TO OBSERVE STREAMING REDING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe4f8d6-35aa-460f-a14d-b0c792be1933",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "\n",
    "for index in range(1,6):\n",
    "\n",
    "    part_1 = index\n",
    "    part_2 = index + 5\n",
    "\n",
    "    file_name = 'flight_logs'\n",
    "    final_file = f'{file_name}_{index}.csv'\n",
    "    \n",
    "    file_1 = f'{file_name}_part_1_{index}.csv'\n",
    "    base_url1 = f'https://raw.githubusercontent.com/JorgeCardona/recursos/main/datasets/{file_1}'\n",
    "    df1 = pd.read_csv(base_url1)\n",
    "    \n",
    "    file_2 = f'{file_name}_part_2_{index}.csv'\n",
    "    base_url2 = f'https://raw.githubusercontent.com/JorgeCardona/recursos/main/datasets/{file_2}'\n",
    "    df2 = pd.read_csv(base_url2)\n",
    "\n",
    "    df3 = pd.merge(df1, df2, left_on='id', right_on='flight_id', how='inner')\n",
    "    df3.to_csv(f'{final_file}',index=False)\n",
    "\n",
    "    print(f'{final_file} saved Successfully!!')\n",
    "    sleep(5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81f64fe-6aac-48d9-95ff-34f9d8a007f6",
   "metadata": {},
   "source": [
    "# LOAD PARQUET FILES DIRECTORY ON DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa23a9d-027e-4fe5-9559-6b3098df2b16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "def find_microbatch_file(directory):\n",
    "    \"\"\"\n",
    "    Busca un archivo en el directorio especificado que contiene el término \"microbatch\" en su nombre.\n",
    "\n",
    "    Args:\n",
    "    - directory (str): La ruta del directorio donde se buscarán los archivos.\n",
    "\n",
    "    Returns:\n",
    "    - str: El nombre del archivo que contiene el término \"microbatch\" en su nombre, o una cadena vacía si no se encuentra.\n",
    "    \"\"\"\n",
    "    # Listar todos los archivos en el directorio\n",
    "    all_files = os.listdir(directory)\n",
    "\n",
    "    # Inicializar la variable para almacenar el nombre del archivo microbatch\n",
    "    microbatch_file_name = \"\"\n",
    "\n",
    "    # Buscar el archivo que contiene el término \"microbatch\" en su nombre\n",
    "    for file_name in all_files:\n",
    "        if \"microbatch\" in file_name:\n",
    "            microbatch_file_name = file_name\n",
    "\n",
    "    return microbatch_file_name\n",
    "\n",
    "\n",
    "def read_parquet_directory_into_dataframe(directory_path):\n",
    "    \"\"\"\n",
    "    Reads all Parquet files from a directory into a single pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - directory_path (str): Path to the directory containing Parquet files.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Combined DataFrame containing data from all Parquet files.\n",
    "    \"\"\"\n",
    "    # Use glob to get a list of all Parquet file paths in the specified directory\n",
    "    parquet_files = glob.glob(f\"{directory_path}/*.parquet\")\n",
    "\n",
    "    # Initialize an empty list to store individual DataFrames\n",
    "    dfs = []\n",
    "\n",
    "    # Loop through each Parquet file and read it into a DataFrame\n",
    "    for parquet_file in parquet_files:\n",
    "        # Read Parquet file into a DataFrame\n",
    "        df = pq.read_table(parquet_file).to_pandas()\n",
    "\n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    all_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "\n",
    "# Specify the directory where Parquet files are located\n",
    "directory_to_save_files = \"/usr/local/kafka/data/files/batch\"\n",
    "microbatch_parquet_directory_name = find_microbatch_file(\n",
    "    directory=directory_to_save_files\n",
    ")\n",
    "directory_path = f\"{directory_to_save_files}/{microbatch_parquet_directory_name}/\"\n",
    "\n",
    "read_parquet_directory_into_dataframe(directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912aab18-669b-4d64-b6fb-31b3b6c71114",
   "metadata": {},
   "source": [
    "# SPARK MLLIB + SCIKIT LEARN\n",
    "## LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "07f88fea-b54c-4d62-9a6a-80bea1dd0b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel\n",
    "from pyspark.ml.evaluation import (\n",
    "    BinaryClassificationEvaluator,\n",
    "    MulticlassClassificationEvaluator,\n",
    "    RegressionEvaluator,\n",
    ")\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.functions import col, format_number, udf\n",
    "from pyspark.sql.types import FloatType, StringType, StructField, StructType\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6799c2-aa8c-4637-9922-3572221f7462",
   "metadata": {},
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e0f2e294-6a8a-4699-881d-a66c347b91de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Create a Spark session.\n",
    "\n",
    "    Returns:\n",
    "    - spark_session (SparkSession): A SparkSession object.\n",
    "    \"\"\"\n",
    "    spark_session = (\n",
    "        SparkSession.builder.master(\"local\")\n",
    "        .appName(\"scikit-learn-with-spark\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    return spark_session\n",
    "\n",
    "\n",
    "def create_dataframe_from_dataset(\n",
    "    dataset,\n",
    "    dataset_data_key=\"data\",\n",
    "    dataset_columns=[\"feature_1\", \"feature_2\", \"feature_3\", \"feature_4\"],\n",
    "    dataset_data_target=\"target\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a pandas DataFrame from a dataset dictionary.\n",
    "\n",
    "    Args:\n",
    "    - dataset (dict): The dataset dictionary containing data and target keys.\n",
    "    - dataset_data_key (str): The key in the dataset dictionary corresponding to the data.\n",
    "    - dataset_columns (list): List of column names for the DataFrame.\n",
    "    - dataset_data_target (str): The key in the dataset dictionary corresponding to the target.\n",
    "\n",
    "    Returns:\n",
    "    - df (DataFrame): A pandas DataFrame containing the data and target from the dataset dictionary.\n",
    "    \"\"\"\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(dataset[dataset_data_key], columns=dataset_columns)\n",
    "    df[dataset_data_target] = dataset[dataset_data_target]\n",
    "\n",
    "    display(df)\n",
    "\n",
    "\n",
    "def create_multiple_samples_datasets(\n",
    "    original_dataset, n_datasets, n_records, delay_time=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Create multiple datasets from an original dataset.\n",
    "\n",
    "    Args:\n",
    "    - original_dataset (Bunch): The original dataset from which to create new datasets.\n",
    "    - n_datasets (int): Number of datasets to create.\n",
    "    - n_records (int): Number of records in each new dataset.\n",
    "    - delay_time (int): Time to wait (in seconds) between creating each dataset.\n",
    "\n",
    "    Returns:\n",
    "    - datasets (list): A list of pandas DataFrames, each representing a new dataset.\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "\n",
    "    for dataset_index in range(n_datasets):\n",
    "        # Create a new dataset with n random records\n",
    "        indices = np.random.choice(\n",
    "            range(len(original_dataset.data)), size=n_records, replace=False\n",
    "        )\n",
    "        data = original_dataset.data[indices]\n",
    "        target = original_dataset.target[indices]\n",
    "        new_dataset = pd.DataFrame(data, columns=original_dataset.feature_names)\n",
    "        new_dataset[\"target\"] = target\n",
    "        datasets.append(new_dataset)\n",
    "\n",
    "        # Write new dataset to a CSV file\n",
    "        new_dataset.to_csv(f\"new_dataset_{dataset_index}.csv\", index=False)\n",
    "\n",
    "        # Wait for a specified delay time\n",
    "        sleep(delay_time)\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    Load the Iris dataset and display it as a pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - data_features (array): Array containing the features of the Iris dataset.\n",
    "    - target_classes (array): Array containing the target classes of the Iris dataset.\n",
    "    \"\"\"\n",
    "    # Load the Iris dataset\n",
    "    dataset = load_iris()\n",
    "    data_features, target_classes = dataset.data, dataset.target\n",
    "    key_column = \"data\"\n",
    "    features_columns = [\"feature_1\", \"feature_2\", \"feature_3\", \"feature_4\"]\n",
    "    taget_column = \"target\"\n",
    "\n",
    "    create_dataframe_from_dataset(\n",
    "        dataset,\n",
    "        dataset_data_key=key_column,\n",
    "        dataset_columns=features_columns,\n",
    "        dataset_data_target=taget_column,\n",
    "    )\n",
    "\n",
    "    return data_features, target_classes\n",
    "\n",
    "\n",
    "def load_dataset_from_csv(csv_file):\n",
    "    \"\"\"\n",
    "    Load the Iris dataset from a CSV file and display it as a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file containing the Iris dataset.\n",
    "\n",
    "    Returns:\n",
    "        - data_features (array): Array containing the features of the Iris dataset.\n",
    "        - target_classes (array): Array containing the target classes of the Iris dataset.\n",
    "    \"\"\"\n",
    "    # Load the Iris dataset from the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Separate the feature columns and the target column\n",
    "    features_columns = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "    target_column = 'class'\n",
    "\n",
    "    data_features = df[features_columns].values\n",
    "    target_classes = df[target_column].values\n",
    "\n",
    "    return data_features, target_classes\n",
    "    \n",
    "def split_dataset_train_testing(data_features, target_classes):\n",
    "    \"\"\"\n",
    "    Split the dataset into training and testing sets.\n",
    "\n",
    "    Args:\n",
    "    - data_features (array): Array containing the features of the dataset.\n",
    "    - target_classes (array): Array containing the target classes of the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - data_features_train_splitted (array): Features of the training set.\n",
    "    - data_features_test_splitted (array): Features of the testing set.\n",
    "    - target_classes_train_splitted (array): Target classes of the training set.\n",
    "    - target_classes_test_splitted (array): Target classes of the testing set.\n",
    "    \"\"\"\n",
    "    # Split the data into training and testing sets\n",
    "    (\n",
    "        data_features_train_splitted,\n",
    "        data_features_test_splitted,\n",
    "        target_classes_train_splitted,\n",
    "        target_classes_test_splitted,\n",
    "    ) = train_test_split(data_features, target_classes, test_size=0.2, random_state=42)\n",
    "\n",
    "    return (\n",
    "        data_features_train_splitted,\n",
    "        data_features_test_splitted,\n",
    "        target_classes_train_splitted,\n",
    "        target_classes_test_splitted,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_filtered_dataset(data_features_dataset, target_classes_dataset):\n",
    "    \"\"\"\n",
    "    Convert dataset features and target classes to appropriate formats.\n",
    "\n",
    "    Args:\n",
    "    - data_features_dataset (array): Array containing the features of the dataset.\n",
    "    - target_classes_dataset (array): Array containing the target classes of the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - data_features_filtered (list): List of feature vectors converted to Vectors.dense format.\n",
    "    - target_classes_filtered (list): List of target classes converted to floats.\n",
    "    \"\"\"\n",
    "    # Convert numpy values to Python floats\n",
    "    data_features_filtered = [\n",
    "        Vectors.dense(features) for features in data_features_dataset\n",
    "    ]\n",
    "    target_classes_filtered = [\n",
    "        float(label) for label in target_classes_dataset\n",
    "    ]  # Convert to float\n",
    "\n",
    "    return data_features_filtered, target_classes_filtered\n",
    "\n",
    "\n",
    "def create_spark_dataframe(\n",
    "    sparkml_session, data_features_dataset, target_classes_dataset\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a Spark DataFrame from the dataset features and target classes.\n",
    "\n",
    "    Args:\n",
    "    - data_features_dataset (array): Array containing the features of the dataset.\n",
    "    - target_classes_dataset (array): Array containing the target classes of the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - final_data_set (DataFrame): A Spark DataFrame containing the features and target classes.\n",
    "    \"\"\"\n",
    "    # Create Spark DataFrames from training data\n",
    "    data_set = list(zip(data_features_dataset, target_classes_dataset))\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"features\", VectorUDT(), True),\n",
    "            StructField(\"label\", FloatType(), True),\n",
    "        ]\n",
    "    )\n",
    "    final_data_set = sparkml_session.createDataFrame(data_set, schema=schema)\n",
    "\n",
    "    return final_data_set\n",
    "\n",
    "\n",
    "def create_ml_trained_model(train_df):\n",
    "    \"\"\"\n",
    "    Create and train a logistic regression model.\n",
    "\n",
    "    Args:\n",
    "    - train_df (DataFrame): DataFrame containing the training data.\n",
    "\n",
    "    Returns:\n",
    "    - model: Trained logistic regression model.\n",
    "    \"\"\"\n",
    "    # Create and train a logistic regression model\n",
    "    lr = LogisticRegression(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"label\",\n",
    "        maxIter=10,\n",
    "        regParam=0.3,\n",
    "        elasticNetParam=0.8,\n",
    "    )\n",
    "    model = lr.fit(train_df)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def model_prediction(model, test_df):\n",
    "    \"\"\"\n",
    "    Make predictions using a trained model and display the results.\n",
    "\n",
    "    Args:\n",
    "    - model: Trained machine learning model.\n",
    "    - test_df (DataFrame): DataFrame containing the test data.\n",
    "\n",
    "    Returns:\n",
    "    - prediction_result (DataFrame): DataFrame containing the prediction results.\n",
    "    \"\"\"\n",
    "    # Make predictions on the test data\n",
    "    prediction_result = model.transform(test_df)\n",
    "    # Format prediction column to display two decimal places\n",
    "    prediction_result = prediction_result.withColumn(\n",
    "        \"formatted_prediction\", format_number(\"prediction\", 2)\n",
    "    )\n",
    "\n",
    "    # Map numeric labels back to target names\n",
    "    target_names = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "    label_to_name_udf = udf(lambda label: target_names[int(label)], StringType())\n",
    "\n",
    "    # Create new columns with predicted and actual class names\n",
    "    prediction_result = prediction_result.withColumn(\n",
    "        \"predicted_class_name\", label_to_name_udf(\"prediction\")\n",
    "    )\n",
    "    prediction_result = prediction_result.withColumn(\n",
    "        \"actual_class_name\", label_to_name_udf(\"label\")\n",
    "    )\n",
    "\n",
    "    # Display the results\n",
    "    prediction_result.show()\n",
    "\n",
    "    return prediction_result\n",
    "\n",
    "\n",
    "def mse_rmse_mae_r2_avaluator(prediction_result):\n",
    "    \"\"\"\n",
    "    Evaluate regression metrics including MSE, RMSE, MAE, and R2 score.\n",
    "\n",
    "    Args:\n",
    "    - prediction_result (DataFrame): DataFrame containing the prediction results.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    regression_evaluator_mse = RegressionEvaluator(\n",
    "        predictionCol=\"prediction\", labelCol=\"label\", metricName=\"mse\"\n",
    "    )\n",
    "    print(\n",
    "        \"Mean Squared Error (Regression):\",\n",
    "        regression_evaluator_mse.evaluate(prediction_result),\n",
    "    )\n",
    "\n",
    "    regression_evaluator_rmse = RegressionEvaluator(\n",
    "        predictionCol=\"prediction\", labelCol=\"label\", metricName=\"rmse\"\n",
    "    )\n",
    "    print(\n",
    "        \"Root Mean Squared Error (Regression):\",\n",
    "        regression_evaluator_rmse.evaluate(prediction_result),\n",
    "    )\n",
    "\n",
    "    regression_evaluator_mae = RegressionEvaluator(\n",
    "        predictionCol=\"prediction\", labelCol=\"label\", metricName=\"mae\"\n",
    "    )\n",
    "    print(\n",
    "        \"Mean Absolute Error (Regression):\",\n",
    "        regression_evaluator_mae.evaluate(prediction_result),\n",
    "    )\n",
    "\n",
    "    # R2 Score (Regression)\n",
    "    r2_evaluator = RegressionEvaluator(\n",
    "        predictionCol=\"prediction\", labelCol=\"label\", metricName=\"r2\"\n",
    "    )\n",
    "    print(\"R2 Score (Regression):\", r2_evaluator.evaluate(prediction_result))\n",
    "\n",
    "\n",
    "def accuracy_f1Score_precision_recall_evaluator(prediction_result):\n",
    "    \"\"\"\n",
    "    Evaluate model performance using accuracy, F1 score, precision, and recall metrics.\n",
    "\n",
    "    Args:\n",
    "    - prediction_result (DataFrame): DataFrame containing the prediction results.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Accuracy\n",
    "    evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "        predictionCol=\"prediction\", labelCol=\"label\", metricName=\"accuracy\"\n",
    "    )\n",
    "    accuracy = evaluator_accuracy.evaluate(prediction_result)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    # F1 Score\n",
    "    evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "        predictionCol=\"prediction\", labelCol=\"label\", metricName=\"f1\"\n",
    "    )\n",
    "    f1_score = evaluator_f1.evaluate(prediction_result)\n",
    "    print(f\"F1 Score: {f1_score}\")\n",
    "\n",
    "    # Precision\n",
    "    evaluator_precision = MulticlassClassificationEvaluator(\n",
    "        predictionCol=\"prediction\", labelCol=\"label\", metricName=\"weightedPrecision\"\n",
    "    )\n",
    "    precision = evaluator_precision.evaluate(prediction_result)\n",
    "    print(f\"Precision: {precision}\")\n",
    "\n",
    "    # Recall\n",
    "    evaluator_recall = MulticlassClassificationEvaluator(\n",
    "        predictionCol=\"prediction\", labelCol=\"label\", metricName=\"weightedRecall\"\n",
    "    )\n",
    "    recall = evaluator_recall.evaluate(prediction_result)\n",
    "    print(f\"Recall: {recall}\")\n",
    "\n",
    "\n",
    "def area_under_curve_confusion_matrix_evaluator(prediction_result):\n",
    "    \"\"\"\n",
    "    Evaluate model performance using area under ROC curve and confusion matrix.\n",
    "\n",
    "    Args:\n",
    "    - prediction_result (DataFrame): DataFrame containing the prediction results.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Binary Classification Evaluator for ROC\n",
    "    binary_evaluator = BinaryClassificationEvaluator(\n",
    "        rawPredictionCol=\"prediction\", labelCol=\"label\"\n",
    "    )\n",
    "    roc_auc = binary_evaluator.evaluate(\n",
    "        prediction_result, {binary_evaluator.metricName: \"areaUnderROC\"}\n",
    "    )\n",
    "    print(f\"Area Under ROC: {roc_auc}\")\n",
    "\n",
    "    # Extract predictions and labels as RDD\n",
    "    prediction_and_label = prediction_result.select(\"prediction\", \"label\").rdd.map(\n",
    "        lambda row: (float(row[\"prediction\"]), float(row[\"label\"]))\n",
    "    )\n",
    "\n",
    "    # Create a MulticlassMetrics object\n",
    "    metrics = MulticlassMetrics(prediction_and_label)\n",
    "\n",
    "    # Output the confusion matrix\n",
    "    confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix)\n",
    "\n",
    "\n",
    "def create_pandas_dataframe(spark_dataframe):\n",
    "    \"\"\"\n",
    "    Convert a Spark DataFrame to a Pandas DataFrame and display the results.\n",
    "\n",
    "    Args:\n",
    "    - spark_dataframe (DataFrame): Spark DataFrame to be converted.\n",
    "\n",
    "    Returns:\n",
    "    - df_pandas (DataFrame): Pandas DataFrame containing the data from the Spark DataFrame.\n",
    "    \"\"\"\n",
    "    # Display the results like a pandas DataFrame\n",
    "    df_pandas = spark_dataframe.toPandas()\n",
    "    display(df_pandas)\n",
    "    return df_pandas\n",
    "\n",
    "\n",
    "def export_spark_model(model, model_path, overwrite=False):\n",
    "    \"\"\"\n",
    "    Export a trained Spark model to the specified path.\n",
    "\n",
    "    Args:\n",
    "    - model (pyspark.ml.Model): The trained Spark model to export.\n",
    "    - model_path (str): The path where the model will be saved.\n",
    "    - overwrite (bool, optional): Whether to overwrite the model if it already exists. Default is False.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Save the model to the specified path\n",
    "    if overwrite:\n",
    "        model.write().overwrite().save(model_path)\n",
    "    else:\n",
    "        model.write().save(model_path)\n",
    "\n",
    "    print(f\"Model saved successfully at: {model_path}\")\n",
    "\n",
    "\n",
    "def load_spark_model(model_path: str) -> Union[LogisticRegressionModel, None]:\n",
    "    \"\"\"\n",
    "    Load a trained Spark model from the specified path.\n",
    "\n",
    "    Args:\n",
    "    - model_path (str): The path from where the model will be loaded.\n",
    "\n",
    "    Returns:\n",
    "    - Union[LogisticRegressionModel, None]: The loaded Spark model or None if loading fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the model from the specified path\n",
    "        loaded_model = LogisticRegressionModel.load(model_path)\n",
    "        print(f\"Model loaded successfully from: {model_path}\")\n",
    "        return loaded_model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c4151b-579b-448e-9d97-dace939f4a29",
   "metadata": {},
   "source": [
    "# RUNNING ML PROCCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a494ea33-3b85-4c9e-b9eb-aec1fdc88d0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature_1  feature_2  feature_3  feature_4  target\n",
       "0          5.1        3.5        1.4        0.2       0\n",
       "1          4.9        3.0        1.4        0.2       0\n",
       "2          4.7        3.2        1.3        0.2       0\n",
       "3          4.6        3.1        1.5        0.2       0\n",
       "4          5.0        3.6        1.4        0.2       0\n",
       "..         ...        ...        ...        ...     ...\n",
       "145        6.7        3.0        5.2        2.3       2\n",
       "146        6.3        2.5        5.0        1.9       2\n",
       "147        6.5        3.0        5.2        2.0       2\n",
       "148        6.2        3.4        5.4        2.3       2\n",
       "149        5.9        3.0        5.1        1.8       2\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/12 06:04:02 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully at: /notebooks/spark_ml_model/dataset_iris\n",
      "Model loaded successfully from: /notebooks/spark_ml_model/dataset_iris\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+--------------------+--------------------+-----------------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|formatted_prediction|predicted_class_name|actual_class_name|\n",
      "+--------------------+-----+--------------------+--------------------+----------+--------------------+--------------------+-----------------+\n",
      "|[6.34,3.03,4.29,1.1]|  1.0|[-0.5405590083053...|[0.28644828464755...|       1.0|                1.00|          versicolor|       versicolor|\n",
      "|[6.08,2.62,3.93,1...|  1.0|[-0.5466408257513...|[0.27689074407750...|       2.0|                2.00|           virginica|       versicolor|\n",
      "|[5.24,2.6,4.74,1.11]|  1.0|[-0.6615694617345...|[0.26211433883602...|       1.0|                1.00|          versicolor|       versicolor|\n",
      "|[4.75,3.12,1.32,0...|  0.0|[0.47294985742744...|[0.54758662353233...|       0.0|                0.00|              setosa|           setosa|\n",
      "|[6.76,3.21,5.29,1...|  2.0|[-1.0773019594656...|[0.17272724854531...|       2.0|                2.00|           virginica|        virginica|\n",
      "|[5.66,2.61,3.88,1.3]|  1.0|[-0.4979637814550...|[0.28977899348067...|       1.0|                1.00|          versicolor|       versicolor|\n",
      "|[7.34,2.9,4.08,0.97]|  1.0|[-0.4435353588106...|[0.31025496235701...|       1.0|                1.00|          versicolor|       versicolor|\n",
      "|[7.72,3.23,6.03,2...|  2.0|[-1.3583358480790...|[0.13154999753580...|       2.0|                2.00|           virginica|        virginica|\n",
      "| [5.0,2.92,1.42,0.2]|  0.0|[0.50178183887837...|[0.55956091035566...|       0.0|                0.00|              setosa|           setosa|\n",
      "|[7.35,3.16,5.4,1.99]|  2.0|[-1.1190335437155...|[0.16603723307095...|       2.0|                2.00|           virginica|        virginica|\n",
      "|[5.42,3.47,5.41,2...|  2.0|[-1.1701838678711...|[0.15614141190010...|       2.0|                2.00|           virginica|        virginica|\n",
      "|[5.15,3.75,1.46,0...|  0.0|[0.47189971616239...|[0.55048494054843...|       0.0|                0.00|              setosa|           setosa|\n",
      "|[4.48,3.45,1.65,0...|  0.0|[0.39305266828325...|[0.52831027614364...|       0.0|                0.00|              setosa|           setosa|\n",
      "|[6.59,3.31,4.13,1...|  1.0|[-0.5407455069699...|[0.28293832367297...|       1.0|                1.00|          versicolor|       versicolor|\n",
      "|[5.22,3.27,6.14,2...|  2.0|[-1.4097740531015...|[0.12462871550908...|       2.0|                2.00|           virginica|        virginica|\n",
      "|[5.28,3.43,1.26,0...|  0.0|[0.55983506484929...|[0.57520218844506...|       0.0|                0.00|              setosa|           setosa|\n",
      "|[5.77,2.47,5.02,1.6]|  1.0|[-0.8933931025491...|[0.20870144707264...|       2.0|                2.00|           virginica|       versicolor|\n",
      "|[4.92,3.38,1.19,0.2]|  0.0|[0.56197790561058...|[0.57433893507614...|       0.0|                0.00|              setosa|           setosa|\n",
      "|[6.31,3.28,6.03,2...|  2.0|[-1.3648069285941...|[0.13047120321374...|       2.0|                2.00|           virginica|        virginica|\n",
      "|[6.79,3.15,6.45,1...|  2.0|[-1.3711928926468...|[0.13517439851462...|       2.0|                2.00|           virginica|        virginica|\n",
      "+--------------------+-----+--------------------+--------------------+----------+--------------------+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Mean Squared Error (Regression): 0.15\n",
      "Root Mean Squared Error (Regression): 0.3872983346207417\n",
      "Mean Absolute Error (Regression): 0.15\n",
      "R2 Score (Regression): 0.7765492584745763\n",
      "Accuracy: 0.85\n",
      "F1 Score: 0.8416971086605998\n",
      "Precision: 0.8980095420576563\n",
      "Recall: 0.8500000000000001\n",
      "Area Under ROC: 0.9973333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[225.   0.   0.]\n",
      " [  2. 104.  88.]\n",
      " [  0.   0. 181.]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "      <th>rawPrediction</th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "      <th>formatted_prediction</th>\n",
       "      <th>predicted_class_name</th>\n",
       "      <th>actual_class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[6.34, 3.03, 4.29, 1.1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[-0.5405590083053435, -0.29034425050591495, -0...</td>\n",
       "      <td>[0.2864482846475523, 0.3678858759322093, 0.345...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[6.08, 2.62, 3.93, 1.41]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[-0.5466408257513566, -0.29034425050591495, -0...</td>\n",
       "      <td>[0.27689074407750475, 0.3577804671001449, 0.36...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[5.24, 2.6, 4.74, 1.11]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[-0.6615694617345729, -0.29034425050591495, -0...</td>\n",
       "      <td>[0.2621143388360299, 0.379937219626648, 0.3579...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[4.75, 3.12, 1.32, 0.37]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.4729498574274437, -0.29034425050591495, -0....</td>\n",
       "      <td>[0.5475866235323384, 0.25524568648885526, 0.19...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[6.76, 3.21, 5.29, 1.95]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[-1.0773019594656876, -0.29034425050591495, -0...</td>\n",
       "      <td>[0.1727272485453161, 0.37943050643351117, 0.44...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>[4.96, 3.66, 1.45, 0.47]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.40657059104700355, -0.29034425050591495, -0...</td>\n",
       "      <td>[0.5281608796017143, 0.263087346223548, 0.2087...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>[6.37, 3.22, 4.94, 1.66]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[-0.8918685817527137, -0.29034425050591495, -0...</td>\n",
       "      <td>[0.20757304550354802, 0.37879972506022336, 0.4...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>[4.9, 3.78, 1.37, 0.1]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.5472233429171827, -0.29034425050591495, -0....</td>\n",
       "      <td>[0.573495586965588, 0.24818703820020727, 0.178...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>[5.32, 3.14, 4.03, 1.17]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[-0.4951600624977246, -0.29034425050591495, -0...</td>\n",
       "      <td>[0.29391589807667134, 0.360722684968249, 0.345...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>[5.82, 2.68, 4.9, 1.43]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[-0.806982274658747, -0.29034425050591495, -0....</td>\n",
       "      <td>[0.22741006974530534, 0.381226194977996, 0.391...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     features  label  \\\n",
       "0     [6.34, 3.03, 4.29, 1.1]    1.0   \n",
       "1    [6.08, 2.62, 3.93, 1.41]    1.0   \n",
       "2     [5.24, 2.6, 4.74, 1.11]    1.0   \n",
       "3    [4.75, 3.12, 1.32, 0.37]    0.0   \n",
       "4    [6.76, 3.21, 5.29, 1.95]    2.0   \n",
       "..                        ...    ...   \n",
       "595  [4.96, 3.66, 1.45, 0.47]    0.0   \n",
       "596  [6.37, 3.22, 4.94, 1.66]    2.0   \n",
       "597    [4.9, 3.78, 1.37, 0.1]    0.0   \n",
       "598  [5.32, 3.14, 4.03, 1.17]    1.0   \n",
       "599   [5.82, 2.68, 4.9, 1.43]    1.0   \n",
       "\n",
       "                                         rawPrediction  \\\n",
       "0    [-0.5405590083053435, -0.29034425050591495, -0...   \n",
       "1    [-0.5466408257513566, -0.29034425050591495, -0...   \n",
       "2    [-0.6615694617345729, -0.29034425050591495, -0...   \n",
       "3    [0.4729498574274437, -0.29034425050591495, -0....   \n",
       "4    [-1.0773019594656876, -0.29034425050591495, -0...   \n",
       "..                                                 ...   \n",
       "595  [0.40657059104700355, -0.29034425050591495, -0...   \n",
       "596  [-0.8918685817527137, -0.29034425050591495, -0...   \n",
       "597  [0.5472233429171827, -0.29034425050591495, -0....   \n",
       "598  [-0.4951600624977246, -0.29034425050591495, -0...   \n",
       "599  [-0.806982274658747, -0.29034425050591495, -0....   \n",
       "\n",
       "                                           probability  prediction  \\\n",
       "0    [0.2864482846475523, 0.3678858759322093, 0.345...         1.0   \n",
       "1    [0.27689074407750475, 0.3577804671001449, 0.36...         2.0   \n",
       "2    [0.2621143388360299, 0.379937219626648, 0.3579...         1.0   \n",
       "3    [0.5475866235323384, 0.25524568648885526, 0.19...         0.0   \n",
       "4    [0.1727272485453161, 0.37943050643351117, 0.44...         2.0   \n",
       "..                                                 ...         ...   \n",
       "595  [0.5281608796017143, 0.263087346223548, 0.2087...         0.0   \n",
       "596  [0.20757304550354802, 0.37879972506022336, 0.4...         2.0   \n",
       "597  [0.573495586965588, 0.24818703820020727, 0.178...         0.0   \n",
       "598  [0.29391589807667134, 0.360722684968249, 0.345...         1.0   \n",
       "599  [0.22741006974530534, 0.381226194977996, 0.391...         2.0   \n",
       "\n",
       "    formatted_prediction predicted_class_name actual_class_name  \n",
       "0                   1.00           versicolor        versicolor  \n",
       "1                   2.00            virginica        versicolor  \n",
       "2                   1.00           versicolor        versicolor  \n",
       "3                   0.00               setosa            setosa  \n",
       "4                   2.00            virginica         virginica  \n",
       "..                   ...                  ...               ...  \n",
       "595                 0.00               setosa            setosa  \n",
       "596                 2.00            virginica         virginica  \n",
       "597                 0.00               setosa            setosa  \n",
       "598                 1.00           versicolor        versicolor  \n",
       "599                 2.00            virginica        versicolor  \n",
       "\n",
       "[600 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Create a Spark ML session\n",
    "spark_ml_session = create_spark_session()\n",
    "\n",
    "# Step 2: Load the dataset\n",
    "data_features, target_classes = load_dataset()\n",
    "\n",
    "# Step 2.1: Load the sample dataset for esternal source\n",
    "dataset_file = 'https://raw.githubusercontent.com/JorgeCardona/recursos/main/datasets/iris_dataset.csv'\n",
    "data_features, target_classes = load_dataset_from_csv(dataset_file)\n",
    "\n",
    "# Step 3: Split the dataset into training and testing sets\n",
    "(\n",
    "    data_features_train_splitted,\n",
    "    data_features_test_splitted,\n",
    "    target_classes_train_splitted,\n",
    "    target_classes_test_splitted,\n",
    ") = split_dataset_train_testing(data_features, target_classes)\n",
    "\n",
    "# Step 4: Filter the split datasets\n",
    "data_features_train, target_classes_train = get_filtered_dataset(\n",
    "    data_features_train_splitted, target_classes_train_splitted\n",
    ")\n",
    "data_features_test, target_classes_test = get_filtered_dataset(\n",
    "    data_features_test_splitted, target_classes_test_splitted\n",
    ")\n",
    "\n",
    "# Step 5: Create Spark train and avlidation dataframes from the filtered datasets\n",
    "# 5.1 Dataset for Training the Model \n",
    "train_df = create_spark_dataframe(\n",
    "    spark_ml_session, data_features_train, target_classes_train\n",
    ")\n",
    "# 5.2 Dataset for Validating the Model\n",
    "test_df = create_spark_dataframe(\n",
    "    spark_ml_session, data_features_test, target_classes_test\n",
    ")\n",
    "\n",
    "# Step 6: Train a machine learning model\n",
    "model_trained = create_ml_trained_model(train_df)\n",
    "\n",
    "# Step 7: Export the trained model to a specified location\n",
    "model_path = \"/notebooks/spark_ml_model/dataset_iris\"\n",
    "export_spark_model(model_trained, model_path, overwrite=True)\n",
    "\n",
    "# Step 8: Load the trained model from the specified location\n",
    "model = load_spark_model(model_path)\n",
    "\n",
    "# Step 9: Make predictions using the loaded model\n",
    "prediction_result = model_prediction(model, test_df)\n",
    "\n",
    "# Step 10: Evaluate the model's performance on specified evaluation metrics\n",
    "mse_rmse_mae_r2_avaluator(prediction_result)\n",
    "accuracy_f1Score_precision_recall_evaluator(prediction_result)\n",
    "area_under_curve_confusion_matrix_evaluator(prediction_result)\n",
    "\n",
    "# Step 11: Create a Pandas dataframe from the resulting Spark dataframe of predictions\n",
    "create_pandas_dataframe(spark_dataframe=prediction_result)\n",
    "\n",
    "# Step 12: Close the Spark ML session\n",
    "spark_ml_session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3606bce-4616-41a7-a4b4-d953ff968b0d",
   "metadata": {},
   "source": [
    "# STOP ZOOKEEPER AND BROKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5c47e9-ff80-4e5c-bce6-5f59bdc9cfa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commands = [\n",
    "    # Delete Topics\n",
    "    \"/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --delete --topic animals-topic-batch,animals-topic-streaming\",\n",
    "    # Stop Zookeeper\n",
    "    \"/usr/local/kafka/bin/zookeeper-server-stop.sh\",\n",
    "    # Stop Broker\n",
    "    \"/usr/local/kafka/bin/kafka-server-stop.sh\",\n",
    "]\n",
    "execute_commands(commands=commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c55a4a-ce6c-43aa-bc46-219f8c21af2b",
   "metadata": {},
   "source": [
    "# CLOSE SPARK SESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c34848-c0ee-42ff-acce-5da36868368b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb0f0c7-7b28-455b-8772-f03dbc4ac47d",
   "metadata": {},
   "source": [
    "# IRIS DATASET GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "89075290-83f2-4723-9df5-89bcc9c7e9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.13</td>\n",
       "      <td>3.49</td>\n",
       "      <td>1.63</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.07</td>\n",
       "      <td>3.56</td>\n",
       "      <td>1.73</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.16</td>\n",
       "      <td>3.03</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.31</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1.42</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.12</td>\n",
       "      <td>3.38</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>6.70</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.20</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>6.30</td>\n",
       "      <td>2.50</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>6.50</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.20</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>6.20</td>\n",
       "      <td>3.40</td>\n",
       "      <td>5.40</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>5.90</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.10</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "0                  5.13              3.49               1.63   \n",
       "1                  5.07              3.56               1.73   \n",
       "2                  5.16              3.03               1.61   \n",
       "3                  5.31              2.90               1.42   \n",
       "4                  5.12              3.38               1.52   \n",
       "...                 ...               ...                ...   \n",
       "2995               6.70              3.00               5.20   \n",
       "2996               6.30              2.50               5.00   \n",
       "2997               6.50              3.00               5.20   \n",
       "2998               6.20              3.40               5.40   \n",
       "2999               5.90              3.00               5.10   \n",
       "\n",
       "      petal width (cm)  class  \n",
       "0                 0.22      0  \n",
       "1                 0.16      0  \n",
       "2                 0.44      0  \n",
       "3                 0.35      0  \n",
       "4                 0.44      0  \n",
       "...                ...    ...  \n",
       "2995              2.30      2  \n",
       "2996              1.90      2  \n",
       "2997              2.00      2  \n",
       "2998              2.30      2  \n",
       "2999              1.80      2  \n",
       "\n",
       "[3000 rows x 5 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Initialize an empty list to store the DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Generate synthetic samples for each class\n",
    "for index in range(3):\n",
    "    class_data = X[y == index]\n",
    "\n",
    "    # Generate 400 new, synthetic samples based on the class\n",
    "    new_data = []\n",
    "    for i in range(4):\n",
    "        mean, std = class_data[:, i].mean(), class_data[:, i].std()\n",
    "        new_values = np.round(np.random.normal(mean, std, (950,)), 2)\n",
    "        new_data.append(new_values)\n",
    "\n",
    "    # Convert the new data to a DataFrame\n",
    "    df = pd.DataFrame(np.array(new_data).T, columns=['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'])\n",
    "    df['class'] = index\n",
    "\n",
    "    # Add the new DataFrame to the list\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "df_complete = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Remove duplicates based on the columns you want to consider unique (e.g., 'sepal_length', 'petal_length', 'sepal_width', 'petal_width')\n",
    "df_unique = df_complete.drop_duplicates(subset=['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)', 'class'], keep='first')\n",
    "\n",
    "# Add the original dataset to df_unique\n",
    "df_original = pd.DataFrame(data=X, columns=iris.feature_names)\n",
    "df_original['class'] = iris.target\n",
    "df_unique = pd.concat([df_unique, df_original], ignore_index=True)\n",
    "\n",
    "df_unique.to_csv('iris_dataset.csv', index=False)\n",
    "df_unique"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
