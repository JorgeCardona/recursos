{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8dc8402-9c6d-40cb-8650-72bc1cc599ec",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24aff42-ffc0-476e-a68b-7d76b437215f",
   "metadata": {},
   "source": [
    "# START ZOOKEPER SERVICE\n",
    "```\n",
    "/usr/local/kafka/bin/zookeeper-server-start.sh /usr/local/kafka/config/zookeeper.properties\n",
    "\n",
    "```\n",
    "# START KAFKA BROKERS\n",
    "```\n",
    "/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server1.properties\n",
    "/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server2.properties\n",
    "/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server3.properties\n",
    "/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server4.properties\n",
    "```\n",
    "\n",
    "# LISTS TOPICS\n",
    "```\n",
    "\r",
    "#/usr/local/kafka/bin/kafka-topics.sh --list --bootstrap-server localhost:909\n",
    "\n",
    "```2``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cfb762-517f-4751-9d95-470531a78e84",
   "metadata": {},
   "source": [
    "# DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9cf8395-7cce-4af4-8f47-4aa0e30f4cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/02 22:51:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Directory where JARs are located\n",
    "jars_directory = \"/usr/local/spark/jars/\"\n",
    "\n",
    "# List of JAR filenames\n",
    "jar_files = [\n",
    "    \"commons-pool2-2.11.1.jar\",\n",
    "    \"kafka-clients-3.3.2.jar\",\n",
    "    \"spark-sql-kafka-0-10_2.12-3.4.1.jar\",\n",
    "    \"spark-token-provider-kafka-0-10_2.12-3.4.1.jar\"\n",
    "]\n",
    "\n",
    "dependencies = \",\".join([os.path.join(jars_directory, jar) for jar in jar_files])\n",
    "\n",
    "# Configure Kafka connection\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "topic = \"animals-topic-batch\"\n",
    "\n",
    "\n",
    "# Create Spark session and add JARs\n",
    "spark_session = SparkSession.builder \\\n",
    "    .appName(\"WriteKafkaAnimals\") \\\n",
    "    .config(\"spark.jars\", dependencies) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e05947-be49-4ba6-a986-3f20fe11babe",
   "metadata": {},
   "source": [
    "# BATCH WRITING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56f14306-a6e2-48c6-8fe8-7926efd1ff2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                        (0 + 16) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Iterations, Data written to Kafka topic (animals-topic-batch).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def save_batch_data(spark_session, kafka_bootstrap_servers, topic, iterations=1):\n",
    "\n",
    "    # Create a sample DataFrame with animal data\n",
    "    data = [(\"zebra\", \"mammal\"), (\"koala\", \"marsupial\"), (\"cheetah\", \"feline\"),(\"dolphin\", \"mammal\"),\n",
    "            (\"parrot\", \"bird\"), (\"rhino\", \"mammal\"), (\"panda\", \"mammal\"), (\"kangaroo\", \"marsupial\"), \n",
    "            (\"panther\", \"feline\"), (\"chimpanzee\", \"primate\"), (\"hippo\", \"mammal\"), (\"eagle\", \"bird\"), \n",
    "            (\"orangutan\", \"primate\"), (\"bear\", \"mammal\"), (\"owl\", \"bird\"), (\"polar bear\", \"mammal\"), \n",
    "            (\"snake\", \"reptile\"), (\"hawk\", \"bird\"), (\"fox\", \"mammal\"), (\"turtle\", \"reptile\"), \n",
    "            (\"swan\", \"bird\"), (\"jaguar\", \"feline\"), (\"seagull\", \"bird\"), (\"gazelle\", \"mammal\")]\n",
    "    \n",
    "    columns = [\"name\", \"type\"]\n",
    "    \n",
    "    df_animals = spark_session.createDataFrame(data, columns)\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        # Write the DataFrame to Kafka topic\n",
    "        df_animals.selectExpr(\"name as key\", \"type as value\") \\\n",
    "            .write \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "            .option(\"topic\", topic) \\\n",
    "            .save()\n",
    "    \n",
    "    # Print a message indicating that the data has been written to the Kafka topic\n",
    "    print(f\"{iterations} Iterations, Data written to Kafka topic ({topic}).\")\n",
    "\n",
    "# Configure Kafka connection\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "topic = \"animals-topic-batch\"\n",
    "save_batch_data(spark_session, kafka_bootstrap_servers, topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18600232-19bf-42b8-a191-a096ba535b08",
   "metadata": {},
   "source": [
    "# BATCH READING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbc3e9cb-e24c-4366-a0da-27cb057aba45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/02 22:51:38 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "24/01/02 22:51:39 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/01/02 22:51:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/01/02 22:51:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/01/02 22:51:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/01/02 22:51:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/01/02 22:51:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/01/02 22:51:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/01/02 22:51:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/01/02 22:51:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/01/02 22:51:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/01/02 22:51:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/01/02 22:51:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/01/02 22:51:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/01/02 22:51:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/01/02 22:51:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/01/02 22:51:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/01/02 22:51:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/01/02 22:51:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/01/02 22:51:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/01/02 22:51:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/01/02 22:51:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------------+---------+------+-----------------------+-------------+\n",
      "|key       |value    |topic              |partition|offset|timestamp              |timestampType|\n",
      "+----------+---------+-------------------+---------+------+-----------------------+-------------+\n",
      "|koala     |marsupial|animals-topic-batch|0        |0     |2024-01-02 22:43:50.69 |0            |\n",
      "|chimpanzee|primate  |animals-topic-batch|0        |1     |2024-01-02 22:43:50.69 |0            |\n",
      "|bear      |mammal   |animals-topic-batch|0        |2     |2024-01-02 22:43:50.689|0            |\n",
      "|parrot    |bird     |animals-topic-batch|0        |3     |2024-01-02 22:43:50.689|0            |\n",
      "|hippo     |mammal   |animals-topic-batch|0        |4     |2024-01-02 22:43:50.69 |0            |\n",
      "|seagull   |bird     |animals-topic-batch|0        |5     |2024-01-02 22:43:50.689|0            |\n",
      "|orangutan |primate  |animals-topic-batch|0        |6     |2024-01-02 22:43:50.69 |0            |\n",
      "|panda     |mammal   |animals-topic-batch|0        |7     |2024-01-02 22:43:50.69 |0            |\n",
      "|turtle    |reptile  |animals-topic-batch|0        |8     |2024-01-02 22:43:50.689|0            |\n",
      "|jaguar    |feline   |animals-topic-batch|0        |9     |2024-01-02 22:43:50.69 |0            |\n",
      "|polar bear|mammal   |animals-topic-batch|0        |10    |2024-01-02 22:43:50.69 |0            |\n",
      "|dolphin   |mammal   |animals-topic-batch|0        |11    |2024-01-02 22:43:50.689|0            |\n",
      "|zebra     |mammal   |animals-topic-batch|0        |12    |2024-01-02 22:43:50.69 |0            |\n",
      "|snake     |reptile  |animals-topic-batch|0        |13    |2024-01-02 22:43:50.689|0            |\n",
      "|swan      |bird     |animals-topic-batch|0        |14    |2024-01-02 22:43:50.702|0            |\n",
      "|fox       |mammal   |animals-topic-batch|0        |15    |2024-01-02 22:43:50.69 |0            |\n",
      "|hawk      |bird     |animals-topic-batch|0        |16    |2024-01-02 22:43:50.702|0            |\n",
      "|kangaroo  |marsupial|animals-topic-batch|0        |17    |2024-01-02 22:43:50.69 |0            |\n",
      "|rhino     |mammal   |animals-topic-batch|0        |18    |2024-01-02 22:43:50.702|0            |\n",
      "|eagle     |bird     |animals-topic-batch|0        |19    |2024-01-02 22:43:50.701|0            |\n",
      "+----------+---------+-------------------+---------+------+-----------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define the hexadecimal decoding function\n",
    "@udf(returnType=StringType())\n",
    "def decode_hex(value):\n",
    "    try:\n",
    "        if isinstance(value, str):\n",
    "            return bytes.fromhex(value).decode('utf-8')\n",
    "        elif isinstance(value, bytearray):\n",
    "            return bytes(value).decode('utf-8')\n",
    "        else:\n",
    "            return str(value)\n",
    "    except (ValueError, UnicodeDecodeError):\n",
    "        return str(value)\n",
    "            \n",
    "def read_batch_data(spark_session, kafka_bootstrap_servers, topic):\n",
    "\n",
    "    # Try to read data from Kafka\n",
    "    try:\n",
    "        # Read data from Kafka\n",
    "        df_kafka = spark_session \\\n",
    "            .read \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "            .option(\"subscribe\", topic) \\\n",
    "            .load()\n",
    "    \n",
    "        # Decode hexadecimal values\n",
    "        df_decoded = df_kafka \\\n",
    "            .withColumn('key', decode_hex('key')) \\\n",
    "            .withColumn('value', decode_hex('value'))\n",
    "    \n",
    "        # Show the DataFrame with decoded data\n",
    "        df_decoded.show(truncate=False)\n",
    "    \n",
    "    except Exception as e:\n",
    "        if \"UnknownTopicOrPartitionException\" in str(e):\n",
    "            print(f\"The topic '{topic}' does not exist in the Kafka cluster.\")\n",
    "        else:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        # Stop the Spark session\n",
    "        None\n",
    "\n",
    "topic = \"animals-topic-batch\"\n",
    "read_batch_data(spark_session, kafka_bootstrap_servers, topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a48a04-8cb8-460b-b9dd-d1b1e601dff6",
   "metadata": {},
   "source": [
    "# STREAMING WRITING\n",
    "## IT WILL BE LISTENING TO THE \"animals-topic-batch\" TOPIC\n",
    "## WHEN NEW DATA ARRIVES, IT READS AND STORES THEM IN THE \"animals-topic-streaming\" TOPIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "050f2e11-cef4-4896-a5f4-ae75d574690a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/02 22:52:12 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/01/02 22:52:12 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "def read_streaming_data(spark_session, kafka_bootstrap_servers, input_topic, output_topic, checkpoint_location):\n",
    "\n",
    "    #spark_session.sparkContext.setLogLevel(\"DEBUG\")    \n",
    "    # Read from Kafka in streaming mode\n",
    "    kafkaStream = spark_session \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "        .option(\"subscribe\", input_topic) \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .load()\n",
    "    \n",
    "    # Perform some transformation on the data (here we are simply renaming the columns)\n",
    "    transformedStream = kafkaStream.selectExpr(\"CAST(key AS STRING) as key\", \"CAST(value AS STRING) as value\")\n",
    "    \n",
    "    query = transformedStream \\\n",
    "        .writeStream \\\n",
    "        .option(\"failOnDataLoss\", \"false\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"truncate\", \"false\") \\\n",
    "        .option(\"checkpointLocation\", checkpoint_location) \\\n",
    "        .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "        .option(\"topic\", output_topic) \\\n",
    "        .start()\n",
    "\n",
    "# Configure Kafka connection\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "input_topic = \"animals-topic-batch\"\n",
    "output_topic = \"animals-topic-streaming\"\n",
    "# Checkpoint directory within the Kafka directory\n",
    "checkpoint_location = \"/usr/local/kafka/checkpoint\"\n",
    "\n",
    "read_streaming_data(spark_session, kafka_bootstrap_servers, input_topic, output_topic, checkpoint_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c291aa23-ce90-4c49-afff-cf522745f3db",
   "metadata": {},
   "source": [
    "## MASSIVE DATA INSERTION TO KAFKA FOR STREAMING READ\r\n",
    "### COPY, PASTE, AND RUN THE FOLLOWING CODE IN ANOTHER NOTEBOOK TO OBSERVE STREAMING REA\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c759bab-bb0c-4279-8602-b24a5a6281c6",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "```\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from time import sleep\n",
    "\n",
    "# Directory where JARs are located\n",
    "jars_directory = \"/usr/local/spark/jars/\"\n",
    "\n",
    "# List of JAR filenames\n",
    "jar_files = [\n",
    "    \"commons-pool2-2.11.1.jar\",\n",
    "    \"kafka-clients-3.3.2.jar\",\n",
    "    \"spark-sql-kafka-0-10_2.12-3.4.1.jar\",\n",
    "    \"spark-token-provider-kafka-0-10_2.12-3.4.1.jar\"\n",
    "]\n",
    "\n",
    "dependencies = \",\".join([os.path.join(jars_directory, jar) for jar in jar_files])\n",
    "\n",
    "# Configure Kafka connection\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "topic = \"animals-topic-batch\"\n",
    "\n",
    "\n",
    "# Create Spark session and add JARs\n",
    "spark_session = SparkSession.builder \\\n",
    "    .appName(\"WriteKafkaAnimals\") \\\n",
    "    .config(\"spark.jars\", dependencies) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "def save_batch_data(spark_session, kafka_bootstrap_servers, topic, iterations=1):\n",
    "\n",
    "    # Create a sample DataFrame with animal data\n",
    "    data = [(\"zebra\", \"mammal\"), (\"koala\", \"marsupial\"), (\"cheetah\", \"feline\"),(\"dolphin\", \"mammal\"),\n",
    "            (\"parrot\", \"bird\"), (\"rhino\", \"mammal\"), (\"panda\", \"mammal\"), (\"kangaroo\", \"marsupial\"), \n",
    "            (\"panther\", \"feline\"), (\"chimpanzee\", \"primate\"), (\"hippo\", \"mammal\"), (\"eagle\", \"bird\"), \n",
    "            (\"orangutan\", \"primate\"), (\"bear\", \"mammal\"), (\"owl\", \"bird\"), (\"polar bear\", \"mammal\"), \n",
    "            (\"snake\", \"reptile\"), (\"hawk\", \"bird\"), (\"fox\", \"mammal\"), (\"turtle\", \"reptile\"), \n",
    "            (\"swan\", \"bird\"), (\"jaguar\", \"feline\"), (\"seagull\", \"bird\"), (\"gazelle\", \"mammal\")]\n",
    "    \n",
    "    columns = [\"name\", \"type\"]\n",
    "    \n",
    "    df_animals = spark_session.createDataFrame(data, columns)\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        sleep(0.2)\n",
    "        # Write the DataFrame to Kafka topic\n",
    "        df_animals.selectExpr(\"name as key\", \"type as value\") \\\n",
    "            .write \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "            .option(\"topic\", topic) \\\n",
    "            .save()\n",
    "        print (f'Iteration {iteration}, completed!!!')\n",
    "    \n",
    "    # Print a message indicating that the data has been written to the Kafka topic\n",
    "    print(f\"{iterations} Iterations, Data written to Kafka topic ({topic}).\")\n",
    "\n",
    "     # Finally, stop the Spark session\n",
    "    spark_session.stop()\n",
    "\n",
    "save_batch_data(spark_session, kafka_bootstrap_servers, topic, iterations=100)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3268fa8-9fa5-49d3-87fd-d7db3c28bf07",
   "metadata": {},
   "source": [
    "# STREAMING READING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1deb07ac-ebf4-424f-b6d9-c9bce35d3a64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/02 22:52:33 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-c9f3230c-8441-4a1d-874b-94d086918c18. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/01/02 22:52:33 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/01/02 22:52:33 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----------+---------+\n",
      "|       key|    value|\n",
      "+----------+---------+\n",
      "|     koala|marsupial|\n",
      "|chimpanzee|  primate|\n",
      "|      bear|   mammal|\n",
      "|    parrot|     bird|\n",
      "|     hippo|   mammal|\n",
      "|   seagull|     bird|\n",
      "| orangutan|  primate|\n",
      "|     panda|   mammal|\n",
      "|    turtle|  reptile|\n",
      "|    jaguar|   feline|\n",
      "|polar bear|   mammal|\n",
      "|   dolphin|   mammal|\n",
      "|     zebra|   mammal|\n",
      "|     snake|  reptile|\n",
      "|      swan|     bird|\n",
      "|       fox|   mammal|\n",
      "|      hawk|     bird|\n",
      "|  kangaroo|marsupial|\n",
      "|     rhino|   mammal|\n",
      "|     eagle|     bird|\n",
      "+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+----------+---------+\n",
      "|       key|    value|\n",
      "+----------+---------+\n",
      "|  kangaroo|marsupial|\n",
      "|   panther|   feline|\n",
      "|       fox|   mammal|\n",
      "|   seagull|     bird|\n",
      "|   gazelle|   mammal|\n",
      "|     hippo|   mammal|\n",
      "|     zebra|   mammal|\n",
      "|     eagle|     bird|\n",
      "|      bear|   mammal|\n",
      "|       owl|     bird|\n",
      "| orangutan|  primate|\n",
      "|chimpanzee|  primate|\n",
      "|     koala|marsupial|\n",
      "|   cheetah|   feline|\n",
      "|    jaguar|   feline|\n",
      "|   dolphin|   mammal|\n",
      "|     snake|  reptile|\n",
      "|      hawk|     bird|\n",
      "|polar bear|   mammal|\n",
      "|     panda|   mammal|\n",
      "+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+----------+---------+\n",
      "|       key|    value|\n",
      "+----------+---------+\n",
      "| orangutan|  primate|\n",
      "|chimpanzee|  primate|\n",
      "|       fox|   mammal|\n",
      "|     koala|marsupial|\n",
      "|   cheetah|   feline|\n",
      "|    parrot|     bird|\n",
      "|     rhino|   mammal|\n",
      "|   dolphin|   mammal|\n",
      "|  kangaroo|marsupial|\n",
      "|   panther|   feline|\n",
      "|     snake|  reptile|\n",
      "|      hawk|     bird|\n",
      "|    turtle|  reptile|\n",
      "|      swan|     bird|\n",
      "|    jaguar|   feline|\n",
      "|     hippo|   mammal|\n",
      "|     eagle|     bird|\n",
      "+----------+---------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+----------+---------+\n",
      "|       key|    value|\n",
      "+----------+---------+\n",
      "|   seagull|     bird|\n",
      "|   gazelle|   mammal|\n",
      "|      bear|   mammal|\n",
      "|       owl|     bird|\n",
      "|polar bear|   mammal|\n",
      "|     panda|   mammal|\n",
      "|  kangaroo|marsupial|\n",
      "|   panther|   feline|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Configure Kafka connection\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "topic = \"animals-topic-streaming\"\n",
    "\n",
    "def read_streaming_data(spark_session,kafka_bootstrap_servers, topic):\n",
    "   \n",
    "    # Read from Kafka in streaming mode\n",
    "    kafkaStream = spark_session.readStream.format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "        .option(\"subscribe\", topic) \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .load()\n",
    "    \n",
    "    # Show the read content\n",
    "    query = kafkaStream.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"console\") \\\n",
    "        .start() \\\n",
    "    \n",
    "    # Wait for the stream to finish (adjust as needed)\n",
    "    #query.awaitTermination()\n",
    "    \n",
    "    # Finally, stop the Spark session (you can stop it after the stream finishes)\n",
    "    #spark_session.stop()\n",
    "\n",
    "read_streaming_data(spark_session, kafka_bootstrap_servers, topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b708c433-648f-4632-a377-5acc74631f73",
   "metadata": {},
   "source": [
    "# CLOSE SPARK SESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c34848-c0ee-42ff-acce-5da36868368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aff4651-3f53-4771-b42f-b73464cf69ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db2bce1-60cf-4a2d-b0c2-98a6b2ac8a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54424d5-ef13-44dc-936d-2423005c1293",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
