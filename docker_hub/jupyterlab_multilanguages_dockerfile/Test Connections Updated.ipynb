{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83c7715-16aa-4a79-bfbb-0e3d40f7327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_database_configuration(database_type = 'mysql', host = None, port = None, database = None, table = None, user = None, password = None, input_collection = None, output_collection = None):\n",
    "    databases = {\n",
    "        'mongodb': {\n",
    "            'app_name': 'MongoDB_Connector',\n",
    "            'format_type':'mongodb',\n",
    "            'host': host if database_type == 'mongodb' and host else 'host.docker.internal',\n",
    "            'port': port if database_type == 'mongodb' and port else 27017,\n",
    "            'user': user if database_type == 'mongodb' and user else 'admin',\n",
    "            'password': password if database_type == 'mongodb' and password else '12345678',\n",
    "            'database': database if database_type == 'mongodb' and database else 'spark',\n",
    "            'input_collection':  input_collection if database_type == 'mongodb' and table else 'users',\n",
    "            'output_collection': output_collection if database_type == 'mongodb' and table else 'users',\n",
    "            'driver': 'com.mongodb.spark.sql.DefaultSource',\n",
    "            'url': f\"mongodb://{user}:{password}@{host}:{port}\" if database_type == 'mongodb' and host and port else 'mongodb://admin:12345678@host.docker.internal:27017'\n",
    "            },\n",
    "        'postgres': {\n",
    "            'app_name': 'PostgreSQL_Connector',\n",
    "            'format_type':'jdbc',\n",
    "            'host': host if database_type == 'postgres' and host else 'host.docker.internal',\n",
    "            'port': port if database_type == 'postgres' and port else 5432,\n",
    "            'user': user if database_type == 'postgres' and user else 'admin',\n",
    "            'password': password if database_type == 'postgres' and password else '12345678',\n",
    "            'database': database if database_type == 'postgres' and database else 'spark',\n",
    "            'table': table if database_type == 'postgres' and table else 'users',\n",
    "            'schema': 'public',\n",
    "            'spark_jars': '/usr/local/spark/jars/postgresql-42.7.1.jar',\n",
    "            'driver': 'org.postgresql.Driver',\n",
    "            'url': f\"jdbc:postgresql://{host}:{port}/{database}\" if database_type == 'postgres' and host and port else 'jdbc:postgresql://host.docker.internal:5432/spark',\n",
    "            'properties': {\n",
    "                'user': user if database_type == 'postgres' and user else 'admin',\n",
    "                'password': password if database_type == 'postgres' and password else '12345678',\n",
    "                'driver': 'org.postgresql.Driver'\n",
    "                            }\n",
    "            },\n",
    "        'mysql': {\n",
    "            'app_name': 'MySQL_Connector',\n",
    "            'format_type':'jdbc',\n",
    "            'host': host if database_type == 'mysql' and host else 'host.docker.internal',\n",
    "            'port': port if database_type == 'mysql' and port else 3306,\n",
    "            'user': user if database_type == 'mysql' and user else 'admin',\n",
    "            'password': password if database_type == 'mysql' and password else '12345678',\n",
    "            'database': database if database_type == 'mysql' and database else 'spark',\n",
    "            'table': table if database_type == 'mysql' and table else 'users',\n",
    "            'spark_jars': '/usr/local/spark/jars/mysql-connector-j-8.2.0.jar',\n",
    "            'driver': 'com.mysql.cj.jdbc.Driver',\n",
    "            'url': f\"jdbc:mysql://{host}:{port}/{database}\" if database_type == 'mysql' and host and port else 'jdbc:mysql://host.docker.internal:3306/spark',\n",
    "            'properties': { \n",
    "                            'user': user if database_type == 'mysql' and user else 'admin', \n",
    "                            'password': password if database_type == 'mysql' and password else '12345678', \n",
    "                            'driver': 'com.mysql.cj.jdbc.Driver'\n",
    "            }\n",
    "                            }\n",
    "    }\n",
    "    return databases.get(database_type.lower(), databases.get('mysql'))\n",
    "\n",
    "def generate_sample_data():\n",
    "    data = [(1, \"Ana\"), (2, \"Cecilia\"), (3, \"Nathalie\"), (4, \"Diana\"), (5, \"Gabriela\"), (6, \"Angela\"), (7, \"Tatiana\"), (8, \"Lucia\"), (9, \"Maria\")]\n",
    "    columns = [\"Id\", \"Name\"]\n",
    "    return data, columns\n",
    "\n",
    "def insert_data_to_database(database_configuration, database_type=None):\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import monotonically_increasing_id\n",
    "    \n",
    "    app_name = database_configuration.get('app_name')\n",
    "    format_type = database_configuration.get('format_type')\n",
    "    database = database_configuration.get('database')\n",
    "    user = database_configuration.get('user')\n",
    "    password = database_configuration.get('password')\n",
    "    driver = database_configuration.get('driver')\n",
    "    url = database_configuration.get('url')\n",
    "    spark_session = SparkSession.builder.master('local').appName(app_name)\n",
    "    \n",
    "    data, columns = generate_sample_data()\n",
    "\n",
    "    try:\n",
    "        message = f'Records Inserted Successfully in {app_name}'\n",
    "        if database_type == 'mongodb':\n",
    "            collection = database_configuration.get('output_collection')\n",
    "            spark_session = spark_session.getOrCreate()\n",
    "            sampleDF = spark_session.createDataFrame(data, columns)\n",
    "            sampleDF_with_id = sampleDF.withColumn(\"id\", monotonically_increasing_id()) # add column 'id' to DataFrame\n",
    "\n",
    "            sampleDF_with_id.write.format(\"mongodb\") \\\n",
    "            .option(\"connection.uri\", url).option(\"database\", database) \\\n",
    "            .option(\"collection\", collection).mode(\"append\").save()\n",
    "        else: \n",
    "            dbtable = database_configuration.get('table')\n",
    "            spark_jars = database_configuration.get('spark_jars')\n",
    "            spark_session = spark_session.config(\"spark.jars\", spark_jars)\n",
    "            spark_session = spark_session.config(\"spark.jars\", spark_jars).getOrCreate()\n",
    "            sampleDF = spark_session.createDataFrame(data, columns)\n",
    "            sampleDF.write \\\n",
    "                .format(format_type).option(\"driver\", driver) \\\n",
    "                .option(\"url\", url).option(\"dbtable\", dbtable) \\\n",
    "                .option(\"user\", user).option(\"password\", password) \\\n",
    "                .mode(\"ignore\").mode(\"append\").save()\n",
    "    except Exception as e:\n",
    "        message = f\"Error inserting data: {str(e)}\"\n",
    "    finally:\n",
    "        spark_session.stop() # stop Spark session\n",
    "        return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcb33b0-e0ca-4ee6-bf47-1de7e77b5115",
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql_configuration = get_database_configuration(database_type = 'mysql')\n",
    "insert_data_to_database(mysql_configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e48619-ac98-4979-8435-d2dff2672372",
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_configuration = get_database_configuration(database_type = 'postgres')\n",
    "insert_data_to_database(postgres_configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ad035a-aa01-4cbb-9aea-f303f03e67c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongodb_configuration = get_database_configuration(database_type = 'mongodb')\n",
    "insert_data_to_database(database_configuration=mongodb_configuration, database_type = 'mongodb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c3d58f-8280-4056-b2dc-22511b5cfed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_from_database(database_type='mysql', host=None, port=None, database=None, table=None, user=None, password=None, input_collection=None, output_collection=None):\n",
    "    database_configuration = get_database_configuration(database_type=database_type, host=host, port=port, database=database, table=table, user=user, password=password, input_collection=input_collection, output_collection=output_collection)\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark_session =  SparkSession.builder.master('local').appName(f'Read data from {database_type}').getOrCreate()\n",
    "    properties = database_configuration.get('properties')\n",
    "    url = database_configuration.get('url')\n",
    "    try:\n",
    "        if database_type == 'mongodb':\n",
    "            database = database_configuration.get('database')\n",
    "            collection = database_configuration.get('input_collection')\n",
    "            result = spark_session.read.format(\"mongodb\").option(\"connection.uri\", url).option(\"database\", database).option(\"collection\", collection).load()\n",
    "        else:\n",
    "            table = database_configuration.get('table')\n",
    "            if database_configuration.get('schema'):\n",
    "                table = f\"{database_configuration.get('schema')}.{database_configuration.get('table')}\"\n",
    "            result = spark_session.read.jdbc(url=url, table=table, properties=properties)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading data: {str(e)}\")\n",
    "        result = None  # Another action you may want to take in case of an exception\n",
    "    finally:\n",
    "        result.printSchema()  # Print schema\n",
    "        result.show()  # Show rows\n",
    "        df = result.toPandas() # converts to pandas\n",
    "        spark_session.stop()\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c888e6-e3c0-4a54-99ae-0095f8969d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_data_from_database(database_type = 'mysql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ca956d-1466-4491-8771-1035744eb9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_data_from_database(database_type = 'postgres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3af9b2-e378-4aba-9bc8-ed30a1827b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_data_from_database(database_type = 'mongodb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91ce430-3041-40f2-a779-a83541a3484b",
   "metadata": {},
   "source": [
    "# EXTENSION SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd15ea8-def4-47ec-a1ea-ad90cef3d49f",
   "metadata": {},
   "source": [
    "# POSTGRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2990e5d-95ac-4e7f-907b-a55e366969a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "%sql postgresql://admin:12345678@host.docker.internal:5432/spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba386b4-dd4b-4aba-8d9d-a5581557a0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql query_result <<\n",
    "SELECT * \n",
    "    FROM users \n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0ffe4c-5dcb-4e18-a4d6-5da57f76d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = query_result.DataFrame()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124297be-f70c-4994-a9e9-5128c44b1077",
   "metadata": {},
   "source": [
    "# MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c3ce1f-0ce8-4a43-a5e5-810c8a88d298",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "%sql mysql://admin:12345678@host.docker.internal:3306/spark\n",
    "result = %sql SELECT * FROM users LIMIT 10;\n",
    "data = result.DataFrame()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71e61f7-cb8c-47f5-ade6-7b12b5e1bae4",
   "metadata": {},
   "source": [
    "# DUCK DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b77d37-a0c4-4aeb-839d-59f10eb7566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "if not Path(\"penguins.csv\").is_file():\n",
    "    urlretrieve(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\",\n",
    "                \"penguins.csv\")\n",
    "\n",
    "%load_ext sql\n",
    "%config SqlMagic.displaylimit = 5\n",
    "%sql duckdb://"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcb4cff-901f-4595-98d6-24524602de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT *\n",
    "FROM penguins.csv\n",
    "LIMIT 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e22e2cb-4e26-4a58-a2ff-bcdf7403f777",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql sqlite:// --alias second-db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd86dbc0-ad78-463e-97df-1bcabea3cf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql duckdb:// --save adelie\n",
    "SELECT *\n",
    "FROM penguins.csv\n",
    "WHERE species = 'Adelie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b55f45-9526-44c8-acf9-6e0560e2e6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql --save not_nulls --no-execute\n",
    "SELECT *\n",
    "FROM penguins.csv\n",
    "WHERE bill_length_mm IS NOT NULL\n",
    "AND bill_depth_mm IS NOT NULL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2971a53e-7163-4be7-8e2f-89bbe8b1105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sqlplot boxplot --column bill_length_mm bill_depth_mm --table not_nulls --with not_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1862714-da96-4dbc-9193-9de898e3de47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sqlplot histogram --column bill_length_mm bill_depth_mm --table not_nulls --with not_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd17d17-3879-4f7f-80f6-423063efba7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
