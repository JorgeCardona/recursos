{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bacc6dc0-56e3-4573-85cd-71ccfb10d19b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Asignar valor de parámetro en el notebook desde un widget de texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92baea1e-7f9b-4a6f-9592-b572f903892e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def assign_notebook_parameter_value_from_text_widget(widget_config: dict) -> None:\n",
    "    \"\"\"\n",
    "    Crea widgets de texto en Databricks y asigna sus valores a variables de Python en el notebook.\n",
    "\n",
    "    Esta función itera sobre un diccionario de widgets, creando cada widget de texto en Databricks y almacenando su\n",
    "    valor en una variable de Python correspondiente al nombre del widget.\n",
    "\n",
    "    Parámetros:\n",
    "    - widget_config (dict): Un diccionario donde cada clave es el nombre del widget, y el valor es otro diccionario\n",
    "                             que contiene 'value' (valor por defecto del widget) y 'label' (etiqueta del widget).\n",
    "                             Ejemplo:\n",
    "                             {\n",
    "                                 'redWineUrl': {'value': 'http://example.com/redWine.csv', 'label': 'URL del Vino Tinto'},\n",
    "                                 'saveDirectory': {'value': '/path/to/save', 'label': 'Directorio de Guardado' }\n",
    "                             }\n",
    "\n",
    "    Nota:\n",
    "    - Esta función no devuelve ningún valor. Modifica los widgets de Databricks y asigna los valores de los widgets\n",
    "      a las variables de Python correspondientes en el ámbito del notebook.\n",
    "    \"\"\"\n",
    "    for widget_name, widget_info in widget_config.items():\n",
    "        # Si el valor es una lista, convertirlo a un string\n",
    "        widget_value = json.dumps(widget_info['value']) if isinstance(widget_info['value'], list) else widget_info['value']\n",
    "        \n",
    "        # Crear el widget de texto\n",
    "        dbutils.widgets.text(widget_name, widget_value, widget_info['label'])\n",
    "\n",
    "    # Imprimir mensaje de éxito después de crear todos los widgets\n",
    "    print(\"Proceso completado: Todos los widgets de texto han sido creados y asignados correctamente.\")\n",
    "\n",
    "# Recuperar el valor del widget y convertirlo de nuevo a una lista\n",
    "def get_widget_value_as_list(widget_name: str) -> list:\n",
    "    \"\"\"\n",
    "    Recupera el valor de un widget de texto y lo convierte de nuevo a una lista de Python.\n",
    "\n",
    "    Parámetros:\n",
    "    - widget_name (str): Nombre del widget.\n",
    "\n",
    "    Retorno:\n",
    "    - list: Lista de valores del widget.\n",
    "    \"\"\"\n",
    "    widget_value = dbutils.widgets.get(widget_name)\n",
    "    \n",
    "    # Convertir el valor del widget de vuelta a una lista\n",
    "    try:\n",
    "        # Intentar convertir el valor a una lista\n",
    "        return json.loads(widget_value)\n",
    "    except json.JSONDecodeError:\n",
    "        # Si no es una lista, devolverlo como un solo valor\n",
    "        return [widget_value]\n",
    "    \n",
    "def remove_assign_notebook_parameter_value_from_widget():\n",
    "    \"\"\"\n",
    "    Elimina todos los widgets en el notebook de Databricks.\n",
    "    \"\"\"\n",
    "    \n",
    "    dbutils.widgets.removeAll()\n",
    "    print(\"Todos los widgets han sido eliminados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9f1635a-8a54-4ba8-9499-f9f35b000548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado: Todos los widgets de texto han sido creados y asignados correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Definir la URL base para los archivos CSV y JSON\n",
    "# Estas URLs apuntan a los repositorios en GitHub donde se encuentran los datasets\n",
    "url_base_csv = 'https://raw.githubusercontent.com/JorgeCardona/data-collection-json-csv-sql/refs/heads/main/csv/wine'\n",
    "url_base_json = 'https://raw.githubusercontent.com/JorgeCardona/data-collection-json-csv-sql/refs/heads/main/json/wine'\n",
    "\n",
    "# Definir la ruta común para el directorio de guardado de los datasets\n",
    "# Este es el directorio en Databricks donde se almacenarán los archivos descargados\n",
    "save_directory = '/FileStore/tables'  # Sin la barra final\n",
    "\n",
    "# Definir los nombres de los datasets en formato snake_case\n",
    "# Estos son los nombres de los archivos que se utilizarán para los datasets\n",
    "dataset_1_name = \"winequality-red.csv\"\n",
    "dataset_2_name = \"winequality-white.json\"\n",
    "\n",
    "# Configuración de widgets con valores y etiquetas para los datasets\n",
    "# Crear un diccionario `widget_config` que asigna valores y etiquetas a los widgets utilizados\n",
    "widget_config = {\n",
    "    \"save_directory\": {\"value\": f\"{save_directory}\", \"label\": f\"save_directory ({save_directory})\"},\n",
    "    \"dataset_1_name\": {\"value\": f\"{dataset_1_name}\", \"label\": f\"Dataset name ({dataset_1_name})\"},\n",
    "    \"dataset_2_name\": {\"value\": f\"{dataset_2_name}\", \"label\": f\"Dataset name ({dataset_2_name})\"},\n",
    "    \"dataset1\": {\"value\": f\"{url_base_csv}/{dataset_1_name}\", \"label\": f\"Dataset 1 URL ({dataset_1_name})\"},\n",
    "    \"dataset2\": {\"value\": f\"{url_base_json}/{dataset_2_name}\", \"label\": f\"Dataset 2 URL ({dataset_2_name})\"},\n",
    "    \"saveDirectory\": {\"value\": save_directory, \"label\": \"Save Directory\"},\n",
    "    \"dataset1FileLocation\": {\"value\": f\"{save_directory}/{dataset_1_name}\", \"label\": f\"Dataset 1 File Location ({dataset_1_name})\"},\n",
    "    \"dataset2FileLocation\": {\"value\": f\"{save_directory}/{dataset_2_name}\", \"label\": f\"Dataset 2 File Location ({dataset_2_name})\"},\n",
    "    \"fileTypeDaset1\": {\"value\": \"csv\", \"label\": \"File Type csv\"},\n",
    "    \"fileTypeDaset2\": {\"value\": \"json\", \"label\": \"File Type json\"},\n",
    "    \"inferSchema\": {\"value\": \"true\", \"label\": \"Infer Schema\"},\n",
    "    \"firstRowIsHeader\": {\"value\": \"true\", \"label\": \"First Row Is Header\"},\n",
    "    \"delimiter\": {\"value\": \",\", \"label\": \"Delimiter\"},\n",
    "    \"bronze\": {\"value\": \"db_bronze\", \"label\": \"Data Base Bronze\"},\n",
    "    \"silver\": {\"value\": \"db_silver\", \"label\": \"Data Base Silver\"},\n",
    "    \"gold\": {\"value\": \"db_gold\", \"label\": \"Data Base Gold\"},\n",
    "    \"dataset_1_table\": {\"value\": \"red_wine\", \"label\": \"Table Name Dataset 1\"},\n",
    "    \"dataset_2_table\": {\"value\": \"white_wine\", \"label\": \"Table Name Dataset 2\"},\n",
    "    \"partition_columns\": {\"value\": [\"quality\", \"free sulfur dioxide\"], \"label\": \"Columns to partitioning datasets\"}\n",
    "}\n",
    "\n",
    "# Llamar a la función `assign_notebook_parameter_value_from_text_widget` con el diccionario `widget_config`\n",
    "# Esto asigna los valores y etiquetas configurados a los widgets del notebook\n",
    "assign_notebook_parameter_value_from_text_widget(widget_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e8e5c05-db73-4a60-aefe-5fdf48c9eca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# FUNCIONES PARA DESCARGAR, LISTAR Y ELIMINAR ARCHIVOS EN UN DIRECTORIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66ce439e-ecd9-4a44-94bb-f2c8a3aa7abc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# Función para descargar y guardar los archivos en databricks\n",
    "\n",
    "def download_dataset_and_store_in_dbfs(url: str, save_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Descarga un archivo desde una URL y lo guarda en el Databricks File System (DBFS).\n",
    "\n",
    "    Esta función descarga el contenido de un archivo desde una URL dada y lo guarda en una ruta dentro de DBFS.\n",
    "\n",
    "    Parámetros:\n",
    "    - url (str): URL desde donde se descargará el archivo.\n",
    "    - save_path (str): Ruta completa en DBFS donde se guardará el archivo descargado.\n",
    "\n",
    "    Excepciones:\n",
    "    - requests.exceptions.RequestException: Si ocurre un error durante la descarga del archivo.\n",
    "    - Exception: Para cualquier otro error inesperado, como problemas al guardar el archivo en DBFS.\n",
    "\n",
    "    Ejemplo de uso:\n",
    "    ```python\n",
    "    download_dataset_and_store_in_dbfs(\"https://example.com/file.csv\", \"/dbfs/tmp/data/file.csv\")\n",
    "    ```\n",
    "\n",
    "    El archivo será guardado en la ubicación especificada dentro de DBFS.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Descargar el contenido del archivo\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Verifica si la solicitud fue exitosa\n",
    "\n",
    "        # Usar dbutils.fs.put para guardar el contenido en DBFS\n",
    "        dbutils.fs.put(save_path, response.text, overwrite=True)\n",
    "\n",
    "        print(f\"Archivo guardado correctamente en {save_path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error al descargar el archivo desde {url}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error inesperado al guardar el archivo en DBFS: {e}\")\n",
    "\n",
    "\n",
    "def list_files_in_directory_path(directory_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Lista los archivos dentro de un directorio especificado. Si no hay archivos, imprime un mensaje indicando\n",
    "    que el directorio está vacío.\n",
    "\n",
    "    Parámetros:\n",
    "    - directory_path (str): Ruta completa del directorio cuyo contenido se desea listar.\n",
    "\n",
    "    Retorno:\n",
    "    - None: Esta función no retorna ningún valor, solo imprime el contenido del directorio o un mensaje de vacío.\n",
    "    \"\"\"\n",
    "    files = dbutils.fs.ls(directory_path)\n",
    "    if not files:\n",
    "        print(f\"No hay archivos en el directorio {directory_path}.\")\n",
    "    else:\n",
    "        print(f\"Archivos en el directorio {directory_path}:\")\n",
    "        for file in files:\n",
    "            print(file.path)\n",
    "\n",
    "def delete_all_files_in_directory_path(directory_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Elimina todos los archivos dentro de un directorio especificado. Si el directorio está vacío, imprime un mensaje \n",
    "    indicando que no hay archivos para eliminar.\n",
    "\n",
    "    Parámetros:\n",
    "    - directory_path (str): Ruta completa del directorio cuyo contenido se desea eliminar.\n",
    "\n",
    "    Retorno:\n",
    "    - None: Esta función no retorna ningún valor, solo imprime el estado de eliminación de los archivos.\n",
    "    \"\"\"\n",
    "    # Listar todos los archivos en el directorio\n",
    "    files = dbutils.fs.ls(directory_path)\n",
    "\n",
    "    if not files:\n",
    "        print(f\"No hay archivos para eliminar en el directorio {directory_path}.\\n\")\n",
    "    else:\n",
    "        # Eliminar cada archivo encontrado\n",
    "        print(f\"Eliminando archivos en el directorio {directory_path}...\\n\")\n",
    "        for file in files:\n",
    "            dbutils.fs.rm(file.path, recurse=True)\n",
    "            print(f\"Archivo eliminado: {file.path}\")\n",
    "        \n",
    "        print(f\"\\nTodos los archivos en el directorio {directory_path} han sido eliminados.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f01074d6-5d25-4886-a795-9a9b573d499f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# FUNCION PARA EJECUTAR LOS QUERIES EN SPARKSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33bb2af6-8277-488b-aaac-59a9efdc581f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_database(database_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Crea una base de datos en Databricks si no existe ya.\n",
    "\n",
    "    Parámetros:\n",
    "    - database_name (str): Nombre de la base de datos a crear.\n",
    "\n",
    "    Retorno:\n",
    "    - None\n",
    "\n",
    "    Ejemplo de uso:\n",
    "    ```python\n",
    "    create_database(\"nombre_de_la_base_de_datos\")\n",
    "    ```\n",
    "    \"\"\"\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "    print(f\"La base de datos '{database_name}' ha sido creada exitosamente (o ya existía).\")\n",
    "\n",
    "\n",
    "def drop_database(database_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Elimina una base de datos en Databricks si existe.\n",
    "\n",
    "    Parámetros:\n",
    "    - database_name (str): Nombre de la base de datos que se desea eliminar.\n",
    "\n",
    "    Retorno:\n",
    "    - None\n",
    "\n",
    "    Nota:\n",
    "    Esta función elimina la base de datos especificada únicamente si existe. Para eliminar una base de datos que contenga\n",
    "    tablas u objetos, utiliza la opción CASCADE dentro del comando SQL.\n",
    "\n",
    "    Ejemplo de uso:\n",
    "    ```python\n",
    "    drop_database(\"nombre_de_la_base_de_datos\")\n",
    "    ```\n",
    "    \"\"\"\n",
    "    spark.sql(f\"DROP DATABASE IF EXISTS {database_name} CASCADE\")\n",
    "    print(f\"La base de datos '{database_name}' ha sido eliminada exitosamente (si existía).\")\n",
    "\n",
    "def execute_spark_sql_query(query: str):\n",
    "    \"\"\"\n",
    "    Ejecuta una consulta SQL en Spark.\n",
    "\n",
    "    Parámetros:\n",
    "    - query (str): Consulta SQL a ejecutar.\n",
    "\n",
    "    Retorno:\n",
    "    - pyspark.sql.dataframe.DataFrame o None: \n",
    "        - Si la consulta produce un resultado (como SELECT), retorna un DataFrame.\n",
    "        - Si la consulta no produce un resultado (como UPDATE o DELETE), retorna None.\n",
    "\n",
    "    Nota:\n",
    "    La función asume que la consulta SQL está correctamente formada y que se puede ejecutar en el entorno de Spark.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Ejecutando consulta SQL:\\n{query}\")\n",
    "        # Ejecutar el query SQL\n",
    "        result = spark.sql(query)\n",
    "        \n",
    "        # Determinar si la consulta devuelve un DataFrame o no\n",
    "        if result.isStreaming or hasattr(result, \"count\"):  # Verifica si tiene un resultado tangible\n",
    "            print(f\"La consulta SQL se ejecutó exitosamente. El número de registros obtenidos es: {result.count()}\")\n",
    "            return result\n",
    "        else:\n",
    "            print(\"La consulta SQL se ejecutó exitosamente. No se generó un resultado para mostrar.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error al ejecutar la consulta SQL:\\n{query}\\nError: {e}\")\n",
    "        return None\n",
    "    \n",
    "def list_tables_in_database(database_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Lista y muestra todas las tablas en una base de datos específica en Databricks.\n",
    "\n",
    "    Parámetros:\n",
    "    - database_name (str): Nombre de la base de datos de la cual se quieren listar las tablas.\n",
    "\n",
    "    Retorno:\n",
    "    - None\n",
    "\n",
    "    Ejemplo de uso:\n",
    "    ```python\n",
    "    list_tables_in_database(\"mi_base_de_datos\")\n",
    "    ```\n",
    "    \"\"\"\n",
    "    tablas = spark.sql(f\"SHOW TABLES IN {database_name}\")\n",
    "    display(tablas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89b143c2-7996-459b-bc4d-9366e14975f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# FUNCIONES PARA GUARDAR LOS DATAFRAMES COMOL DELTA Y PARQUET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c15de1e-5389-4a81-8630-ef0b70656226",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def standardize_column_names_from_list(column_names: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Estandariza los nombres de las columnas reemplazando caracteres no alfanuméricos\n",
    "    por guiones bajos (\"_\"). Esta transformación ayuda a evitar problemas de formato\n",
    "    en el procesamiento de datos.\n",
    "\n",
    "    Parámetros:\n",
    "    - column_names (List[str]): Lista de nombres de columnas a estandarizar.\n",
    "\n",
    "    Retorno:\n",
    "    - List[str]: Nueva lista con los nombres de las columnas estandarizados.\n",
    "    \"\"\"\n",
    "    standardized_columns = []\n",
    "    \n",
    "    for col_name in column_names:\n",
    "        # Reemplazar caracteres no alfanuméricos por \"_\"\n",
    "        new_col_name = ''.join([char if char.isalnum() or char == '_' else '_' for char in col_name])\n",
    "        standardized_columns.append(new_col_name)\n",
    "    \n",
    "    print(f\"Los nombres de las columnas han sido estandarizados exitosamente. Las columnas ahora usan solo caracteres alfanuméricos y guiones bajos.\")\n",
    "    return standardized_columns\n",
    "\n",
    "def standardize_column_names_from_spark_dataframe(spark_dataframe: \"pyspark.sql.dataframe.DataFrame\") -> \"pyspark.sql.dataframe.DataFrame\":\n",
    "    \"\"\"\n",
    "    Estandariza los nombres de las columnas de un DataFrame de Spark reemplazando caracteres no alfanuméricos\n",
    "    por guiones bajos (\"_\"). Esta transformación ayuda a evitar problemas de formato en el procesamiento de datos.\n",
    "\n",
    "    Parámetros:\n",
    "    - spark_dataframe (pyspark.sql.dataframe.DataFrame): DataFrame de Spark con los nombres de columnas a estandarizar.\n",
    "\n",
    "    Retorno:\n",
    "    - pyspark.sql.dataframe.DataFrame: Nuevo DataFrame de Spark con los nombres de columnas estandarizados.\n",
    "    \"\"\"\n",
    "    for col_name in spark_dataframe.columns:\n",
    "        # Reemplazar caracteres no alfanuméricos por \"_\"\n",
    "        new_col_name = ''.join([char if char.isalnum() or char == '_' else '_' for char in col_name])\n",
    "        spark_dataframe = spark_dataframe.withColumnRenamed(col_name, new_col_name)\n",
    "    \n",
    "    print(f\"Los nombres de las columnas han sido estandarizados exitosamente. Las columnas ahora usan solo caracteres alfanuméricos y guiones bajos.\")\n",
    "    return spark_dataframe\n",
    "\n",
    "def save_dataframe_as_delta_format(spark_dataframe: \"pyspark.sql.dataframe.DataFrame\", path: str, partition_keys: List[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Guarda un DataFrame de Spark en formato Delta en una ubicación específica de almacenamiento,\n",
    "    opcionalmente particionado por una o más columnas.\n",
    "\n",
    "    Parámetros:\n",
    "    - spark_dataframe (DataFrame): DataFrame de Spark a guardar.\n",
    "    - path (str): Ruta en el sistema de archivos donde se guardará el DataFrame en formato Delta.\n",
    "    - partition_keys (List[str], opcional): Lista de nombres de columnas por las que se particionará el DataFrame.\n",
    "\n",
    "    Retorno:\n",
    "    - None\n",
    "\n",
    "    Nota:\n",
    "    Esta función sobrescribe cualquier archivo existente en la ruta especificada.\n",
    "    \"\"\"\n",
    "    # Habilitar la fusión automática de esquemas (si es necesario)\n",
    "    spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "    # Estandarizar nombres de columnas (opcional)\n",
    "    spark_dataframe = standardize_column_names_from_spark_dataframe(spark_dataframe)\n",
    "    partition_keys = standardize_column_names_from_list(partition_keys)\n",
    "    # Guardar el DataFrame en formato Delta, particionado si se especifica\n",
    "    if partition_keys:\n",
    "        spark_dataframe.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .partitionBy(partition_keys) \\\n",
    "            .save(path)\n",
    "        print(f\"El DataFrame ha sido guardado en formato Delta en '{path}', particionado por {partition_keys}\")\n",
    "        return partition_keys\n",
    "    else:\n",
    "        spark_dataframe.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(path)\n",
    "        print(f\"El DataFrame ha sido guardado exitosamente en formato Delta en la ruta: {path}\")\n",
    "\n",
    "def save_dataframe_as_delta_table(spark_dataframe: \"pyspark.sql.dataframe.DataFrame\", \n",
    "                                  database_name: str, \n",
    "                                  table_name: str, \n",
    "                                  partition_by: list = None) -> None:\n",
    "    \"\"\"\n",
    "    Guarda un DataFrame de Spark como una tabla Delta en una base de datos de Databricks.\n",
    "\n",
    "    Parámetros:\n",
    "    - spark_dataframe (pyspark.sql.dataframe.DataFrame): DataFrame de Spark a guardar.\n",
    "    - database_name (str): Nombre de la base de datos donde se creará la tabla.\n",
    "    - table_name (str): Nombre de la tabla Delta a crear en la base de datos.\n",
    "    - partition_by (list): Lista de nombres de columnas por las cuales se debe particionar la tabla. Si no se proporciona, la tabla no será particionada.\n",
    "\n",
    "    Retorno:\n",
    "    - None\n",
    "\n",
    "    Nota:\n",
    "    Esta función sobrescribe cualquier tabla existente con el mismo nombre en la base de datos especificada.\n",
    "    \"\"\"\n",
    "    spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "    full_table_name = f\"{database_name}.{table_name}\"\n",
    "\n",
    "    # Estandarizar los nombres de las columnas (si es necesario)\n",
    "    spark_dataframe = standardize_column_names_from_spark_dataframe(spark_dataframe)\n",
    "\n",
    "    if partition_by:\n",
    "        # Estandarizar los nombres de las columnas en partition_by (si es necesario)\n",
    "        partition_by = standardize_column_names_from_list(partition_by)\n",
    "\n",
    "        # Guardar la tabla particionada por las columnas especificadas\n",
    "        spark_dataframe.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .partitionBy(*partition_by) \\\n",
    "            .saveAsTable(full_table_name)\n",
    "        print(f\"El DataFrame ha sido guardado exitosamente como tabla Delta en la base de datos: {database_name}, \"\n",
    "              f\"tabla: {table_name}, particionada por {partition_by}\")\n",
    "    else:\n",
    "        # Guardar la tabla sin particionamiento\n",
    "        spark_dataframe.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(full_table_name)\n",
    "        print(f\"El DataFrame ha sido guardado exitosamente como tabla Delta en la base de datos: {database_name}, \"\n",
    "              f\"tabla: {table_name}\")\n",
    "\n",
    "def save_dataframe_as_parquet_format(spark_dataframe: \"pyspark.sql.dataframe.DataFrame\", \n",
    "                                     path: str, \n",
    "                                     partition_by: list = None) -> None:\n",
    "    \"\"\"\n",
    "    Guarda un DataFrame de Spark en formato Parquet en una ubicación específica de almacenamiento,\n",
    "    opcionalmente particionado por columnas específicas.\n",
    "\n",
    "    Parámetros:\n",
    "    - spark_dataframe (pyspark.sql.dataframe.DataFrame): DataFrame de Spark a guardar.\n",
    "    - path (str): Ruta en el sistema de archivos donde se guardará el DataFrame en formato Parquet.\n",
    "    - partition_by (list): Lista de nombres de columnas por las cuales se debe particionar el archivo Parquet.\n",
    "\n",
    "    Retorno:\n",
    "    - None\n",
    "\n",
    "    Nota:\n",
    "    Esta función sobrescribe cualquier archivo existente en la ruta especificada.\n",
    "    \"\"\"\n",
    "    spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "    if partition_by:\n",
    "        # Guardar con particionamiento\n",
    "        spark_dataframe.write.format(\"parquet\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .partitionBy(partition_by) \\\n",
    "            .save(path)\n",
    "        print(f\"El DataFrame ha sido guardado en formato Parquet en '{path}', particionado por {partition_by}\")\n",
    "        return partition_by\n",
    "    else:\n",
    "        # Guardar sin particionamiento\n",
    "        spark_dataframe.write.format(\"parquet\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(path)\n",
    "        print(f\"El DataFrame ha sido guardado exitosamente en formato Parquet en la ruta: {path}\")\n",
    "\n",
    "\n",
    "def save_dataframe_as_parquet_table(spark_dataframe: \"pyspark.sql.dataframe.DataFrame\", \n",
    "                                    database_name: str, \n",
    "                                    table_name: str, \n",
    "                                    partition_by: list = None) -> None:\n",
    "    \"\"\"\n",
    "    Guarda un DataFrame de Spark como una tabla Parquet en una base de datos de Databricks.\n",
    "\n",
    "    Parámetros:\n",
    "    - spark_dataframe (pyspark.sql.dataframe.DataFrame): DataFrame de Spark a guardar.\n",
    "    - database_name (str): Nombre de la base de datos donde se creará la tabla.\n",
    "    - table_name (str): Nombre de la tabla Parquet a crear en la base de datos.\n",
    "    - partition_by (list): Lista de nombres de columnas por las cuales se debe particionar la tabla. Si no se proporciona, la tabla no será particionada.\n",
    "\n",
    "    Retorno:\n",
    "    - None\n",
    "\n",
    "    Nota:\n",
    "    Esta función sobrescribe cualquier tabla existente con el mismo nombre en la base de datos especificada.\n",
    "    \"\"\"\n",
    "    spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "    full_table_name = f\"{database_name}.{table_name}\"\n",
    "\n",
    "    if partition_by:\n",
    "        # Guardar la tabla particionada por las columnas especificadas\n",
    "        spark_dataframe.write.format(\"parquet\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .partitionBy(partition_by) \\\n",
    "            .saveAsTable(full_table_name)\n",
    "        print(f\"El DataFrame ha sido guardado exitosamente como tabla Parquet en la base de datos: {database_name}, \"\n",
    "              f\"tabla: {table_name}, particionada por {partition_by}\")\n",
    "    else:\n",
    "        # Guardar la tabla sin particionamiento\n",
    "        spark_dataframe.write.format(\"parquet\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(full_table_name)\n",
    "        print(f\"El DataFrame ha sido guardado exitosamente como tabla Parquet en la base de datos: {database_name}, \"\n",
    "              f\"tabla: {table_name}\")\n",
    "\n",
    "def save_dataframe_as_temp_table(spark_dataframe: \"pyspark.sql.dataframe.DataFrame\", \n",
    "                                 table_name: str, \n",
    "                                 partition_by: list = None) -> None:\n",
    "    \"\"\"\n",
    "    Guarda un DataFrame de Spark como una tabla temporal en Databricks.\n",
    "\n",
    "    Parámetros:\n",
    "    - spark_dataframe (pyspark.sql.dataframe.DataFrame): DataFrame de Spark a guardar como tabla temporal.\n",
    "    - table_name (str): Nombre de la tabla temporal a crear.\n",
    "    - partition_by (list): Lista de columnas por las cuales particionar los datos antes de guardarlos como tabla temporal (opcional).\n",
    "\n",
    "    Retorno:\n",
    "    - None\n",
    "\n",
    "    Nota:\n",
    "    Esta función crea o reemplaza una tabla temporal, que existe solo durante la sesión activa de Spark.\n",
    "    \"\"\"\n",
    "    spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "    \n",
    "    # Si se especifica particionamiento, particionamos el DataFrame antes de crear la tabla temporal\n",
    "    if partition_by:\n",
    "        spark_dataframe = spark_dataframe.repartition(*partition_by)\n",
    "\n",
    "    # Crear la vista temporal\n",
    "    spark_dataframe.createOrReplaceTempView(table_name)\n",
    "    print(f\"El DataFrame ha sido guardado exitosamente como tabla temporal: {table_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b53a438-55e6-42d0-9f1b-2a2ecbd5206c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# FUNCION PARA LEER LOS ARCHIVOS DESCARGADOS COMO SPARK DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36efc388-ad6a-432e-8514-24c2586de499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_data_to_spark_dataframe(file_location: str, \n",
    "                                 file_type: str = \"csv\", \n",
    "                                 infer_schema: bool = True, \n",
    "                                 first_row_is_header: bool = True, \n",
    "                                 delimiter: str = \",\",\n",
    "                                 is_json_array: bool = True) -> \"pyspark.sql.dataframe.DataFrame\":\n",
    "    \"\"\"\n",
    "    Carga un archivo en un DataFrame de Spark, con opciones configurables para tipos de archivo, inferencia de esquema,\n",
    "    encabezado y delimitador. Compatible con archivos CSV, Parquet, Delta, JSON, ORC y Avro.\n",
    "\n",
    "    Parámetros:\n",
    "    - file_location (str): La ubicación del archivo que se desea cargar.\n",
    "    - file_type (str): El tipo de archivo. Valores permitidos: 'csv', 'parquet', 'delta', 'json', 'orc', 'avro'. Por defecto es 'csv'.\n",
    "    - infer_schema (bool): Si se debe inferir el esquema automáticamente (aplicable solo para CSV y JSON). Por defecto es True.\n",
    "    - first_row_is_header (bool): Si la primera fila debe ser usada como encabezado (aplicable solo para CSV). Por defecto es True.\n",
    "    - delimiter (str): El delimitador para los archivos CSV. Por defecto es ','.\n",
    "    - is_json_array (bool): Si el archivo JSON es un array de objetos (por defecto es True).\n",
    "\n",
    "    Retorna:\n",
    "    - DataFrame: El DataFrame cargado con los datos del archivo.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Diccionario que mapea el tipo de archivo al método de carga correspondiente\n",
    "        format_options = {\n",
    "            \"csv\": {\n",
    "                \"format\": \"csv\",\n",
    "                \"options\": {\n",
    "                    \"inferSchema\": infer_schema,\n",
    "                    \"header\": first_row_is_header,\n",
    "                    \"sep\": delimiter\n",
    "                }\n",
    "            },\n",
    "            \"parquet\": {\"format\": \"parquet\", \"options\": {}},\n",
    "            \"delta\": {\"format\": \"delta\", \"options\": {}},\n",
    "            \"json\": {\n",
    "                \"format\": \"json\",\n",
    "                \"options\": {\"multiline\": \"true\" if is_json_array else \"false\", \"inferSchema\": infer_schema}  # Cambiar según si es un array\n",
    "            },\n",
    "            \"orc\": {\"format\": \"orc\", \"options\": {}},\n",
    "            \"avro\": {\"format\": \"avro\", \"options\": {}}\n",
    "        }\n",
    "\n",
    "        # Verificar si el tipo de archivo es soportado\n",
    "        if file_type not in format_options:\n",
    "            raise ValueError(f\"Tipo de archivo no soportado: {file_type}\")\n",
    "\n",
    "        # Obtener el formato y las opciones correspondientes\n",
    "        file_format = format_options[file_type][\"format\"]\n",
    "        options = format_options[file_type][\"options\"]\n",
    "\n",
    "        # Cargar el archivo en un DataFrame\n",
    "        if file_type == \"json\":\n",
    "            # Para archivos JSON, intentamos cargarlo con las opciones definidas (multiline si es un array)\n",
    "            df = spark.read.format(file_format).options(**options).json(file_location)\n",
    "            \n",
    "            # Verificar si el JSON es de una sola columna (_corrupt_record) y aplicar un 'flatten' si es necesario\n",
    "            if df.columns == [\"_corrupt_record\"]:\n",
    "                raise ValueError(\"Error al leer el archivo JSON. El archivo puede estar corrupto o no es un formato JSON válido.\")\n",
    "            \n",
    "        else:\n",
    "            # Para otros formatos de archivo, usamos el método general de carga\n",
    "            df = spark.read.format(file_format).options(**options).load(file_location)\n",
    "\n",
    "        print(f\"Archivo {file_type} cargado correctamente desde {file_location}\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar el archivo desde {file_location}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76cd56e2-4db5-477e-86d8-afc664e1d49d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# FUNCION PARA LISTAR LAS TABLAS DE LA BASE DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc8fc09e-7d6e-4f9c-a31b-e7a6f5e81359",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def list_temp_tables(show_global: bool = False) -> \"pyspark.sql.dataframe.DataFrame\":\n",
    "    \"\"\"\n",
    "    Lista las tablas temporales en la sesión de Spark y las devuelve como un DataFrame de Spark.\n",
    "\n",
    "    Parámetros:\n",
    "    - show_global (bool): Si es True, muestra tablas temporales globales (prefijadas con 'global_temp'). \n",
    "                          Si es False, muestra las tablas temporales locales de la sesión.\n",
    "\n",
    "    Retorno:\n",
    "    - pyspark.sql.dataframe.DataFrame: Un DataFrame con las tablas temporales disponibles.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if show_global:\n",
    "            # Listar tablas temporales globales\n",
    "            global_temp_tables = [\n",
    "                (f\"global_temp.{table.name}\", \"Global Temporary\")\n",
    "                for table in spark.catalog.listTables(\"global_temp\")\n",
    "            ]\n",
    "        else:\n",
    "            # Listar tablas temporales locales\n",
    "            temp_tables = [\n",
    "                (table.name, \"Local Temporary\")\n",
    "                for table in spark.catalog.listTables()\n",
    "                if table.isTemporary\n",
    "            ]\n",
    "        \n",
    "        # Combinar los resultados y convertirlos a un DataFrame de Spark\n",
    "        tables = global_temp_tables if show_global else temp_tables\n",
    "        if tables:\n",
    "            schema = [\"Table\", \"Type\"]\n",
    "            display(spark.createDataFrame(tables, schema=schema))\n",
    "        else:\n",
    "            print(\"No se encontraron tablas temporales.\")\n",
    "            schema = [\"Table\", \"Type\"]\n",
    "            display(spark.createDataFrame([], schema=schema))\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error al listar las tablas temporales: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11fa9fec-6c22-439d-be32-cf1769aedeec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DESCARGAR LOS DATASETS ORIGINALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11252095-238e-4b2e-8c17-22bc380a5ee8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 100951 bytes.\nArchivo guardado correctamente en /FileStore/tables/winequality-red.csv\nWrote 1707556 bytes.\nArchivo guardado correctamente en /FileStore/tables/winequality-white.json\nArchivos en el directorio /FileStore/tables:\ndbfs:/FileStore/tables/winequality-red.csv\ndbfs:/FileStore/tables/winequality-white.json\n"
     ]
    }
   ],
   "source": [
    "# Recuperar los valores de los widgets configurados en Databricks\n",
    "# Estos widgets permiten obtener URLs de descarga, nombres de archivos y rutas de almacenamiento dinámicamente\n",
    "\n",
    "# Obtener la URL del primer dataset (vino tinto) a partir del widget \"dataset1\"\n",
    "dataset1Url = dbutils.widgets.get(\"dataset1\")\n",
    "\n",
    "# Obtener la URL del segundo dataset (vino blanco) a partir del widget \"dataset2\"\n",
    "dataset2Url = dbutils.widgets.get(\"dataset2\")\n",
    "\n",
    "# Obtener el directorio donde se guardarán los archivos descargados (por ejemplo, en DBFS - Databricks File System)\n",
    "saveDirectory = dbutils.widgets.get(\"saveDirectory\")\n",
    "\n",
    "# Obtener los nombres de los archivos a partir de los widgets\n",
    "# Estos nombres se utilizarán al guardar los archivos en el almacenamiento local\n",
    "dataset_1_name = dbutils.widgets.get(\"dataset_1_name\")\n",
    "dataset_2_name = dbutils.widgets.get(\"dataset_2_name\")\n",
    "\n",
    "# Descargar y guardar los archivos de datasets en el sistema de archivos de Databricks (DBFS)\n",
    "# La función `download_dataset_and_store_in_dbfs` descarga el archivo desde la URL proporcionada\n",
    "# y lo guarda en la ruta especificada en DBFS\n",
    "download_dataset_and_store_in_dbfs(\n",
    "    dataset1Url, \n",
    "    os.path.join(saveDirectory, dataset_1_name)\n",
    ")\n",
    "\n",
    "download_dataset_and_store_in_dbfs(\n",
    "    dataset2Url, \n",
    "    os.path.join(saveDirectory, dataset_2_name)\n",
    ")\n",
    "\n",
    "# Listar todos los archivos presentes en el directorio especificado\n",
    "# La función `list_files_in_directory_path` muestra los archivos en la ruta `saveDirectory`\n",
    "list_files_in_directory_path(saveDirectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcbc2fa5-b0a4-49e7-ab54-07714d85e62d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PROCESAMIENTO DE DATOS PARA CAPA SILVER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99589348-9dca-4f28-beb4-cec025ae8bdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## LEER LOS DATASETS DESCARGADOS COMO SPARK DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e58e1276-23a2-4385-b7c4-6f5c2722d670",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo csv cargado correctamente desde /FileStore/tables/winequality-red.csv\nArchivo json cargado correctamente desde /FileStore/tables/winequality-white.json\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>fixed acidity</th><th>volatile acidity</th><th>citric acid</th><th>residual sugar</th><th>chlorides</th><th>free sulfur dioxide</th><th>total sulfur dioxide</th><th>density</th><th>pH</th><th>sulphates</th><th>alcohol</th><th>quality</th></tr></thead><tbody><tr><td>7.4</td><td>0.7</td><td>0.0</td><td>1.9</td><td>0.076</td><td>11.0</td><td>34.0</td><td>0.9978</td><td>3.51</td><td>0.56</td><td>9.4</td><td>5</td></tr><tr><td>7.8</td><td>0.88</td><td>0.0</td><td>2.6</td><td>0.098</td><td>25.0</td><td>67.0</td><td>0.9968</td><td>3.2</td><td>0.68</td><td>9.8</td><td>5</td></tr><tr><td>7.8</td><td>0.76</td><td>0.04</td><td>2.3</td><td>0.092</td><td>15.0</td><td>54.0</td><td>0.997</td><td>3.26</td><td>0.65</td><td>9.8</td><td>5</td></tr><tr><td>11.2</td><td>0.28</td><td>0.56</td><td>1.9</td><td>0.075</td><td>17.0</td><td>60.0</td><td>0.998</td><td>3.16</td><td>0.58</td><td>9.8</td><td>6</td></tr><tr><td>7.4</td><td>0.7</td><td>0.0</td><td>1.9</td><td>0.076</td><td>11.0</td><td>34.0</td><td>0.9978</td><td>3.51</td><td>0.56</td><td>9.4</td><td>5</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         7.4,
         0.7,
         0.0,
         1.9,
         0.076,
         11.0,
         34.0,
         0.9978,
         3.51,
         0.56,
         9.4,
         5
        ],
        [
         7.8,
         0.88,
         0.0,
         2.6,
         0.098,
         25.0,
         67.0,
         0.9968,
         3.2,
         0.68,
         9.8,
         5
        ],
        [
         7.8,
         0.76,
         0.04,
         2.3,
         0.092,
         15.0,
         54.0,
         0.997,
         3.26,
         0.65,
         9.8,
         5
        ],
        [
         11.2,
         0.28,
         0.56,
         1.9,
         0.075,
         17.0,
         60.0,
         0.998,
         3.16,
         0.58,
         9.8,
         6
        ],
        [
         7.4,
         0.7,
         0.0,
         1.9,
         0.076,
         11.0,
         34.0,
         0.9978,
         3.51,
         0.56,
         9.4,
         5
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "fixed acidity",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "volatile acidity",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "citric acid",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "residual sugar",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "chlorides",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "free sulfur dioxide",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "total sulfur dioxide",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "density",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "pH",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "sulphates",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "alcohol",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "quality",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>alcohol</th><th>chlorides</th><th>citric acid</th><th>density</th><th>fixed acidity</th><th>free sulfur dioxide</th><th>pH</th><th>quality</th><th>residual sugar</th><th>sulphates</th><th>total sulfur dioxide</th><th>volatile acidity</th></tr></thead><tbody><tr><td>8.8</td><td>0.045</td><td>0.36</td><td>1.001</td><td>7.0</td><td>45.0</td><td>3.0</td><td>6</td><td>20.7</td><td>0.45</td><td>170.0</td><td>0.27</td></tr><tr><td>9.5</td><td>0.049</td><td>0.34</td><td>0.994</td><td>6.3</td><td>14.0</td><td>3.3</td><td>6</td><td>1.6</td><td>0.49</td><td>132.0</td><td>0.3</td></tr><tr><td>10.1</td><td>0.05</td><td>0.4</td><td>0.9951</td><td>8.1</td><td>30.0</td><td>3.26</td><td>6</td><td>6.9</td><td>0.44</td><td>97.0</td><td>0.28</td></tr><tr><td>9.9</td><td>0.058</td><td>0.32</td><td>0.9956</td><td>7.2</td><td>47.0</td><td>3.19</td><td>6</td><td>8.5</td><td>0.4</td><td>186.0</td><td>0.23</td></tr><tr><td>9.9</td><td>0.058</td><td>0.32</td><td>0.9956</td><td>7.2</td><td>47.0</td><td>3.19</td><td>6</td><td>8.5</td><td>0.4</td><td>186.0</td><td>0.23</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         8.8,
         0.045,
         0.36,
         1.001,
         7.0,
         45.0,
         3.0,
         6,
         20.7,
         0.45,
         170.0,
         0.27
        ],
        [
         9.5,
         0.049,
         0.34,
         0.994,
         6.3,
         14.0,
         3.3,
         6,
         1.6,
         0.49,
         132.0,
         0.3
        ],
        [
         10.1,
         0.05,
         0.4,
         0.9951,
         8.1,
         30.0,
         3.26,
         6,
         6.9,
         0.44,
         97.0,
         0.28
        ],
        [
         9.9,
         0.058,
         0.32,
         0.9956,
         7.2,
         47.0,
         3.19,
         6,
         8.5,
         0.4,
         186.0,
         0.23
        ],
        [
         9.9,
         0.058,
         0.32,
         0.9956,
         7.2,
         47.0,
         3.19,
         6,
         8.5,
         0.4,
         186.0,
         0.23
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "alcohol",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "chlorides",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "citric acid",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "density",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "fixed acidity",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "free sulfur dioxide",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "pH",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "quality",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "residual sugar",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "sulphates",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "total sulfur dioxide",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "volatile acidity",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# File locations and types\n",
    "# Recuperar los valores de los widgets configurados en Databricks\n",
    "\n",
    "# Obtener el tipo de archivo (por ejemplo, \"csv\", \"parquet\", etc.) a partir de los widgets\n",
    "fileTypeDaset1 = dbutils.widgets.get(\"fileTypeDaset1\")\n",
    "fileTypeDaset2 = dbutils.widgets.get(\"fileTypeDaset2\")\n",
    "\n",
    "# Determinar si se debe inferir el esquema automáticamente\n",
    "inferSchema = dbutils.widgets.get(\"inferSchema\")\n",
    "\n",
    "# Indicar si la primera fila contiene los nombres de las columnas (cabecera)\n",
    "firstRowIsHeader = dbutils.widgets.get(\"firstRowIsHeader\")\n",
    "\n",
    "# Obtener el delimitador de columnas en el archivo (por ejemplo, \",\" para CSV)\n",
    "delimiter = dbutils.widgets.get(\"delimiter\")\n",
    "\n",
    "# Obtener los nombres de los datasets a partir de los widgets\n",
    "dataset_1_name = dbutils.widgets.get(\"dataset_1_name\")\n",
    "dataset_2_name = dbutils.widgets.get(\"dataset_2_name\")\n",
    "\n",
    "# Obtener el directorio donde se guardan los archivos de datos\n",
    "saveDirectory = dbutils.widgets.get(\"saveDirectory\")\n",
    "\n",
    "# Construir las rutas completas a los archivos de datos utilizando el directorio y nombres de datasets\n",
    "dataset_1_location = f\"{saveDirectory}/{dataset_1_name}\"\n",
    "dataset_2_location = f\"{saveDirectory}/{dataset_2_name}\"\n",
    "\n",
    "# Cargar el primer dataset en un DataFrame de Spark utilizando los parámetros obtenidos\n",
    "red_wine_df = load_data_to_spark_dataframe(\n",
    "    dataset_1_location, fileTypeDaset1, inferSchema, firstRowIsHeader, delimiter\n",
    ")\n",
    "\n",
    "# Persistir el DataFrame en memoria y disco para optimizar las operaciones posteriores\n",
    "red_wine_df.persist()\n",
    "\n",
    "# Cargar el segundo dataset en otro DataFrame de Spark\n",
    "white_wine_df = load_data_to_spark_dataframe(\n",
    "    dataset_2_location, fileTypeDaset2, inferSchema, firstRowIsHeader, delimiter\n",
    ")\n",
    "\n",
    "# Cachear el DataFrame en memoria para mejorar el rendimiento si se reutiliza\n",
    "white_wine_df.cache()\n",
    "\n",
    "# Mostrar las primeras 5 filas del DataFrame `red_wine_df` en el notebook\n",
    "display(red_wine_df.limit(5))\n",
    "\n",
    "# Mostrar las primeras 5 filas del DataFrame `white_wine_df` en el notebook\n",
    "display(white_wine_df.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0028549-134d-44fc-b60e-e38b81c0ac53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DEFINICION DE VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a73344e0-f9cb-416f-9849-0e7210f31a10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener los valores de las columnas de partición desde un widget en Databricks.\n",
    "# Se asume que la función `get_widget_value_as_list` convierte el valor del widget en una lista.\n",
    "partition_columns = get_widget_value_as_list(\"partition_columns\")\n",
    "\n",
    "# Recuperar el directorio de guardado a partir del valor del widget.\n",
    "# Esto indica dónde se guardarán o leerán los archivos dentro del entorno de Databricks.\n",
    "saveDirectory = dbutils.widgets.get(\"saveDirectory\")\n",
    "\n",
    "# Obtener el nombre de la base de datos (por ejemplo, capa \"silver\") configurado en los widgets.\n",
    "# Las bases de datos en Databricks suelen organizarse en diferentes capas como \"bronze\", \"silver\", y \"gold\".\n",
    "database = dbutils.widgets.get(\"silver\")\n",
    "\n",
    "# Asignar valores específicos a las columnas de partición.\n",
    "# Estos valores se usarán para filtrar o seleccionar particiones en los DataFrames.\n",
    "partition_column_1_value = '3'      # Ejemplo de valor para la primera columna de partición\n",
    "partition_column_2_value = '5.0'    # Ejemplo de valor para la segunda columna de partición"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0a628ba-75e6-4f59-8f9d-8fb05d213fe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### GUARDA EL DATAFRAME EN FORMATO DELTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f3e59d0-2d2c-4fa3-8855-62f7ff5b0557",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los nombres de las columnas han sido estandarizados exitosamente. Las columnas ahora usan solo caracteres alfanuméricos y guiones bajos.\nLos nombres de las columnas han sido estandarizados exitosamente. Las columnas ahora usan solo caracteres alfanuméricos y guiones bajos.\nEl DataFrame ha sido guardado en formato Delta en '/FileStore/tables/Delta_dataset1', particionado por ['quality', 'free_sulfur_dioxide']\nArchivos en el directorio /FileStore/tables/Delta_dataset1:\ndbfs:/FileStore/tables/Delta_dataset1/_delta_log/\ndbfs:/FileStore/tables/Delta_dataset1/quality=3/\ndbfs:/FileStore/tables/Delta_dataset1/quality=4/\ndbfs:/FileStore/tables/Delta_dataset1/quality=5/\ndbfs:/FileStore/tables/Delta_dataset1/quality=6/\ndbfs:/FileStore/tables/Delta_dataset1/quality=7/\ndbfs:/FileStore/tables/Delta_dataset1/quality=8/\n\nLos nombres de las columnas han sido estandarizados exitosamente. Las columnas ahora usan solo caracteres alfanuméricos y guiones bajos.\nLos nombres de las columnas han sido estandarizados exitosamente. Las columnas ahora usan solo caracteres alfanuméricos y guiones bajos.\nEl DataFrame ha sido guardado en formato Delta en '/FileStore/tables/Delta_dataset2', particionado por ['quality', 'free_sulfur_dioxide']\nArchivos en el directorio /FileStore/tables/Delta_dataset2:\ndbfs:/FileStore/tables/Delta_dataset2/_delta_log/\ndbfs:/FileStore/tables/Delta_dataset2/quality=3/\ndbfs:/FileStore/tables/Delta_dataset2/quality=4/\ndbfs:/FileStore/tables/Delta_dataset2/quality=5/\ndbfs:/FileStore/tables/Delta_dataset2/quality=6/\ndbfs:/FileStore/tables/Delta_dataset2/quality=7/\ndbfs:/FileStore/tables/Delta_dataset2/quality=8/\ndbfs:/FileStore/tables/Delta_dataset2/quality=9/\n\nArchivos en el directorio /FileStore/tables/Delta_dataset1/quality=3/free_sulfur_dioxide=5.0:\ndbfs:/FileStore/tables/Delta_dataset1/quality=3/free_sulfur_dioxide=5.0/part-00000-856a23bc-f572-41e6-82eb-6b3d062c4835.c000.snappy.parquet\n\nArchivos en el directorio /FileStore/tables/Delta_dataset2/quality=3/free_sulfur_dioxide=5.0:\ndbfs:/FileStore/tables/Delta_dataset2/quality=3/free_sulfur_dioxide=5.0/part-00000-c4c15c82-52e6-4646-9b6f-45ff434c7b0c.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "# Definir las rutas para guardar los DataFrames en formato Delta\n",
    "# Utilizando el directorio de guardado (saveDirectory) y nombres descriptivos\n",
    "deltaPath1 = f\"{saveDirectory}/Delta_dataset1\"\n",
    "deltaPath2 = f\"{saveDirectory}/Delta_dataset2\"\n",
    "\n",
    "# Guardar el DataFrame `red_wine_df` en formato Delta en la ruta especificada (deltaPath1)\n",
    "# Se utiliza partición basada en las columnas definidas en `partition_columns`\n",
    "# La función `save_dataframe_as_delta_format` devuelve la lista de columnas de partición usadas\n",
    "partition_columns_delta_1 = save_dataframe_as_delta_format(\n",
    "    red_wine_df, \n",
    "    deltaPath1, \n",
    "    partition_columns\n",
    ")\n",
    "\n",
    "# Listar todos los archivos y carpetas creados en el directorio `deltaPath1`\n",
    "# Esto verifica que el DataFrame se haya guardado correctamente en formato Delta\n",
    "list_files_in_directory_path(deltaPath1)\n",
    "print()  # Imprimir una línea en blanco para mayor claridad en la salida\n",
    "\n",
    "# Guardar el DataFrame `white_wine_df` en formato Delta en la ruta especificada (deltaPath2)\n",
    "partition_columns_delta_2 = save_dataframe_as_delta_format(\n",
    "    white_wine_df, \n",
    "    deltaPath2, \n",
    "    partition_columns\n",
    ")\n",
    "\n",
    "# Listar todos los archivos y carpetas creados en el directorio `deltaPath2`\n",
    "list_files_in_directory_path(deltaPath2)\n",
    "print()  # Imprimir una línea en blanco para separar la salida\n",
    "\n",
    "# Listar los archivos dentro de una partición específica en `Delta_dataset1`\n",
    "# Se construye la ruta de la partición utilizando los valores de las columnas de partición\n",
    "list_files_in_directory_path(\n",
    "    f'{deltaPath1}/{partition_columns_delta_1[0]}={partition_column_1_value}/'\n",
    "    f'{partition_columns_delta_1[1]}={partition_column_2_value}'\n",
    ")\n",
    "print()  # Línea en blanco para separar la salida\n",
    "\n",
    "# Listar los archivos dentro de una partición específica en `Delta_dataset2`\n",
    "list_files_in_directory_path(\n",
    "    f'{deltaPath2}/{partition_columns_delta_2[0]}={partition_column_1_value}/'\n",
    "    f'{partition_columns_delta_2[1]}={partition_column_2_value}'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c73788f0-00ed-410f-8ba8-b9987cfd0347",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### GUARDA EL DATAFRAME EN FORMATO PARQUET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "459100b1-2569-4119-ab28-8e099aed147b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El DataFrame ha sido guardado en formato Parquet en '/FileStore/tables/Parquet_dataset1', particionado por ['quality', 'free sulfur dioxide']\nArchivos en el directorio /FileStore/tables/Parquet_dataset1:\ndbfs:/FileStore/tables/Parquet_dataset1/_SUCCESS\ndbfs:/FileStore/tables/Parquet_dataset1/quality=3/\ndbfs:/FileStore/tables/Parquet_dataset1/quality=4/\ndbfs:/FileStore/tables/Parquet_dataset1/quality=5/\ndbfs:/FileStore/tables/Parquet_dataset1/quality=6/\ndbfs:/FileStore/tables/Parquet_dataset1/quality=7/\ndbfs:/FileStore/tables/Parquet_dataset1/quality=8/\n\nEl DataFrame ha sido guardado en formato Parquet en '/FileStore/tables/Parquet_dataset2', particionado por ['quality', 'free sulfur dioxide']\nArchivos en el directorio /FileStore/tables/Parquet_dataset2:\ndbfs:/FileStore/tables/Parquet_dataset2/_SUCCESS\ndbfs:/FileStore/tables/Parquet_dataset2/quality=3/\ndbfs:/FileStore/tables/Parquet_dataset2/quality=4/\ndbfs:/FileStore/tables/Parquet_dataset2/quality=5/\ndbfs:/FileStore/tables/Parquet_dataset2/quality=6/\ndbfs:/FileStore/tables/Parquet_dataset2/quality=7/\ndbfs:/FileStore/tables/Parquet_dataset2/quality=8/\ndbfs:/FileStore/tables/Parquet_dataset2/quality=9/\n\nArchivos en el directorio /FileStore/tables/Parquet_dataset1/quality=3/free sulfur dioxide=5.0:\ndbfs:/FileStore/tables/Parquet_dataset1/quality=3/free sulfur dioxide=5.0/_SUCCESS\ndbfs:/FileStore/tables/Parquet_dataset1/quality=3/free sulfur dioxide=5.0/_committed_8871259216303208616\ndbfs:/FileStore/tables/Parquet_dataset1/quality=3/free sulfur dioxide=5.0/_started_8871259216303208616\ndbfs:/FileStore/tables/Parquet_dataset1/quality=3/free sulfur dioxide=5.0/part-00000-tid-8871259216303208616-054a1379-e2e1-484f-b199-ef720737ca85-29-2.c000.snappy.parquet\n\nArchivos en el directorio /FileStore/tables/Parquet_dataset2/quality=3/free sulfur dioxide=5.0:\ndbfs:/FileStore/tables/Parquet_dataset2/quality=3/free sulfur dioxide=5.0/_SUCCESS\ndbfs:/FileStore/tables/Parquet_dataset2/quality=3/free sulfur dioxide=5.0/_committed_7896268169681797842\ndbfs:/FileStore/tables/Parquet_dataset2/quality=3/free sulfur dioxide=5.0/_started_7896268169681797842\ndbfs:/FileStore/tables/Parquet_dataset2/quality=3/free sulfur dioxide=5.0/part-00000-tid-7896268169681797842-891288c0-2a8f-4a73-8e6e-f1287ef8f795-30-1.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "# Definir las rutas para guardar los DataFrames en formato Parquet\n",
    "# Utilizando el directorio de guardado (saveDirectory) y nombres descriptivos\n",
    "parquetPath1 = f\"{saveDirectory}/Parquet_dataset1\"\n",
    "parquetPath2 = f\"{saveDirectory}/Parquet_dataset2\"\n",
    "\n",
    "# Guardar el DataFrame `red_wine_df` en formato Parquet en la ruta especificada (parquetPath1)\n",
    "# Utiliza las columnas de partición definidas en `partition_columns`\n",
    "# La función `save_dataframe_as_parquet_format` devuelve la lista de columnas de partición usadas\n",
    "partition_columns_parquet_1 = save_dataframe_as_parquet_format(\n",
    "    red_wine_df, \n",
    "    parquetPath1, \n",
    "    partition_columns\n",
    ")\n",
    "\n",
    "# Listar todos los archivos y carpetas creados en el directorio `parquetPath1`\n",
    "# Esto verifica que el DataFrame se haya guardado correctamente en formato Parquet\n",
    "list_files_in_directory_path(parquetPath1)\n",
    "print()  # Imprimir una línea en blanco para separar la salida\n",
    "\n",
    "# Guardar el DataFrame `white_wine_df` en formato Parquet en la ruta especificada (parquetPath2)\n",
    "partition_columns_parquet_2 = save_dataframe_as_parquet_format(\n",
    "    white_wine_df, \n",
    "    parquetPath2, \n",
    "    partition_columns\n",
    ")\n",
    "\n",
    "# Listar todos los archivos y carpetas creados en el directorio `parquetPath2`\n",
    "list_files_in_directory_path(parquetPath2)\n",
    "print()  # Imprimir una línea en blanco para mayor claridad en la salida\n",
    "\n",
    "# Listar los archivos dentro de una partición específica en `Parquet_dataset1`\n",
    "# La ruta de la partición se construye usando los nombres y valores de las columnas de partición\n",
    "list_files_in_directory_path(\n",
    "    f'{parquetPath1}/{partition_columns_parquet_1[0]}={partition_column_1_value}/'\n",
    "    f'{partition_columns_parquet_1[1]}={partition_column_2_value}'\n",
    ")\n",
    "print()  # Línea en blanco para separar la salida\n",
    "\n",
    "# Listar los archivos dentro de una partición específica en `Parquet_dataset2`\n",
    "list_files_in_directory_path(\n",
    "    f'{parquetPath2}/{partition_columns_parquet_2[0]}={partition_column_1_value}/'\n",
    "    f'{partition_columns_parquet_2[1]}={partition_column_2_value}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee5659b2-05e4-4f07-a024-53b849577c3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### GUARDA EL DATAFRAME COMO TABLA EN FORMATO DELTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "864d2cc9-9864-4bd3-b5d6-6cc4a3e1ff22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La base de datos 'db_silver' ha sido creada exitosamente (o ya existía).\n"
     ]
    }
   ],
   "source": [
    "# Llama a la función `create_database` con el nombre de la base de datos que fue previamente almacenado en la variable `database`.\n",
    "# Esta función tiene como objetivo crear una base de datos en el entorno de Spark si aún no existe.\n",
    "create_database(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dd2597a-062e-44ae-a2a8-64b6552e21c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los nombres de las columnas han sido estandarizados exitosamente. Las columnas ahora usan solo caracteres alfanuméricos y guiones bajos.\nLos nombres de las columnas han sido estandarizados exitosamente. Las columnas ahora usan solo caracteres alfanuméricos y guiones bajos.\nEl DataFrame ha sido guardado exitosamente como tabla Delta en la base de datos: db_silver, tabla: dataset1_delta, particionada por ['quality', 'free_sulfur_dioxide']\n\nLos nombres de las columnas han sido estandarizados exitosamente. Las columnas ahora usan solo caracteres alfanuméricos y guiones bajos.\nLos nombres de las columnas han sido estandarizados exitosamente. Las columnas ahora usan solo caracteres alfanuméricos y guiones bajos.\nEl DataFrame ha sido guardado exitosamente como tabla Delta en la base de datos: db_silver, tabla: dataset2_delta, particionada por ['quality', 'free_sulfur_dioxide']\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>database</th><th>tableName</th><th>isTemporary</th></tr></thead><tbody><tr><td>db_silver</td><td>dataset1_delta</td><td>false</td></tr><tr><td>db_silver</td><td>dataset2_delta</td><td>false</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "db_silver",
         "dataset1_delta",
         false
        ],
        [
         "db_silver",
         "dataset2_delta",
         false
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "database",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tableName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isTemporary",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Definir el nombre de la tabla Delta para el primer dataset (vino tinto)\n",
    "tabla_dataset1 = f'dataset1_delta'\n",
    "\n",
    "# Definir la ruta donde se almacenará la tabla Delta para `dataset1`\n",
    "tableParquetPath1 = f\"{saveDirectory}/{tabla_dataset1}\"\n",
    "\n",
    "# Definir el nombre de la tabla Delta para el segundo dataset (vino blanco)\n",
    "tabla_dataset2 = f'dataset2_delta'\n",
    "\n",
    "# Definir la ruta donde se almacenará la tabla Delta para `dataset2`\n",
    "tableParquetPath2 = f\"{saveDirectory}/{tabla_dataset2}\"\n",
    "\n",
    "# Guardar el DataFrame `red_wine_df` como tabla Delta en la base de datos especificada\n",
    "# Se utiliza partición según las columnas definidas en `partition_columns`\n",
    "save_dataframe_as_delta_table(red_wine_df, database, tabla_dataset1, partition_columns)\n",
    "print()  # Línea en blanco para separar la salida en la consola\n",
    "\n",
    "# Guardar el DataFrame `white_wine_df` como tabla Delta en la misma base de datos\n",
    "save_dataframe_as_delta_table(white_wine_df, database, tabla_dataset2, partition_columns)\n",
    "print()  # Línea en blanco para separar la salida en la consola\n",
    "\n",
    "# Listar todas las tablas disponibles en la base de datos especificada\n",
    "list_tables_in_database(database)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f97d1cb-79d9-4821-b688-b191339a1bc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# VERIFICAR LAS COLUMNAS Y EL TIPO DE DATOS DE LA TABLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba5b5d33-edf6-4fb7-82a1-6113c329f23a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando consulta SQL:\n\nDESCRIBE EXTENDED db_silver.dataset1_delta\n\nLa consulta SQL se ejecutó exitosamente. El número de registros obtenidos es: 30\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col_name</th><th>data_type</th><th>comment</th></tr></thead><tbody><tr><td>fixed_acidity</td><td>double</td><td>null</td></tr><tr><td>volatile_acidity</td><td>double</td><td>null</td></tr><tr><td>citric_acid</td><td>double</td><td>null</td></tr><tr><td>residual_sugar</td><td>double</td><td>null</td></tr><tr><td>chlorides</td><td>double</td><td>null</td></tr><tr><td>free_sulfur_dioxide</td><td>double</td><td>null</td></tr><tr><td>total_sulfur_dioxide</td><td>double</td><td>null</td></tr><tr><td>density</td><td>double</td><td>null</td></tr><tr><td>pH</td><td>double</td><td>null</td></tr><tr><td>sulphates</td><td>double</td><td>null</td></tr><tr><td>alcohol</td><td>double</td><td>null</td></tr><tr><td>quality</td><td>int</td><td>null</td></tr><tr><td># Partition Information</td><td></td><td></td></tr><tr><td># col_name</td><td>data_type</td><td>comment</td></tr><tr><td>quality</td><td>int</td><td>null</td></tr><tr><td>free_sulfur_dioxide</td><td>double</td><td>null</td></tr><tr><td></td><td></td><td></td></tr><tr><td># Detailed Table Information</td><td></td><td></td></tr><tr><td>Catalog</td><td>spark_catalog</td><td></td></tr><tr><td>Database</td><td>db_silver</td><td></td></tr><tr><td>Table</td><td>dataset1_delta</td><td></td></tr><tr><td>Created Time</td><td>Sun Nov 17 00:09:53 UTC 2024</td><td></td></tr><tr><td>Last Access</td><td>UNKNOWN</td><td></td></tr><tr><td>Created By</td><td>Spark 3.3.2</td><td></td></tr><tr><td>Type</td><td>MANAGED</td><td></td></tr><tr><td>Location</td><td>dbfs:/user/hive/warehouse/db_silver.db/dataset1_delta</td><td></td></tr><tr><td>Provider</td><td>delta</td><td></td></tr><tr><td>Owner</td><td>root</td><td></td></tr><tr><td>Is_managed_location</td><td>true</td><td></td></tr><tr><td>Table Properties</td><td>[delta.minReaderVersion=1,delta.minWriterVersion=2]</td><td></td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "fixed_acidity",
         "double",
         null
        ],
        [
         "volatile_acidity",
         "double",
         null
        ],
        [
         "citric_acid",
         "double",
         null
        ],
        [
         "residual_sugar",
         "double",
         null
        ],
        [
         "chlorides",
         "double",
         null
        ],
        [
         "free_sulfur_dioxide",
         "double",
         null
        ],
        [
         "total_sulfur_dioxide",
         "double",
         null
        ],
        [
         "density",
         "double",
         null
        ],
        [
         "pH",
         "double",
         null
        ],
        [
         "sulphates",
         "double",
         null
        ],
        [
         "alcohol",
         "double",
         null
        ],
        [
         "quality",
         "int",
         null
        ],
        [
         "# Partition Information",
         "",
         ""
        ],
        [
         "# col_name",
         "data_type",
         "comment"
        ],
        [
         "quality",
         "int",
         null
        ],
        [
         "free_sulfur_dioxide",
         "double",
         null
        ],
        [
         "",
         "",
         ""
        ],
        [
         "# Detailed Table Information",
         "",
         ""
        ],
        [
         "Catalog",
         "spark_catalog",
         ""
        ],
        [
         "Database",
         "db_silver",
         ""
        ],
        [
         "Table",
         "dataset1_delta",
         ""
        ],
        [
         "Created Time",
         "Sun Nov 17 00:09:53 UTC 2024",
         ""
        ],
        [
         "Last Access",
         "UNKNOWN",
         ""
        ],
        [
         "Created By",
         "Spark 3.3.2",
         ""
        ],
        [
         "Type",
         "MANAGED",
         ""
        ],
        [
         "Location",
         "dbfs:/user/hive/warehouse/db_silver.db/dataset1_delta",
         ""
        ],
        [
         "Provider",
         "delta",
         ""
        ],
        [
         "Owner",
         "root",
         ""
        ],
        [
         "Is_managed_location",
         "true",
         ""
        ],
        [
         "Table Properties",
         "[delta.minReaderVersion=1,delta.minWriterVersion=2]",
         ""
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"comment\":\"name of the column\"}",
         "name": "col_name",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\":\"data type of the column\"}",
         "name": "data_type",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\":\"comment of the column\"}",
         "name": "comment",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando consulta SQL:\n\nDESCRIBE EXTENDED db_silver.dataset2_delta\n\nLa consulta SQL se ejecutó exitosamente. El número de registros obtenidos es: 30\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col_name</th><th>data_type</th><th>comment</th></tr></thead><tbody><tr><td>alcohol</td><td>double</td><td>null</td></tr><tr><td>chlorides</td><td>double</td><td>null</td></tr><tr><td>citric_acid</td><td>double</td><td>null</td></tr><tr><td>density</td><td>double</td><td>null</td></tr><tr><td>fixed_acidity</td><td>double</td><td>null</td></tr><tr><td>free_sulfur_dioxide</td><td>double</td><td>null</td></tr><tr><td>pH</td><td>double</td><td>null</td></tr><tr><td>quality</td><td>bigint</td><td>null</td></tr><tr><td>residual_sugar</td><td>double</td><td>null</td></tr><tr><td>sulphates</td><td>double</td><td>null</td></tr><tr><td>total_sulfur_dioxide</td><td>double</td><td>null</td></tr><tr><td>volatile_acidity</td><td>double</td><td>null</td></tr><tr><td># Partition Information</td><td></td><td></td></tr><tr><td># col_name</td><td>data_type</td><td>comment</td></tr><tr><td>quality</td><td>bigint</td><td>null</td></tr><tr><td>free_sulfur_dioxide</td><td>double</td><td>null</td></tr><tr><td></td><td></td><td></td></tr><tr><td># Detailed Table Information</td><td></td><td></td></tr><tr><td>Catalog</td><td>spark_catalog</td><td></td></tr><tr><td>Database</td><td>db_silver</td><td></td></tr><tr><td>Table</td><td>dataset2_delta</td><td></td></tr><tr><td>Created Time</td><td>Sun Nov 17 00:10:35 UTC 2024</td><td></td></tr><tr><td>Last Access</td><td>UNKNOWN</td><td></td></tr><tr><td>Created By</td><td>Spark 3.3.2</td><td></td></tr><tr><td>Type</td><td>MANAGED</td><td></td></tr><tr><td>Location</td><td>dbfs:/user/hive/warehouse/db_silver.db/dataset2_delta</td><td></td></tr><tr><td>Provider</td><td>delta</td><td></td></tr><tr><td>Owner</td><td>root</td><td></td></tr><tr><td>Is_managed_location</td><td>true</td><td></td></tr><tr><td>Table Properties</td><td>[delta.minReaderVersion=1,delta.minWriterVersion=2]</td><td></td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "alcohol",
         "double",
         null
        ],
        [
         "chlorides",
         "double",
         null
        ],
        [
         "citric_acid",
         "double",
         null
        ],
        [
         "density",
         "double",
         null
        ],
        [
         "fixed_acidity",
         "double",
         null
        ],
        [
         "free_sulfur_dioxide",
         "double",
         null
        ],
        [
         "pH",
         "double",
         null
        ],
        [
         "quality",
         "bigint",
         null
        ],
        [
         "residual_sugar",
         "double",
         null
        ],
        [
         "sulphates",
         "double",
         null
        ],
        [
         "total_sulfur_dioxide",
         "double",
         null
        ],
        [
         "volatile_acidity",
         "double",
         null
        ],
        [
         "# Partition Information",
         "",
         ""
        ],
        [
         "# col_name",
         "data_type",
         "comment"
        ],
        [
         "quality",
         "bigint",
         null
        ],
        [
         "free_sulfur_dioxide",
         "double",
         null
        ],
        [
         "",
         "",
         ""
        ],
        [
         "# Detailed Table Information",
         "",
         ""
        ],
        [
         "Catalog",
         "spark_catalog",
         ""
        ],
        [
         "Database",
         "db_silver",
         ""
        ],
        [
         "Table",
         "dataset2_delta",
         ""
        ],
        [
         "Created Time",
         "Sun Nov 17 00:10:35 UTC 2024",
         ""
        ],
        [
         "Last Access",
         "UNKNOWN",
         ""
        ],
        [
         "Created By",
         "Spark 3.3.2",
         ""
        ],
        [
         "Type",
         "MANAGED",
         ""
        ],
        [
         "Location",
         "dbfs:/user/hive/warehouse/db_silver.db/dataset2_delta",
         ""
        ],
        [
         "Provider",
         "delta",
         ""
        ],
        [
         "Owner",
         "root",
         ""
        ],
        [
         "Is_managed_location",
         "true",
         ""
        ],
        [
         "Table Properties",
         "[delta.minReaderVersion=1,delta.minWriterVersion=2]",
         ""
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"comment\":\"name of the column\"}",
         "name": "col_name",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\":\"data type of the column\"}",
         "name": "data_type",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\":\"comment of the column\"}",
         "name": "comment",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = f'''\n",
    "DESCRIBE EXTENDED {database}.{tabla_dataset1}\n",
    "'''\n",
    "display(execute_spark_sql_query(query))\n",
    "\n",
    "query = f'''\n",
    "DESCRIBE EXTENDED {database}.{tabla_dataset2}\n",
    "'''\n",
    "display(execute_spark_sql_query(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f07c50b-91ba-4b09-a15e-93d7723d17ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### GUARDA EL DATAFRAME COMO TABLA EN FORMATO PARQUET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50fc2bc9-3e8b-4b46-b67d-5ddce3a10b64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El DataFrame ha sido guardado exitosamente como tabla Parquet en la base de datos: db_silver, tabla: dataset1_parquet, particionada por ['quality', 'free sulfur dioxide']\n\nEl DataFrame ha sido guardado exitosamente como tabla Parquet en la base de datos: db_silver, tabla: dataset2_parquet, particionada por ['quality', 'free sulfur dioxide']\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>database</th><th>tableName</th><th>isTemporary</th></tr></thead><tbody><tr><td>db_silver</td><td>dataset1_delta</td><td>false</td></tr><tr><td>db_silver</td><td>dataset1_parquet</td><td>false</td></tr><tr><td>db_silver</td><td>dataset2_delta</td><td>false</td></tr><tr><td>db_silver</td><td>dataset2_parquet</td><td>false</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "db_silver",
         "dataset1_delta",
         false
        ],
        [
         "db_silver",
         "dataset1_parquet",
         false
        ],
        [
         "db_silver",
         "dataset2_delta",
         false
        ],
        [
         "db_silver",
         "dataset2_parquet",
         false
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "database",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tableName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isTemporary",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Definir el nombre de la tabla Parquet para el primer dataset (vino tinto)\n",
    "tabla_dataset1 = f'dataset1_parquet'\n",
    "\n",
    "# Definir la ruta donde se almacenará la tabla Parquet para `dataset1`\n",
    "tableParquetPath1 = f\"{saveDirectory}/{tabla_dataset1}\"\n",
    "\n",
    "# Definir el nombre de la tabla Parquet para el segundo dataset (vino blanco)\n",
    "tabla_dataset2 = f'dataset2_parquet'\n",
    "\n",
    "# Definir la ruta donde se almacenará la tabla Parquet para `dataset2`\n",
    "tableParquetPath2 = f\"{saveDirectory}/{tabla_dataset2}\"\n",
    "\n",
    "# Guardar el DataFrame `red_wine_df` como tabla Parquet en la base de datos especificada\n",
    "# Se utiliza partición según las columnas definidas en `partition_columns`\n",
    "save_dataframe_as_parquet_table(red_wine_df, database, tabla_dataset1, partition_columns)\n",
    "print()  # Línea en blanco para separar la salida en la consola\n",
    "\n",
    "# Guardar el DataFrame `white_wine_df` como tabla Parquet en la misma base de datos\n",
    "save_dataframe_as_parquet_table(white_wine_df, database, tabla_dataset2, partition_columns)\n",
    "print()  # Línea en blanco para separar la salida en la consola\n",
    "\n",
    "# Listar todas las tablas disponibles en la base de datos especificada\n",
    "list_tables_in_database(database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1fd3e08-128b-48b7-99d7-e27c02a28410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## GUARDAR EL DATAFRAME COMO TABLAS TEMPORALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a8e20f6-a646-491b-bf16-8bbbfc0c55cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El DataFrame ha sido guardado exitosamente como tabla temporal: dataset1_temp\n\nEl DataFrame ha sido guardado exitosamente como tabla temporal: dataset2_temp\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Table</th><th>Type</th></tr></thead><tbody><tr><td>dataset1_temp</td><td>Local Temporary</td></tr><tr><td>dataset2_temp</td><td>Local Temporary</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dataset1_temp",
         "Local Temporary"
        ],
        [
         "dataset2_temp",
         "Local Temporary"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Table",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Type",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Table</th><th>Type</th></tr></thead><tbody><tr><td>global_temp.dataset1_temp</td><td>Global Temporary</td></tr><tr><td>global_temp.dataset2_temp</td><td>Global Temporary</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "global_temp.dataset1_temp",
         "Global Temporary"
        ],
        [
         "global_temp.dataset2_temp",
         "Global Temporary"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Table",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Type",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>database</th><th>tableName</th><th>isTemporary</th></tr></thead><tbody><tr><td>db_silver</td><td>dataset1_delta</td><td>false</td></tr><tr><td>db_silver</td><td>dataset1_parquet</td><td>false</td></tr><tr><td>db_silver</td><td>dataset2_delta</td><td>false</td></tr><tr><td>db_silver</td><td>dataset2_parquet</td><td>false</td></tr><tr><td></td><td>dataset1_temp</td><td>true</td></tr><tr><td></td><td>dataset2_temp</td><td>true</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "db_silver",
         "dataset1_delta",
         false
        ],
        [
         "db_silver",
         "dataset1_parquet",
         false
        ],
        [
         "db_silver",
         "dataset2_delta",
         false
        ],
        [
         "db_silver",
         "dataset2_parquet",
         false
        ],
        [
         "",
         "dataset1_temp",
         true
        ],
        [
         "",
         "dataset2_temp",
         true
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "database",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tableName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isTemporary",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Definir el nombre de la tabla temporal para el primer dataset (vino tinto)\n",
    "tabla_dataset1 = f'dataset1_temp'\n",
    "\n",
    "# Definir la ruta donde se almacenará la tabla temporal para `dataset1`\n",
    "tableParquetPath1 = f\"{saveDirectory}/{tabla_dataset1}\"\n",
    "\n",
    "# Definir el nombre de la tabla temporal para el segundo dataset (vino blanco)\n",
    "tabla_dataset2 = f'dataset2_temp'\n",
    "\n",
    "# Definir la ruta donde se almacenará la tabla temporal para `dataset2`\n",
    "tableParquetPath2 = f\"{saveDirectory}/{tabla_dataset2}\"\n",
    "\n",
    "# Guardar el DataFrame `red_wine_df` como una tabla temporal local en el entorno de Spark\n",
    "# Se utiliza partición según las columnas definidas en `partition_columns`\n",
    "save_dataframe_as_temp_table(red_wine_df, tabla_dataset1, partition_columns)\n",
    "print()  # Línea en blanco para separar la salida en la consola\n",
    "\n",
    "# Guardar el DataFrame `white_wine_df` como una tabla temporal local en el entorno de Spark\n",
    "save_dataframe_as_temp_table(white_wine_df, tabla_dataset2, partition_columns)\n",
    "print()  # Línea en blanco para separar la salida en la consola\n",
    "\n",
    "# Listar todas las tablas temporales locales en el entorno de Spark\n",
    "list_temp_tables()\n",
    "\n",
    "# Listar todas las tablas temporales globales en el entorno de Spark\n",
    "# El parámetro `show_global=True` muestra tablas temporales globales si existen\n",
    "list_temp_tables(show_global=True)\n",
    "\n",
    "# Listar todas las tablas disponibles en la base de datos especificada\n",
    "list_tables_in_database(database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "851c25b4-8df2-4a99-acf1-399e6e433cd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ELIMINAS LOS PARAMETROS DEL NOTEBOOK Y LOS ARCHIVOS CREADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5acbe92-1244-4fcd-8e03-ce89fcb46061",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La base de datos 'db_silver' ha sido eliminada exitosamente (si existía).\nEliminando archivos en el directorio /FileStore/tables...\n\nArchivo eliminado: dbfs:/FileStore/tables/Delta_dataset1/\nArchivo eliminado: dbfs:/FileStore/tables/Delta_dataset2/\nArchivo eliminado: dbfs:/FileStore/tables/Parquet_dataset1/\nArchivo eliminado: dbfs:/FileStore/tables/Parquet_dataset2/\nArchivo eliminado: dbfs:/FileStore/tables/winequality-red.csv\nArchivo eliminado: dbfs:/FileStore/tables/winequality-white.json\n\nTodos los archivos en el directorio /FileStore/tables han sido eliminados.\n\nTodos los widgets han sido eliminados.\n"
     ]
    }
   ],
   "source": [
    "# Descartar los DataFrames en memoria para liberar espacio en caché\n",
    "# `unpersist()` elimina el almacenamiento en caché de los DataFrames previamente cargados\n",
    "red_wine_df.unpersist()\n",
    "white_wine_df.unpersist()\n",
    "\n",
    "# Recuperar el nombre de la base de datos desde el widget \"silver\"\n",
    "# Esta base de datos puede estar asociada a un entorno de datos de tipo \"silver\"\n",
    "database = dbutils.widgets.get(\"silver\")\n",
    "\n",
    "# Eliminar la base de datos especificada por la variable `database`\n",
    "# Esto eliminará la base de datos y todos los objetos asociados a ella\n",
    "drop_database(database)\n",
    "\n",
    "# Eliminar todos los archivos en el directorio especificado por `saveDirectory`\n",
    "# Esto puede incluir los archivos de datos previamente guardados\n",
    "delete_all_files_in_directory_path(saveDirectory)\n",
    "\n",
    "# Eliminar el valor asignado al parámetro del notebook en el widget\n",
    "# Esto limpia el valor del parámetro del widget, lo que puede ser útil si ya no es necesario\n",
    "remove_assign_notebook_parameter_value_from_widget()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "1_Bronze",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
