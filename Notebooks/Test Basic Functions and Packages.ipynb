{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ee468fd-9eeb-4ef5-b6f0-23b5ce52593d",
   "metadata": {},
   "source": [
    "# ITABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa5b693-e635-45c4-a0ba-ef236d8a69d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to enable itables\n",
    "from itables import init_notebook_mode\n",
    "init_notebook_mode(all_interactive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e459ee7-1417-4503-866a-b5f8e9d385da",
   "metadata": {},
   "source": [
    "# PYSPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9c5700-1e08-4872-bc91-507417ee0e21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"JorgeCardonaSpark\").getOrCreate()\n",
    "\n",
    "# Perform a simple DataFrame operation\n",
    "data = [('Nathalie', 0), ('Ana', 3), ('Diana', 7), ('Lucia', 10), ('Tatiana', 13), ('Angela', 17), ('Cecilia', 25), ('Alice', 31), ('Kristin', 35), ('Carolina', 37), ('Lina', 39), ('Marcela', 40), ('Maria', 42)]\n",
    "\n",
    "# Create a Dataframe\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "df.show()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16bbe1e-05c4-484e-bb71-55cd641f5998",
   "metadata": {},
   "source": [
    "# PANDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9667d438-7025-4575-a9cc-d27c2dd2e974",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Name': [\"Nathalie\", \"Ana\", \"Diana\", \"Lucia\", \"Tatiana\", \"Angela\", \"Cecilia\", \"Alice\", \"Kristin\", \"Carolina\", \"Lina\", \"Marcela\", \"Maria\"],\n",
    "    'Age': [0, 3, 7, 10, 13, 17, 25, 31, 35, 37, 39, 40, 42]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aa7c4d-fdbb-42a1-a902-ea6315ed4bb4",
   "metadata": {},
   "source": [
    "# APACHE BEAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f1db1c-3759-4ae8-ab9d-eb51e7d33234",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "def regular_case_function(element):\n",
    "    return element.lower()\n",
    "\n",
    "def to_uppercase_function(element):\n",
    "    return element.upper()\n",
    "\n",
    "def calculate_length_function(element):\n",
    "    return len(element)\n",
    "\n",
    "def calculate_square_function(element):\n",
    "    return element ** 2\n",
    "\n",
    "# Create a pipeline\n",
    "with beam.Pipeline() as pipeline:\n",
    "    # Prepare a list of names to be processed\n",
    "    names_list = [\"Nathalie\", \"Ana\", \"Diana\", \"Lucia\", \"Tatiana\", \"Angela\", \"Cecilia\", \"Alice\", \"Kristin\", \"Carolina\", \"Lina\", \"Marcela\", \"Maria\"]\n",
    "\n",
    "    # Create a PCollection with the given data\n",
    "    data = pipeline | beam.Create(names_list)\n",
    "\n",
    "    # Apply transformation functions to the data\n",
    "    regular_case_data = data | beam.Map(regular_case_function) # Transform to lowercase\n",
    "    uppercase_data = data | beam.Map(to_uppercase_function) # Transform to uppercase\n",
    "    length_data = data | beam.Map(calculate_length_function) # Apply transformation to calculate the length of each name\n",
    "    square_data = length_data | beam.Map(calculate_square_function) # Apply transformation to calculate the square\n",
    "\n",
    "    # Print the results of each transformation\n",
    "    length_data | \"Show_Length\" >> beam.Map(print) # Print length results\n",
    "    regular_case_data | \"Show_Lowercase\" >> beam.Map(print) # Print lowercase results\n",
    "    uppercase_data | \"Show_Uppercase\" >> beam.Map(print) # Print uppercase results\n",
    "    square_data | \"Show_Square\" >> beam.Map(print) # Print square results\n",
    "    combined_data = (length_data, regular_case_data, uppercase_data, square_data) | beam.Flatten()\n",
    "    combined_data | \"Show_All\" >> beam.Map(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6833db17-ce53-4a3c-8c5e-54bc2bb6a7dd",
   "metadata": {},
   "source": [
    "# FAKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d31392-44ae-4d4d-86a3-def39b274ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "fake = Faker()\n",
    "name = fake.name()\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace41d88-752b-4e19-9dff-752a8de64edf",
   "metadata": {},
   "source": [
    "# PANEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f22529-3346-45ee-bdaf-b1c36f9d477e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "\n",
    "def model(n=5):\n",
    "    return \"⭐\"*n\n",
    "\n",
    "pn.extension()\n",
    "\n",
    "slider = pn.widgets.IntSlider(value=5, start=1, end=5)\n",
    "\n",
    "interactive_model = pn.bind(model, n=slider)\n",
    "\n",
    "layout = pn.Column(slider, interactive_model)\n",
    "\n",
    "app = pn.serve(layout, port=5006, show=True)\n",
    "\n",
    "#app.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f164ebfd-a914-4e26-990f-c785f80b26a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e762273c-252e-41c7-b62d-ebab8e5d8994",
   "metadata": {},
   "source": [
    "# SEABORN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942ee40f-43b1-43b3-8160-6dfe86a6e552",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the \"tips\" dataset from Seaborn\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "\n",
    "# Create a scatter plot with Matplotlib\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(x='total_bill', y='tip', data=tips, alpha=0.7)\n",
    "plt.title('Scatter Plot of Total Bill vs Tip')\n",
    "plt.xlabel('Total Bill')\n",
    "plt.ylabel('Tip')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7078d2-862c-41c1-ba07-6b41f8c39e28",
   "metadata": {},
   "source": [
    "# BOKEH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723f281d-397e-432e-9344-c857d14f546a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the \"tips\" dataset from Seaborn\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "\n",
    "# Enable output in the notebook\n",
    "output_notebook()\n",
    "\n",
    "# Create a scatter plot using Bokeh with custom size\n",
    "p = figure(\n",
    "    title=\"Scatter Plot of Total Bill vs Tip\",\n",
    "    x_axis_label='Total Bill',\n",
    "    y_axis_label='Tip',\n",
    "    width=900, # adjust as needed\n",
    "    height=500 # adjust as needed\n",
    ")\n",
    "\n",
    "# Add the data to the plot\n",
    "p.circle(x='total_bill', y='tip', source=tips, size=8, color=\"navy\", alpha=0.5)\n",
    "\n",
    "# Show the plot in the notebook\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbc8ab9-7b6f-4121-9cdf-f607343ce7fc",
   "metadata": {},
   "source": [
    "# DIAGRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4178112-22be-451f-9ab5-464a911f1042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diagrams import Diagram\n",
    "from diagrams.aws.compute import EC2\n",
    "from diagrams.aws.database import RDS\n",
    "from diagrams.aws.network import ELB\n",
    "\n",
    "with Diagram(\"Grouped Workers\", show=False, direction=\"TB\"):\n",
    "    ELB(\"lb\") >> [\n",
    "                  EC2(\"worker1\"),\n",
    "                  EC2(\"worker2\"),\n",
    "                  EC2(\"worker3\"),\n",
    "                  EC2(\"worker4\"),\n",
    "                  EC2(\"worker5\")\n",
    "                  ] >> RDS(\"events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f24fe03-d6ab-455a-91fb-9f8d1e92e7de",
   "metadata": {},
   "source": [
    "# CODE FOR TESTING CONNECTION TO DATABASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048798d4-a782-418f-82fa-a73ccd27b0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mongo_connection(host, port, database, collection, user=None, password=None):\n",
    "    \"\"\"\n",
    "    host = \"host.docker.internal\"  # Replace with the IP address or hostname of your MongoDB server\n",
    "    port = 27017\n",
    "    database = \"spark\"\n",
    "    collection = \"users\"\n",
    "    user = \"admin\"  # Username (optional, if MongoDB is configured with authentication)\n",
    "    password = \"12345678\"  # Password (optional, if MongoDB is configured with authentication)\n",
    "    # If you want to use authentication, you need to provide credentials\n",
    "    # mongodb://admin:12345678@localhost:27017\n",
    "    # test the connection\n",
    "    test_mongo_connection(host, port, database, collection)\n",
    "    test_mongo_connection(host, port, database, collection, user, password)\n",
    "    \"\"\"\n",
    "    from pymongo import MongoClient\n",
    "    try:\n",
    "        client = MongoClient(host, port, username=user, password=password)\n",
    "        if not user or not password:\n",
    "            client = MongoClient(host, port)\n",
    "        db = client[database]\n",
    "        # You can use any query here; for example, count_documents({})\n",
    "        collection_loaded = db[collection]\n",
    "        result = collection_loaded.find_one()\n",
    "        if result:\n",
    "            print(\"Connection successful. MongoDB server is accessible.\", result)\n",
    "        else:\n",
    "            print(\"Connection successful, but no data was found in the database.\")\n",
    "        client.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to the database: {e}\")\n",
    "                \n",
    "def test_postgres_connection(host, port, database, user, password):\n",
    "    \"\"\"\n",
    "    host = \"host.docker.internal\"  # Replace with the IP address or hostname of your PostgreSQL server\n",
    "    port = 5432\n",
    "    database = \"spark\"\n",
    "    user = \"admin\"\n",
    "    password = \"12345678\"\n",
    "    # test the connection\n",
    "    test_postgres_connection(host, port, database, user, password)\n",
    "    \"\"\"\n",
    "    import psycopg2\n",
    "    try:\n",
    "        connection = psycopg2.connect(host=host, port=port, database=database, user=user, password=password)\n",
    "        connection.close()\n",
    "        print(\"Connection successful. PostgreSQL server is accessible.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to the database: {e}\")\n",
    "                \n",
    "def test_mysql_connection(host, port, database, user, password):\n",
    "    \"\"\"\n",
    "    host = \"host.docker.internal\"  # Replace with the IP address or hostname of your MySQL server\n",
    "    port = 3306  # Default port for MySQL\n",
    "    database = \"spark\"  # Name of the database you want to connect to\n",
    "    user = \"admin\"  # Username\n",
    "    password = \"12345678\"  # Password\n",
    "    # test the connection\n",
    "    test_mysql_connection(host, port, database, user, password)\n",
    "    \"\"\"    \n",
    "    import mysql.connector\n",
    "    try:\n",
    "        connection = mysql.connector.connect(host=host, port=port, database=database, user=user, password=password)\n",
    "        connection.close()\n",
    "        print(\"Connection successful. MySQL server is accessible.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to the database: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c52161-3a1a-4a79-8d03-05e63142ee7b",
   "metadata": {},
   "source": [
    "# DATABASES CONFIGURATION USING SPARK SESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9422c22-1407-400d-ba7b-baf1aaafd9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_database_configuration(database_type = 'mysql', host = None, port = None, database = None, table = None, user = None, password = None, input_collection = None, output_collection = None):\n",
    "    databases = {\n",
    "        'mongodb': {\n",
    "            'app_name': 'MongoDB_Connector',\n",
    "            'format_type':'mongodb',\n",
    "            'host': host if database_type == 'mongodb' and host else 'host.docker.internal',\n",
    "            'port': port if database_type == 'mongodb' and port else 27017,\n",
    "            'user': user if database_type == 'mongodb' and user else 'admin',\n",
    "            'password': password if database_type == 'mongodb' and password else '12345678',\n",
    "            'database': database if database_type == 'mongodb' and database else 'spark',\n",
    "            'input_collection':  input_collection if database_type == 'mongodb' and table else 'users',\n",
    "            'output_collection': output_collection if database_type == 'mongodb' and table else 'users',\n",
    "            'driver': 'com.mongodb.spark.sql.DefaultSource',\n",
    "            'url': f\"mongodb://{user}:{password}@{host}:{port}\" if database_type == 'mongodb' and host and port else 'mongodb://admin:12345678@host.docker.internal:27017'\n",
    "            },\n",
    "        'postgres': {\n",
    "            'app_name': 'PostgreSQL_Connector',\n",
    "            'format_type':'jdbc',\n",
    "            'host': host if database_type == 'postgres' and host else 'host.docker.internal',\n",
    "            'port': port if database_type == 'postgres' and port else 5432,\n",
    "            'user': user if database_type == 'postgres' and user else 'admin',\n",
    "            'password': password if database_type == 'postgres' and password else '12345678',\n",
    "            'database': database if database_type == 'postgres' and database else 'spark',\n",
    "            'table': table if database_type == 'postgres' and table else 'users',\n",
    "            'schema': 'public',\n",
    "            'spark_jars': '/usr/local/spark/jars/postgresql-42.7.1.jar',\n",
    "            'driver': 'org.postgresql.Driver',\n",
    "            'url': f\"jdbc:postgresql://{host}:{port}/{database}\" if database_type == 'postgres' and host and port else 'jdbc:postgresql://host.docker.internal:5432/spark',\n",
    "            'properties': {\n",
    "                'user': user if database_type == 'postgres' and user else 'admin',\n",
    "                'password': password if database_type == 'postgres' and password else '12345678',\n",
    "                'driver': 'org.postgresql.Driver'\n",
    "                            }\n",
    "            },\n",
    "        'mysql': {\n",
    "            'app_name': 'MySQL_Connector',\n",
    "            'format_type':'jdbc',\n",
    "            'host': host if database_type == 'mysql' and host else 'host.docker.internal',\n",
    "            'port': port if database_type == 'mysql' and port else 3306,\n",
    "            'user': user if database_type == 'mysql' and user else 'admin',\n",
    "            'password': password if database_type == 'mysql' and password else '12345678',\n",
    "            'database': database if database_type == 'mysql' and database else 'spark',\n",
    "            'table': table if database_type == 'mysql' and table else 'users',\n",
    "            'spark_jars': '/usr/local/spark/jars/mysql-connector-j-8.2.0.jar',\n",
    "            'driver': 'com.mysql.cj.jdbc.Driver',\n",
    "            'url': f\"jdbc:mysql://{host}:{port}/{database}\" if database_type == 'mysql' and host and port else 'jdbc:mysql://host.docker.internal:3306/spark',\n",
    "            'properties': { \n",
    "                            'user': user if database_type == 'mysql' and user else 'admin', \n",
    "                            'password': password if database_type == 'mysql' and password else '12345678', \n",
    "                            'driver': 'com.mysql.cj.jdbc.Driver'\n",
    "            }\n",
    "                            }\n",
    "    }\n",
    "    return databases.get(database_type.lower(), databases.get('mysql'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ad2e4f-9d98-4237-b02d-1d3236152901",
   "metadata": {},
   "source": [
    "# INSERT DATA - TEST DATABASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a853bd34-3fe9-4a69-9717-9f8990244f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_data():\n",
    "    data = [(1, \"Ana\"), (2, \"Cecilia\"), (3, \"Nathalie\"), (4, \"Diana\"), (5, \"Gabriela\"), (6, \"Angela\"), (7, \"Tatiana\"), (8, \"Lucia\"), (9, \"Maria\")]\n",
    "    columns = [\"Id\", \"Name\"]\n",
    "    return data, columns\n",
    "\n",
    "def insert_data_to_database(database_configuration, database_type=None):\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import monotonically_increasing_id\n",
    "    \n",
    "    app_name = database_configuration.get('app_name')\n",
    "    format_type = database_configuration.get('format_type')\n",
    "    database = database_configuration.get('database')\n",
    "    user = database_configuration.get('user')\n",
    "    password = database_configuration.get('password')\n",
    "    driver = database_configuration.get('driver')\n",
    "    url = database_configuration.get('url')\n",
    "    spark_session = SparkSession.builder.master('local').appName(app_name)\n",
    "    \n",
    "    data, columns = generate_sample_data()\n",
    "\n",
    "    try:\n",
    "        message = f'Records Inserted Successfully in {app_name}'\n",
    "        if database_type == 'mongodb':\n",
    "            collection = database_configuration.get('output_collection')\n",
    "            spark_session = spark_session.getOrCreate()\n",
    "            sampleDF = spark_session.createDataFrame(data, columns)\n",
    "            sampleDF_with_id = sampleDF.withColumn(\"id\", monotonically_increasing_id()) # add column 'id' to DataFrame\n",
    "\n",
    "            sampleDF_with_id.write.format(\"mongodb\") \\\n",
    "            .option(\"connection.uri\", url).option(\"database\", database) \\\n",
    "            .option(\"collection\", collection).mode(\"append\").save()\n",
    "        else: \n",
    "            dbtable = database_configuration.get('table')\n",
    "            spark_jars = database_configuration.get('spark_jars')\n",
    "            spark_session = spark_session.config(\"spark.jars\", spark_jars)\n",
    "            spark_session = spark_session.config(\"spark.jars\", spark_jars).getOrCreate()\n",
    "            sampleDF = spark_session.createDataFrame(data, columns)\n",
    "            sampleDF.write \\\n",
    "                .format(format_type).option(\"driver\", driver) \\\n",
    "                .option(\"url\", url).option(\"dbtable\", dbtable) \\\n",
    "                .option(\"user\", user).option(\"password\", password) \\\n",
    "                .mode(\"ignore\").mode(\"append\").save()\n",
    "    except Exception as e:\n",
    "        message = f\"Error inserting data: {str(e)}\"\n",
    "    finally:\n",
    "        spark_session.stop() # stop Spark session\n",
    "        return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66144b39-42ba-43f1-b47b-375e3c231c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql_configuration = get_database_configuration(database_type = 'mysql')\n",
    "insert_data_to_database(mysql_configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d907fd1a-ef35-4864-a128-1626f730778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_configuration = get_database_configuration(database_type = 'postgres')\n",
    "insert_data_to_database(postgres_configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9623d8a-c5f3-4658-a8f9-c03460a9dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongodb_configuration = get_database_configuration(database_type = 'mongodb')\n",
    "insert_data_to_database(database_configuration=mongodb_configuration, database_type = 'mongodb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6f3652-87d4-412b-b1a6-7f55bb73c4c3",
   "metadata": {},
   "source": [
    "# READ DATA - TEST DATABASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750d2044-c963-4e35-aace-78d172e51b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_from_database(database_type='mysql', host=None, port=None, database=None, table=None, user=None, password=None, input_collection=None, output_collection=None):\n",
    "    database_configuration = get_database_configuration(database_type=database_type, host=host, port=port, database=database, table=table, user=user, password=password, input_collection=input_collection, output_collection=output_collection)\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark_session =  SparkSession.builder.master('local').appName(f'Read data from {database_type}').getOrCreate()\n",
    "    properties = database_configuration.get('properties')\n",
    "    url = database_configuration.get('url')\n",
    "    try:\n",
    "        if database_type == 'mongodb':\n",
    "            database = database_configuration.get('database')\n",
    "            collection = database_configuration.get('input_collection')\n",
    "            result = spark_session.read.format(\"mongodb\").option(\"connection.uri\", url).option(\"database\", database).option(\"collection\", collection).load()\n",
    "        else:\n",
    "            table = database_configuration.get('table')\n",
    "            if database_configuration.get('schema'):\n",
    "                table = f\"{database_configuration.get('schema')}.{database_configuration.get('table')}\"\n",
    "            result = spark_session.read.jdbc(url=url, table=table, properties=properties)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading data: {str(e)}\")\n",
    "        result = None  # Another action you may want to take in case of an exception\n",
    "    finally:\n",
    "        result.printSchema()  # Print schema\n",
    "        result.show()  # Show rows\n",
    "        df = result.toPandas() # converts to pandas\n",
    "        spark_session.stop()\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c974f737-b3a6-436b-badf-2f24903f67ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_data_from_database(database_type = 'mysql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3ff1a1-7bb1-430b-9536-6fda1361fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_data_from_database(database_type = 'postgres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc5cc3a-3fa4-4474-96f8-cec9fa127db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_data_from_database(database_type = 'mongodb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdf01c5-5619-43ae-88e0-be271fe9d636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
