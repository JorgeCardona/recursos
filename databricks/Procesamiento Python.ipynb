{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bacc6dc0-56e3-4573-85cd-71ccfb10d19b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Asignar valor de parámetro en el notebook desde un widget de texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92baea1e-7f9b-4a6f-9592-b572f903892e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def assign_notebook_parameter_value_from_text_widget(widget_config: dict) -> None:\n",
    "    \"\"\"\n",
    "    Crea widgets de texto en Databricks y asigna sus valores a variables de Python en el notebook.\n",
    "\n",
    "    Esta función itera sobre un diccionario de widgets, creando cada widget de texto en Databricks y almacenando su\n",
    "    valor en una variable de Python correspondiente al nombre del widget.\n",
    "\n",
    "    Parámetros:\n",
    "    - widget_config (dict): Un diccionario donde cada clave es el nombre del widget, y el valor es otro diccionario\n",
    "                             que contiene 'value' (valor por defecto del widget) y 'label' (etiqueta del widget).\n",
    "                             Ejemplo:\n",
    "                             {\n",
    "                                 'redWineUrl': {'value': 'http://example.com/redWine.csv', 'label': 'URL del Vino Tinto'},\n",
    "                                 'saveDirectory': {'value': '/path/to/save', 'label': 'Directorio de Guardado' }\n",
    "                             }\n",
    "\n",
    "    Nota:\n",
    "    - Esta función no devuelve ningún valor. Modifica los widgets de Databricks y asigna los valores de los widgets\n",
    "      a las variables de Python correspondientes en el ámbito del notebook.\n",
    "    \"\"\"\n",
    "    for widget_name, widget_info in widget_config.items():\n",
    "        # Crear el widget de texto\n",
    "        dbutils.widgets.text(widget_name, widget_info['value'], widget_info['label'])\n",
    "\n",
    "    # Imprimir mensaje de éxito después de crear todos los widgets\n",
    "    print(\"Proceso completado: Todos los widgets de texto han sido creados y asignados correctamente.\")\n",
    "\n",
    "\n",
    "def remove_assign_notebook_parameter_value_from_widget():\n",
    "    \"\"\"\n",
    "    Elimina todos los widgets en el notebook de Databricks.\n",
    "    \"\"\"\n",
    "    \n",
    "    dbutils.widgets.removeAll()\n",
    "    print(\"Todos los widgets han sido eliminados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9f1635a-8a54-4ba8-9499-f9f35b000548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado: Todos los widgets de texto han sido creados y asignados correctamente.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configuración de widgets con valores y etiquetas\n",
    "# Definir la URL base\n",
    "url_base = 'https://raw.githubusercontent.com/JorgeCardona/data-collection-json-csv-sql/refs/heads/main/csv/wine'\n",
    "\n",
    "# Definir la ruta común para el directorio de guardado\n",
    "save_directory = '/FileStore/tables'  # Sin la barra final\n",
    "\n",
    "# Definir los nombres de los datasets en formato snake_case\n",
    "dataset_1_name = \"winequality-red.csv\"\n",
    "dataset_2_name = \"winequality-white.csv\"\n",
    "\n",
    "# Configuración de widgets con valores y etiquetas para los datasets\n",
    "widget_config = {\n",
    "    \"save_directory\": {\"value\": f\"{save_directory}\", \"label\": f\"save_directory ({save_directory})\"},\n",
    "    \"dataset_1_name\": {\"value\": f\"{dataset_1_name}\", \"label\": f\"Dataset name ({dataset_1_name})\"},\n",
    "    \"dataset_2_name\": {\"value\": f\"{dataset_2_name}\", \"label\": f\"Dataset name ({dataset_2_name})\"},\n",
    "    \"dataset1\": {\"value\": f\"{url_base}/{dataset_1_name}\", \"label\": f\"Dataset 1 URL ({dataset_1_name})\"},\n",
    "    \"dataset2\": {\"value\": f\"{url_base}/{dataset_2_name}\", \"label\": f\"Dataset 2 URL ({dataset_2_name})\"},\n",
    "    \"saveDirectory\": {\"value\": save_directory, \"label\": \"Save Directory\"},\n",
    "    \"dataset1FileLocation\": {\"value\": f\"{save_directory}/{dataset_1_name}\", \"label\": f\"Dataset 1 File Location ({dataset_1_name})\"},\n",
    "    \"dataset2FileLocation\": {\"value\": f\"{save_directory}/{dataset_2_name}\", \"label\": f\"Dataset 2 File Location ({dataset_2_name})\"},\n",
    "    \"fileType\": {\"value\": \"csv\", \"label\": \"File Type\"},\n",
    "    \"inferSchema\": {\"value\": \"true\", \"label\": \"Infer Schema\"},\n",
    "    \"firstRowIsHeader\": {\"value\": \"true\", \"label\": \"First Row Is Header\"},\n",
    "    \"delimiter\": {\"value\": \",\", \"label\": \"Delimiter\"},\n",
    "    \"bronze\": {\"value\": \"db_bronze\", \"label\": \"Data Base Bronze\"},\n",
    "    \"silver\": {\"value\": \"db_silver\", \"label\": \"Data Base Silver\"},\n",
    "    \"gold\": {\"value\": \"db_gold\", \"label\": \"Data Base Gold\"},\n",
    "    \"dataset_1_table\": {\"value\": \"red_wine\", \"label\": \"Table Name Dataset 1\"},\n",
    "    \"dataset_2_table\": {\"value\": \"white_wine\", \"label\": \"Table Name Dataset 2\"}\n",
    "}\n",
    "\n",
    "# Llamar la función con el diccionario configurado\n",
    "assign_notebook_parameter_value_from_text_widget(widget_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e8e5c05-db73-4a60-aefe-5fdf48c9eca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# FUNCIONES PARA DESCARGAR, LISTAR Y ELIMINAR ARCHIVOS EN UN DIRECTORIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66ce439e-ecd9-4a44-94bb-f2c8a3aa7abc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# Función para descargar y guardar los archivos en databricks\n",
    "\n",
    "def download_dataset_and_store_in_dbfs(url: str, save_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Descarga un archivo desde una URL y lo guarda en el Databricks File System (DBFS).\n",
    "\n",
    "    Esta función descarga el contenido de un archivo desde una URL dada y lo guarda en una ruta dentro de DBFS.\n",
    "\n",
    "    Parámetros:\n",
    "    - url (str): URL desde donde se descargará el archivo.\n",
    "    - save_path (str): Ruta completa en DBFS donde se guardará el archivo descargado.\n",
    "\n",
    "    Excepciones:\n",
    "    - requests.exceptions.RequestException: Si ocurre un error durante la descarga del archivo.\n",
    "    - Exception: Para cualquier otro error inesperado, como problemas al guardar el archivo en DBFS.\n",
    "\n",
    "    Ejemplo de uso:\n",
    "    ```python\n",
    "    download_dataset_and_store_in_dbfs(\"https://example.com/file.csv\", \"/dbfs/tmp/data/file.csv\")\n",
    "    ```\n",
    "\n",
    "    El archivo será guardado en la ubicación especificada dentro de DBFS.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Descargar el contenido del archivo\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Verifica si la solicitud fue exitosa\n",
    "\n",
    "        # Usar dbutils.fs.put para guardar el contenido en DBFS\n",
    "        dbutils.fs.put(save_path, response.text, overwrite=True)\n",
    "\n",
    "        print(f\"Archivo guardado correctamente en {save_path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error al descargar el archivo desde {url}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error inesperado al guardar el archivo en DBFS: {e}\")\n",
    "\n",
    "\n",
    "def list_files_in_directory_path(directory_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Lista los archivos dentro de un directorio especificado. Si no hay archivos, imprime un mensaje indicando\n",
    "    que el directorio está vacío.\n",
    "\n",
    "    Parámetros:\n",
    "    - directory_path (str): Ruta completa del directorio cuyo contenido se desea listar.\n",
    "\n",
    "    Retorno:\n",
    "    - None: Esta función no retorna ningún valor, solo imprime el contenido del directorio o un mensaje de vacío.\n",
    "    \"\"\"\n",
    "    files = dbutils.fs.ls(directory_path)\n",
    "    if not files:\n",
    "        print(f\"No hay archivos en el directorio {directory_path}.\")\n",
    "    else:\n",
    "        print(f\"Archivos en el directorio {directory_path}:\")\n",
    "        for file in files:\n",
    "            print(file.path)\n",
    "\n",
    "def delete_all_files_in_directory_path(directory_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Elimina todos los archivos dentro de un directorio especificado. Si el directorio está vacío, imprime un mensaje \n",
    "    indicando que no hay archivos para eliminar.\n",
    "\n",
    "    Parámetros:\n",
    "    - directory_path (str): Ruta completa del directorio cuyo contenido se desea eliminar.\n",
    "\n",
    "    Retorno:\n",
    "    - None: Esta función no retorna ningún valor, solo imprime el estado de eliminación de los archivos.\n",
    "    \"\"\"\n",
    "    # Listar todos los archivos en el directorio\n",
    "    files = dbutils.fs.ls(directory_path)\n",
    "\n",
    "    if not files:\n",
    "        print(f\"No hay archivos para eliminar en el directorio {directory_path}.\\n\")\n",
    "    else:\n",
    "        # Eliminar cada archivo encontrado\n",
    "        print(f\"Eliminando archivos en el directorio {directory_path}...\\n\")\n",
    "        for file in files:\n",
    "            dbutils.fs.rm(file.path, recurse=True)\n",
    "            print(f\"Archivo eliminado: {file.path}\")\n",
    "        \n",
    "        print(f\"\\nTodos los archivos en el directorio {directory_path} han sido eliminados.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f01074d6-5d25-4886-a795-9a9b573d499f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# FUNCION PARA EJECUTAR LOS QUERIES EN SPARKSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33bb2af6-8277-488b-aaac-59a9efdc581f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def execute_spark_sql_query(query: str) -> \"pyspark.sql.dataframe.DataFrame\":\n",
    "    \"\"\"\n",
    "    Ejecuta una consulta SQL en Spark y devuelve el resultado en un DataFrame.\n",
    "\n",
    "    Parámetros:\n",
    "    - query (str): Consulta SQL a ejecutar.\n",
    "\n",
    "    Retorno:\n",
    "    - pyspark.sql.dataframe.DataFrame: El DataFrame resultante de la ejecución de la consulta SQL.\n",
    "\n",
    "    Nota:\n",
    "    La función asume que la consulta SQL está correctamente formada y que se puede ejecutar en el entorno de Spark.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ejecutar el query SQL\n",
    "        result_df = spark.sql(query)\n",
    "        print(f\"La consulta SQL se ejecutó exitosamente. El número de registros obtenidos es: {result_df.count()}\")\n",
    "        return result_df\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error al ejecutar la consulta SQL: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89b143c2-7996-459b-bc4d-9366e14975f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# FUNCIONES PARA GUARDAR LOS DATAFRAMES COMOL DELTA Y PARQUET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c15de1e-5389-4a81-8630-ef0b70656226",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "def standardize_column_names_from_spark_dataframe(spark_dataframe: \"pyspark.sql.dataframe.DataFrame\") -> \"pyspark.sql.dataframe.DataFrame\":\n",
    "    \"\"\"\n",
    "    Estandariza los nombres de las columnas de un DataFrame de Spark reemplazando caracteres no alfanuméricos\n",
    "    por guiones bajos (\"_\"). Esta transformación ayuda a evitar problemas de formato en el procesamiento de datos.\n",
    "\n",
    "    Parámetros:\n",
    "    - spark_dataframe (pyspark.sql.dataframe.DataFrame): DataFrame de Spark con los nombres de columnas a estandarizar.\n",
    "\n",
    "    Retorno:\n",
    "    - pyspark.sql.dataframe.DataFrame: Nuevo DataFrame de Spark con los nombres de columnas estandarizados.\n",
    "    \"\"\"\n",
    "    for col_name in spark_dataframe.columns:\n",
    "        # Reemplazar caracteres no alfanuméricos por \"_\"\n",
    "        new_col_name = ''.join([char if char.isalnum() or char == '_' else '_' for char in col_name])\n",
    "        spark_dataframe = spark_dataframe.withColumnRenamed(col_name, new_col_name)\n",
    "    \n",
    "    print(f\"Los nombres de las columnas han sido estandarizados exitosamente. Las columnas ahora usan solo caracteres alfanuméricos y guiones bajos.\")\n",
    "    return spark_dataframe\n",
    "\n",
    "\n",
    "def save_dataframe_as_delta_format(spark_dataframe: \"pyspark.sql.dataframe.DataFrame\", path: str) -> None:\n",
    "    \"\"\"\n",
    "    Guarda un DataFrame de Spark en formato Delta en una ubicación específica de almacenamiento.\n",
    "\n",
    "    Parámetros:\n",
    "    - spark_dataframe (pyspark.sql.dataframe.DataFrame): DataFrame de Spark a guardar.\n",
    "    - path (str): Ruta en el sistema de archivos donde se guardará el DataFrame en formato Delta.\n",
    "\n",
    "    Retorno:\n",
    "    - None\n",
    "\n",
    "    Nota:\n",
    "    Esta función sobrescribe cualquier archivo existente en la ruta especificada.\n",
    "    \"\"\"\n",
    "    spark_dataframe = standardize_column_names_from_spark_dataframe(spark_dataframe)\n",
    "    \n",
    "    spark_dataframe.write.format(\"delta\").mode(\"overwrite\").save(path)\n",
    "    print(f\"El DataFrame ha sido guardado exitosamente en formato Delta en la ruta: {path}\")\n",
    "\n",
    "\n",
    "def save_dataframe_as_delta_table(spark_dataframe: \"pyspark.sql.dataframe.DataFrame\", database_name: str, table_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Guarda un DataFrame de Spark como una tabla Delta en una base de datos de Databricks.\n",
    "\n",
    "    Parámetros:\n",
    "    - spark_dataframe (pyspark.sql.dataframe.DataFrame): DataFrame de Spark a guardar.\n",
    "    - database_name (str): Nombre de la base de datos donde se creará la tabla.\n",
    "    - table_name (str): Nombre de la tabla Delta a crear en la base de datos.\n",
    "\n",
    "    Retorno:\n",
    "    - None\n",
    "\n",
    "    Nota:\n",
    "    Esta función sobrescribe cualquier tabla existente con el mismo nombre en la base de datos especificada.\n",
    "    \"\"\"\n",
    "    full_table_name = f\"{database_name}.{table_name}\"\n",
    "\n",
    "    spark_dataframe = standardize_column_names_from_spark_dataframe(spark_dataframe)\n",
    "\n",
    "    spark_dataframe.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_table_name)\n",
    "    print(f\"El DataFrame ha sido guardado exitosamente como tabla Delta en la base de datos: {database_name}, tabla: {table_name}\")\n",
    "\n",
    "\n",
    "def save_dataframe_as_parquet_format(spark_dataframe: \"pyspark.sql.dataframe.DataFrame\", path: str) -> None:\n",
    "    \"\"\"\n",
    "    Guarda un DataFrame de Spark en formato Parquet en una ubicación específica de almacenamiento.\n",
    "\n",
    "    Parámetros:\n",
    "    - spark_dataframe (pyspark.sql.dataframe.DataFrame): DataFrame de Spark a guardar.\n",
    "    - path (str): Ruta en el sistema de archivos donde se guardará el DataFrame en formato Parquet.\n",
    "\n",
    "    Retorno:\n",
    "    - None\n",
    "\n",
    "    Nota:\n",
    "    Esta función sobrescribe cualquier archivo existente en la ruta especificada.\n",
    "    \"\"\"\n",
    "    spark_dataframe.write.format(\"parquet\").mode(\"overwrite\").save(path)\n",
    "    print(f\"El DataFrame ha sido guardado exitosamente en formato Parquet en la ruta: {path}\")\n",
    "\n",
    "\n",
    "def save_dataframe_as_parquet_table(spark_dataframe: \"pyspark.sql.dataframe.DataFrame\", database_name: str, table_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Guarda un DataFrame de Spark como una tabla Parquet en una base de datos de Databricks.\n",
    "\n",
    "    Parámetros:\n",
    "    - spark_dataframe (pyspark.sql.dataframe.DataFrame): DataFrame de Spark a guardar.\n",
    "    - database_name (str): Nombre de la base de datos donde se creará la tabla.\n",
    "    - table_name (str): Nombre de la tabla Parquet a crear en la base de datos.\n",
    "\n",
    "    Retorno:\n",
    "    - None\n",
    "\n",
    "    Nota:\n",
    "    Esta función sobrescribe cualquier tabla existente con el mismo nombre en la base de datos especificada.\n",
    "    \"\"\"\n",
    "    full_table_name = f\"{database_name}.{table_name}\"\n",
    "    spark_dataframe.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(full_table_name)\n",
    "    print(f\"El DataFrame ha sido guardado exitosamente como tabla Parquet en la base de datos: {database_name}, tabla: {table_name}\")\n",
    "\n",
    "def save_dataframe_as_temp_table(spark_dataframe: \"pyspark.sql.dataframe.DataFrame\", table_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Guarda un DataFrame de Spark como una tabla temporal en Databricks.\n",
    "\n",
    "    Parámetros:\n",
    "    - spark_dataframe (pyspark.sql.dataframe.DataFrame): DataFrame de Spark a guardar como tabla temporal.\n",
    "    - table_name (str): Nombre de la tabla temporal a crear.\n",
    "\n",
    "    Retorno:\n",
    "    - None\n",
    "\n",
    "    Nota:\n",
    "    Esta función crea o reemplaza una tabla temporal, que existe solo durante la sesión activa de Spark.\n",
    "    \"\"\"\n",
    "    spark_dataframe.createOrReplaceTempView(table_name)\n",
    "    print(f\"El DataFrame ha sido guardado exitosamente como tabla temporal: {table_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b53a438-55e6-42d0-9f1b-2a2ecbd5206c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# FUNCION PARA LEER LOS ARCHIVOS DESCARGADOS COMO SPARK DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36efc388-ad6a-432e-8514-24c2586de499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def load_data_to_spark_dataframe(file_location: str, file_type: str = \"csv\", infer_schema: bool = True, \n",
    "                           first_row_is_header: bool = True, delimiter: str = \",\") -> \"DataFrame\":\n",
    "    \"\"\"\n",
    "    Carga un archivo en un DataFrame de Spark, con opciones configurables para tipos de archivo, inferencia de esquema,\n",
    "    encabezado, y delimitador.\n",
    "\n",
    "    Parámetros:\n",
    "    - file_location (str): La ubicación del archivo que se desea cargar.\n",
    "    - file_type (str): El tipo de archivo. Por defecto es 'csv'. Otros valores pueden incluir 'parquet', 'json', etc.\n",
    "    - infer_schema (bool): Si se debe inferir el esquema automáticamente del archivo. Por defecto es True.\n",
    "    - first_row_is_header (bool): Si la primera fila debe ser usada como encabezado. Por defecto es True.\n",
    "    - delimiter (str): El delimitador para los archivos CSV. Por defecto es ','.\n",
    "\n",
    "    Retorna:\n",
    "    - DataFrame: El DataFrame cargado con los datos del archivo.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Cargar el archivo en el DataFrame\n",
    "        df = spark.read.format(file_type) \\\n",
    "            .option(\"inferSchema\", infer_schema) \\\n",
    "            .option(\"header\", first_row_is_header) \\\n",
    "            .option(\"sep\", delimiter) \\\n",
    "            .load(file_location)\n",
    "        \n",
    "        print(f\"Archivo cargado correctamente desde {file_location}\")\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar el archivo desde {file_location}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76cd56e2-4db5-477e-86d8-afc664e1d49d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# FUNCION PARA LISTAR LAS TABLAS DE LA BASE DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc8fc09e-7d6e-4f9c-a31b-e7a6f5e81359",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_database(database_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Crea una base de datos en Databricks si no existe ya.\n",
    "\n",
    "    Parámetros:\n",
    "    - database_name (str): Nombre de la base de datos a crear.\n",
    "\n",
    "    Retorno:\n",
    "    - None\n",
    "\n",
    "    Ejemplo de uso:\n",
    "    ```python\n",
    "    create_database(\"nombre_de_la_base_de_datos\")\n",
    "    ```\n",
    "    \"\"\"\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "    print(f\"La base de datos '{database_name}' ha sido creada exitosamente (o ya existía).\")\n",
    "\n",
    "def drop_database(database_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Elimina una base de datos en Databricks si existe.\n",
    "\n",
    "    Parámetros:\n",
    "    - database_name (str): Nombre de la base de datos que se desea eliminar.\n",
    "\n",
    "    Retorno:\n",
    "    - None\n",
    "\n",
    "    Nota:\n",
    "    Esta función elimina la base de datos especificada únicamente si existe. Para eliminar una base de datos que contenga\n",
    "    tablas u objetos, utiliza la opción CASCADE dentro del comando SQL.\n",
    "\n",
    "    Ejemplo de uso:\n",
    "    ```python\n",
    "    drop_database(\"nombre_de_la_base_de_datos\")\n",
    "    ```\n",
    "    \"\"\"\n",
    "    spark.sql(f\"DROP DATABASE IF EXISTS {database_name} CASCADE\")\n",
    "    print(f\"La base de datos '{database_name}' ha sido eliminada exitosamente (si existía).\")\n",
    "\n",
    "def list_tables_in_database(database_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Lista y muestra todas las tablas en una base de datos específica en Databricks.\n",
    "\n",
    "    Parámetros:\n",
    "    - database_name (str): Nombre de la base de datos de la cual se quieren listar las tablas.\n",
    "\n",
    "    Retorno:\n",
    "    - None\n",
    "\n",
    "    Ejemplo de uso:\n",
    "    ```python\n",
    "    list_tables_in_database(\"mi_base_de_datos\")\n",
    "    ```\n",
    "    \"\"\"\n",
    "    tablas = spark.sql(f\"SHOW TABLES IN {database_name}\")\n",
    "    display(tablas)\n",
    "\n",
    "def list_temp_tables(show_global: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Lista las tablas temporales en la sesión de Spark.\n",
    "\n",
    "    Parámetros:\n",
    "    - show_global (bool): Si es True, muestra tablas temporales globales (prefijadas con 'global_temp'). \n",
    "                          Si es False, muestra las tablas temporales locales de la sesión.\n",
    "\n",
    "    Retorno:\n",
    "    - None\n",
    "\n",
    "    Nota:\n",
    "    Esta función imprime las tablas temporales disponibles en la sesión actual de Spark.\n",
    "    \"\"\"\n",
    "    if show_global:\n",
    "        global_temp_tables = [table.name for table in spark.catalog.listTables(\"global_temp\")]\n",
    "        print(\"Tablas temporales globales:\")\n",
    "        for table in global_temp_tables:\n",
    "            print(f\"- global_temp.{table}\")\n",
    "    else:\n",
    "        temp_tables = [table.name for table in spark.catalog.listTables() if table.isTemporary]\n",
    "        print(\"Tablas temporales locales:\")\n",
    "        for table in temp_tables:\n",
    "            print(f\"- {table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11fa9fec-6c22-439d-be32-cf1769aedeec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DESCARGAR LOS DATASETS ORIGINALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11252095-238e-4b2e-8c17-22bc380a5ee8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 100951 bytes.\nArchivo guardado correctamente en /FileStore/tables/winequality-red.csv\nWrote 264426 bytes.\nArchivo guardado correctamente en /FileStore/tables/winequality-white.csv\nArchivos en el directorio /FileStore/tables:\ndbfs:/FileStore/tables/winequality-red.csv\ndbfs:/FileStore/tables/winequality-white.csv\n"
     ]
    }
   ],
   "source": [
    "# Recuperar los valores de los widgets\n",
    "dataset1Url = dbutils.widgets.get(\"dataset1\")\n",
    "dataset2Url = dbutils.widgets.get(\"dataset2\")\n",
    "saveDirectory = dbutils.widgets.get(\"saveDirectory\")\n",
    "dataset_1_name = dbutils.widgets.get(\"dataset_1_name\")\n",
    "dataset_2_name = dbutils.widgets.get(\"dataset_2_name\")\n",
    "\n",
    "# Descargar y guardar los archivos de vino tinto y vino blanco en el almacenamiento local\n",
    "download_dataset_and_store_in_dbfs(dataset1Url, os.path.join(saveDirectory, dataset_1_name))\n",
    "download_dataset_and_store_in_dbfs(dataset2Url, os.path.join(saveDirectory, dataset_2_name))\n",
    "\n",
    "# listar los archivos del directorio creado\n",
    "list_files_in_directory_path(saveDirectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99589348-9dca-4f28-beb4-cec025ae8bdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## LEER LOS DATASETS DESCARGADOS COMO SPARK DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e58e1276-23a2-4385-b7c4-6f5c2722d670",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo cargado correctamente desde /FileStore/tables/winequality-red.csv\nArchivo cargado correctamente desde /FileStore/tables/winequality-white.csv\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>fixed acidity</th><th>volatile acidity</th><th>citric acid</th><th>residual sugar</th><th>chlorides</th><th>free sulfur dioxide</th><th>total sulfur dioxide</th><th>density</th><th>pH</th><th>sulphates</th><th>alcohol</th><th>quality</th></tr></thead><tbody><tr><td>7.4</td><td>0.7</td><td>0.0</td><td>1.9</td><td>0.076</td><td>11.0</td><td>34.0</td><td>0.9978</td><td>3.51</td><td>0.56</td><td>9.4</td><td>5</td></tr><tr><td>7.8</td><td>0.88</td><td>0.0</td><td>2.6</td><td>0.098</td><td>25.0</td><td>67.0</td><td>0.9968</td><td>3.2</td><td>0.68</td><td>9.8</td><td>5</td></tr><tr><td>7.8</td><td>0.76</td><td>0.04</td><td>2.3</td><td>0.092</td><td>15.0</td><td>54.0</td><td>0.997</td><td>3.26</td><td>0.65</td><td>9.8</td><td>5</td></tr><tr><td>11.2</td><td>0.28</td><td>0.56</td><td>1.9</td><td>0.075</td><td>17.0</td><td>60.0</td><td>0.998</td><td>3.16</td><td>0.58</td><td>9.8</td><td>6</td></tr><tr><td>7.4</td><td>0.7</td><td>0.0</td><td>1.9</td><td>0.076</td><td>11.0</td><td>34.0</td><td>0.9978</td><td>3.51</td><td>0.56</td><td>9.4</td><td>5</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         7.4,
         0.7,
         0.0,
         1.9,
         0.076,
         11.0,
         34.0,
         0.9978,
         3.51,
         0.56,
         9.4,
         5
        ],
        [
         7.8,
         0.88,
         0.0,
         2.6,
         0.098,
         25.0,
         67.0,
         0.9968,
         3.2,
         0.68,
         9.8,
         5
        ],
        [
         7.8,
         0.76,
         0.04,
         2.3,
         0.092,
         15.0,
         54.0,
         0.997,
         3.26,
         0.65,
         9.8,
         5
        ],
        [
         11.2,
         0.28,
         0.56,
         1.9,
         0.075,
         17.0,
         60.0,
         0.998,
         3.16,
         0.58,
         9.8,
         6
        ],
        [
         7.4,
         0.7,
         0.0,
         1.9,
         0.076,
         11.0,
         34.0,
         0.9978,
         3.51,
         0.56,
         9.4,
         5
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "fixed acidity",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "volatile acidity",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "citric acid",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "residual sugar",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "chlorides",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "free sulfur dioxide",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "total sulfur dioxide",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "density",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "pH",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "sulphates",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "alcohol",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "quality",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>fixed acidity</th><th>volatile acidity</th><th>citric acid</th><th>residual sugar</th><th>chlorides</th><th>free sulfur dioxide</th><th>total sulfur dioxide</th><th>density</th><th>pH</th><th>sulphates</th><th>alcohol</th><th>quality</th></tr></thead><tbody><tr><td>7.0</td><td>0.27</td><td>0.36</td><td>20.7</td><td>0.045</td><td>45.0</td><td>170.0</td><td>1.001</td><td>3.0</td><td>0.45</td><td>8.8</td><td>6</td></tr><tr><td>6.3</td><td>0.3</td><td>0.34</td><td>1.6</td><td>0.049</td><td>14.0</td><td>132.0</td><td>0.994</td><td>3.3</td><td>0.49</td><td>9.5</td><td>6</td></tr><tr><td>8.1</td><td>0.28</td><td>0.4</td><td>6.9</td><td>0.05</td><td>30.0</td><td>97.0</td><td>0.9951</td><td>3.26</td><td>0.44</td><td>10.1</td><td>6</td></tr><tr><td>7.2</td><td>0.23</td><td>0.32</td><td>8.5</td><td>0.058</td><td>47.0</td><td>186.0</td><td>0.9956</td><td>3.19</td><td>0.4</td><td>9.9</td><td>6</td></tr><tr><td>7.2</td><td>0.23</td><td>0.32</td><td>8.5</td><td>0.058</td><td>47.0</td><td>186.0</td><td>0.9956</td><td>3.19</td><td>0.4</td><td>9.9</td><td>6</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         7.0,
         0.27,
         0.36,
         20.7,
         0.045,
         45.0,
         170.0,
         1.001,
         3.0,
         0.45,
         8.8,
         6
        ],
        [
         6.3,
         0.3,
         0.34,
         1.6,
         0.049,
         14.0,
         132.0,
         0.994,
         3.3,
         0.49,
         9.5,
         6
        ],
        [
         8.1,
         0.28,
         0.4,
         6.9,
         0.05,
         30.0,
         97.0,
         0.9951,
         3.26,
         0.44,
         10.1,
         6
        ],
        [
         7.2,
         0.23,
         0.32,
         8.5,
         0.058,
         47.0,
         186.0,
         0.9956,
         3.19,
         0.4,
         9.9,
         6
        ],
        [
         7.2,
         0.23,
         0.32,
         8.5,
         0.058,
         47.0,
         186.0,
         0.9956,
         3.19,
         0.4,
         9.9,
         6
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "fixed acidity",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "volatile acidity",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "citric acid",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "residual sugar",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "chlorides",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "free sulfur dioxide",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "total sulfur dioxide",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "density",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "pH",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "sulphates",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "alcohol",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "quality",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# File locations and types\n",
    "# Recuperar los valores de los widgets\n",
    "fileType = dbutils.widgets.get(\"fileType\")\n",
    "inferSchema = dbutils.widgets.get(\"inferSchema\")\n",
    "firstRowIsHeader = dbutils.widgets.get(\"firstRowIsHeader\")\n",
    "delimiter = dbutils.widgets.get(\"delimiter\")\n",
    "\n",
    "dataset_1_name = dbutils.widgets.get(\"dataset_1_name\")\n",
    "dataset_2_name = dbutils.widgets.get(\"dataset_2_name\")\n",
    "saveDirectory = dbutils.widgets.get(\"saveDirectory\")\n",
    "\n",
    "dataset_1_location = f\"{saveDirectory}/{dataset_1_name}\"\n",
    "dataset_2_location = f\"{saveDirectory}/{dataset_2_name}\"\n",
    "\n",
    "red_wine_df = load_data_to_spark_dataframe(dataset_1_location, fileType, inferSchema, firstRowIsHeader, delimiter)\n",
    "white_wine_df = load_data_to_spark_dataframe(dataset_2_location, fileType, inferSchema, firstRowIsHeader, delimiter)\n",
    "\n",
    "display(red_wine_df.limit(5))\n",
    "display(white_wine_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0a628ba-75e6-4f59-8f9d-8fb05d213fe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### GUARDA EL DATAFRAME EN FORMATO DELTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f3e59d0-2d2c-4fa3-8855-62f7ff5b0557",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los nombres de las columnas han sido estandarizados exitosamente. Las columnas ahora usan solo caracteres alfanuméricos y guiones bajos.\nEl DataFrame ha sido guardado exitosamente en formato Delta en la ruta: /FileStore/tables/Delta_dataset1\n\nLos nombres de las columnas han sido estandarizados exitosamente. Las columnas ahora usan solo caracteres alfanuméricos y guiones bajos.\nEl DataFrame ha sido guardado exitosamente en formato Delta en la ruta: /FileStore/tables/Delta_dataset2\n\nArchivos en el directorio /FileStore/tables/Delta_dataset1:\ndbfs:/FileStore/tables/Delta_dataset1/_delta_log/\ndbfs:/FileStore/tables/Delta_dataset1/part-00000-2eb6b2da-a7b5-456b-b08e-fef1983726c2-c000.snappy.parquet\n\nArchivos en el directorio /FileStore/tables/Delta_dataset2:\ndbfs:/FileStore/tables/Delta_dataset2/_delta_log/\ndbfs:/FileStore/tables/Delta_dataset2/part-00000-516c9ff8-d747-404f-81b3-efd5d5c484d1-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "saveDirectory = dbutils.widgets.get(\"saveDirectory\")\n",
    "\n",
    "deltaPath1 = f\"{saveDirectory}/Delta_dataset1\"\n",
    "deltaPath2 = f\"{saveDirectory}/Delta_dataset2\"\n",
    "\n",
    "save_dataframe_as_delta_format(red_wine_df, deltaPath1)\n",
    "print()\n",
    "save_dataframe_as_delta_format(white_wine_df, deltaPath2)\n",
    "\n",
    "print()\n",
    "# listar los archivos del directorio creado\n",
    "list_files_in_directory_path(deltaPath1)\n",
    "print()\n",
    "list_files_in_directory_path(deltaPath2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c73788f0-00ed-410f-8ba8-b9987cfd0347",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### GUARDA EL DATAFRAME EN FORMATO PARQUET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "459100b1-2569-4119-ab28-8e099aed147b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El DataFrame ha sido guardado exitosamente en formato Parquet en la ruta: /FileStore/tables/Parquet_dataset1\n\nEl DataFrame ha sido guardado exitosamente en formato Parquet en la ruta: /FileStore/tables/Parquet_dataset2\n\nArchivos en el directorio /FileStore/tables/Parquet_dataset1:\ndbfs:/FileStore/tables/Parquet_dataset1/_SUCCESS\ndbfs:/FileStore/tables/Parquet_dataset1/_committed_6795752328866930210\ndbfs:/FileStore/tables/Parquet_dataset1/_started_6795752328866930210\ndbfs:/FileStore/tables/Parquet_dataset1/part-00000-tid-6795752328866930210-652d195e-5da9-4ca5-8f5e-d50057712b95-443-1-c000.snappy.parquet\n\nArchivos en el directorio /FileStore/tables/Parquet_dataset2:\ndbfs:/FileStore/tables/Parquet_dataset2/_SUCCESS\ndbfs:/FileStore/tables/Parquet_dataset2/_committed_7937310117741971714\ndbfs:/FileStore/tables/Parquet_dataset2/_started_7937310117741971714\ndbfs:/FileStore/tables/Parquet_dataset2/part-00000-tid-7937310117741971714-24078c0e-66af-48d1-8883-64d69b46fbbc-444-1-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "saveDirectory = dbutils.widgets.get(\"saveDirectory\")\n",
    "\n",
    "parquetPath1  = f\"{saveDirectory}/Parquet_dataset1\"\n",
    "parquetPath2  = f\"{saveDirectory}/Parquet_dataset2\"\n",
    "\n",
    "save_dataframe_as_parquet_format(red_wine_df, parquetPath1)\n",
    "print()\n",
    "save_dataframe_as_parquet_format(white_wine_df, parquetPath2)\n",
    "\n",
    "print()\n",
    "# listar los archivos del directorio creado\n",
    "list_files_in_directory_path(parquetPath1)\n",
    "print()\n",
    "list_files_in_directory_path(parquetPath2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee5659b2-05e4-4f07-a024-53b849577c3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### GUARDA EL DATAFRAME COMO TABLA EN FORMATO DELTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "864d2cc9-9864-4bd3-b5d6-6cc4a3e1ff22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La base de datos 'db_bronze' ha sido creada exitosamente (o ya existía).\n"
     ]
    }
   ],
   "source": [
    "database = dbutils.widgets.get(\"bronze\")\n",
    "create_database(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dd2597a-062e-44ae-a2a8-64b6552e21c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los nombres de las columnas han sido estandarizados exitosamente. Las columnas ahora usan solo caracteres alfanuméricos y guiones bajos.\nEl DataFrame ha sido guardado exitosamente como tabla Delta en la base de datos: db_bronze, tabla: dataset1_delta\n\nLos nombres de las columnas han sido estandarizados exitosamente. Las columnas ahora usan solo caracteres alfanuméricos y guiones bajos.\nEl DataFrame ha sido guardado exitosamente como tabla Delta en la base de datos: db_bronze, tabla: dataset2_delta\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>database</th><th>tableName</th><th>isTemporary</th></tr></thead><tbody><tr><td>db_bronze</td><td>dataset1_delta</td><td>false</td></tr><tr><td>db_bronze</td><td>dataset2_delta</td><td>false</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "db_bronze",
         "dataset1_delta",
         false
        ],
        [
         "db_bronze",
         "dataset2_delta",
         false
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "database",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tableName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isTemporary",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "saveDirectory = dbutils.widgets.get(\"saveDirectory\")\n",
    "database = dbutils.widgets.get(\"bronze\")\n",
    "tabla_dataset1 = f'dataset1_delta'\n",
    "tableParquetPath1  = f\"{saveDirectory}/{tabla_dataset1}\"\n",
    "\n",
    "tabla_dataset2 = f'dataset2_delta'\n",
    "tableParquetPath2  = f\"{saveDirectory}/{tabla_dataset2}\"\n",
    "\n",
    "save_dataframe_as_delta_table(red_wine_df, database, tabla_dataset1)\n",
    "print()\n",
    "save_dataframe_as_delta_table(white_wine_df, database, tabla_dataset2)\n",
    "\n",
    "print()\n",
    "list_tables_in_database(database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f07c50b-91ba-4b09-a15e-93d7723d17ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### GUARDA EL DATAFRAME COMO TABLA EN FORMATO PARQUET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50fc2bc9-3e8b-4b46-b67d-5ddce3a10b64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El DataFrame ha sido guardado exitosamente como tabla Parquet en la base de datos: db_bronze, tabla: dataset1_parquet\n\nEl DataFrame ha sido guardado exitosamente como tabla Parquet en la base de datos: db_bronze, tabla: dataset2_parquet\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>database</th><th>tableName</th><th>isTemporary</th></tr></thead><tbody><tr><td>db_bronze</td><td>dataset1_delta</td><td>false</td></tr><tr><td>db_bronze</td><td>dataset1_parquet</td><td>false</td></tr><tr><td>db_bronze</td><td>dataset2_delta</td><td>false</td></tr><tr><td>db_bronze</td><td>dataset2_parquet</td><td>false</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "db_bronze",
         "dataset1_delta",
         false
        ],
        [
         "db_bronze",
         "dataset1_parquet",
         false
        ],
        [
         "db_bronze",
         "dataset2_delta",
         false
        ],
        [
         "db_bronze",
         "dataset2_parquet",
         false
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "database",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tableName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isTemporary",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "saveDirectory = dbutils.widgets.get(\"saveDirectory\")\n",
    "database = dbutils.widgets.get(\"bronze\")\n",
    "tabla_dataset1 = f'dataset1_parquet'\n",
    "tableParquetPath1  = f\"{saveDirectory}/{tabla_dataset1}\"\n",
    "\n",
    "tabla_dataset2 = f'dataset2_parquet'\n",
    "tableParquetPath2  = f\"{saveDirectory}/{tabla_dataset2}\"\n",
    "\n",
    "save_dataframe_as_parquet_table(red_wine_df, database, tabla_dataset1)\n",
    "print()\n",
    "save_dataframe_as_parquet_table(white_wine_df, database, tabla_dataset2)\n",
    "\n",
    "print()\n",
    "list_tables_in_database(database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1fd3e08-128b-48b7-99d7-e27c02a28410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## GUARDAR EL DATAFRAME COMO TABLAS TEMPORALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a8e20f6-a646-491b-bf16-8bbbfc0c55cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El DataFrame ha sido guardado exitosamente como tabla temporal: dataset1_temp\n\nEl DataFrame ha sido guardado exitosamente como tabla temporal: dataset2_temp\n\nTablas temporales locales:\n- dataset1_temp\n- dataset2_temp\n\nTablas temporales globales:\n- global_temp.dataset1_temp\n- global_temp.dataset2_temp\n"
     ]
    }
   ],
   "source": [
    "save_dataframe_as_temp_table\n",
    "\n",
    "saveDirectory = dbutils.widgets.get(\"saveDirectory\")\n",
    "tabla_dataset1 = f'dataset1_temp'\n",
    "tableParquetPath1  = f\"{saveDirectory}/{tabla_dataset1}\"\n",
    "\n",
    "tabla_dataset2 = f'dataset2_temp'\n",
    "tableParquetPath2  = f\"{saveDirectory}/{tabla_dataset2}\"\n",
    "\n",
    "save_dataframe_as_temp_table(red_wine_df, tabla_dataset1)\n",
    "print()\n",
    "save_dataframe_as_temp_table(white_wine_df, tabla_dataset2)\n",
    "\n",
    "print()\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# Listar tablas temporales locales\n",
    "list_temp_tables()\n",
    "\n",
    "print()\n",
    "\n",
    "# Listar tablas temporales globales\n",
    "list_temp_tables(show_global=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "851c25b4-8df2-4a99-acf1-399e6e433cd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ELIMINAS LOS PARAMETROS DEL NOTEBOOK Y LOS ARCHIVOS CREADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5acbe92-1244-4fcd-8e03-ce89fcb46061",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La base de datos 'db_bronze' ha sido eliminada exitosamente (si existía).\nEliminando archivos en el directorio /FileStore/tables...\n\nArchivo eliminado: dbfs:/FileStore/tables/Delta_dataset1/\nArchivo eliminado: dbfs:/FileStore/tables/Delta_dataset2/\nArchivo eliminado: dbfs:/FileStore/tables/Parquet_dataset1/\nArchivo eliminado: dbfs:/FileStore/tables/Parquet_dataset2/\nArchivo eliminado: dbfs:/FileStore/tables/winequality-red.csv\nArchivo eliminado: dbfs:/FileStore/tables/winequality-white.csv\n\nTodos los archivos en el directorio /FileStore/tables han sido eliminados.\n\nTodos los widgets han sido eliminados.\n"
     ]
    }
   ],
   "source": [
    "database = dbutils.widgets.get(\"bronze\")\n",
    "drop_database(database)\n",
    "\n",
    "delete_all_files_in_directory_path(saveDirectory)\n",
    "remove_assign_notebook_parameter_value_from_widget()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Procesamiento Python",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
