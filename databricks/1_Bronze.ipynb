{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bacc6dc0-56e3-4573-85cd-71ccfb10d19b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Asignar valor de parámetro en el notebook desde un widget de texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92baea1e-7f9b-4a6f-9592-b572f903892e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def assign_notebook_parameter_value_from_text_widget(widget_config: dict) -> None:\n",
    "    \"\"\"\n",
    "    Crea widgets de texto en Databricks y asigna sus valores a variables de Python en el notebook.\n",
    "\n",
    "    Esta función itera sobre un diccionario de widgets, creando cada widget de texto en Databricks y almacenando su\n",
    "    valor en una variable de Python correspondiente al nombre del widget.\n",
    "\n",
    "    Parámetros:\n",
    "    - widget_config (dict): Un diccionario donde cada clave es el nombre del widget, y el valor es otro diccionario\n",
    "                             que contiene 'value' (valor por defecto del widget) y 'label' (etiqueta del widget).\n",
    "                             Ejemplo:\n",
    "                             {\n",
    "                                 'redWineUrl': {'value': 'http://example.com/redWine.csv', 'label': 'URL del Vino Tinto'},\n",
    "                                 'saveDirectory': {'value': '/path/to/save', 'label': 'Directorio de Guardado' }\n",
    "                             }\n",
    "\n",
    "    Nota:\n",
    "    - Esta función no devuelve ningún valor. Modifica los widgets de Databricks y asigna los valores de los widgets\n",
    "      a las variables de Python correspondientes en el ámbito del notebook.\n",
    "    \"\"\"\n",
    "    for widget_name, widget_info in widget_config.items():\n",
    "        # Si el valor es una lista, convertirlo a un string\n",
    "        widget_value = json.dumps(widget_info['value']) if isinstance(widget_info['value'], list) else widget_info['value']\n",
    "        \n",
    "        # Crear el widget de texto\n",
    "        dbutils.widgets.text(widget_name, widget_value, widget_info['label'])\n",
    "\n",
    "    # Imprimir mensaje de éxito después de crear todos los widgets\n",
    "    print(\"Proceso completado: Todos los widgets de texto han sido creados y asignados correctamente.\")\n",
    "\n",
    "# Recuperar el valor del widget y convertirlo de nuevo a una lista\n",
    "def get_widget_value_as_list(widget_name: str) -> list:\n",
    "    \"\"\"\n",
    "    Recupera el valor de un widget de texto y lo convierte de nuevo a una lista de Python.\n",
    "\n",
    "    Parámetros:\n",
    "    - widget_name (str): Nombre del widget.\n",
    "\n",
    "    Retorno:\n",
    "    - list: Lista de valores del widget.\n",
    "    \"\"\"\n",
    "    widget_value = dbutils.widgets.get(widget_name)\n",
    "    \n",
    "    # Convertir el valor del widget de vuelta a una lista\n",
    "    try:\n",
    "        # Intentar convertir el valor a una lista\n",
    "        return json.loads(widget_value)\n",
    "    except json.JSONDecodeError:\n",
    "        # Si no es una lista, devolverlo como un solo valor\n",
    "        return [widget_value]\n",
    "    \n",
    "def remove_assign_notebook_parameter_value_from_widget():\n",
    "    \"\"\"\n",
    "    Elimina todos los widgets en el notebook de Databricks.\n",
    "    \"\"\"\n",
    "    \n",
    "    dbutils.widgets.removeAll()\n",
    "    print(\"Todos los widgets han sido eliminados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9f1635a-8a54-4ba8-9499-f9f35b000548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado: Todos los widgets de texto han sido creados y asignados correctamente.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configuración de widgets con valores y etiquetas\n",
    "# Definir la URL base\n",
    "url_base = 'https://raw.githubusercontent.com/JorgeCardona/data-collection-json-csv-sql/refs/heads/main/csv/wine'\n",
    "\n",
    "# Definir la ruta común para el directorio de guardado\n",
    "save_directory = '/FileStore/tables'  # Sin la barra final\n",
    "\n",
    "# Definir los nombres de los datasets en formato snake_case\n",
    "dataset_1_name = \"winequality-red.csv\"\n",
    "dataset_2_name = \"winequality-white.csv\"\n",
    "\n",
    "# Configuración de widgets con valores y etiquetas para los datasets\n",
    "widget_config = {\n",
    "    \"save_directory\": {\"value\": f\"{save_directory}\", \"label\": f\"save_directory ({save_directory})\"},\n",
    "    \"dataset_1_name\": {\"value\": f\"{dataset_1_name}\", \"label\": f\"Dataset name ({dataset_1_name})\"},\n",
    "    \"dataset_2_name\": {\"value\": f\"{dataset_2_name}\", \"label\": f\"Dataset name ({dataset_2_name})\"},\n",
    "    \"dataset1\": {\"value\": f\"{url_base}/{dataset_1_name}\", \"label\": f\"Dataset 1 URL ({dataset_1_name})\"},\n",
    "    \"dataset2\": {\"value\": f\"{url_base}/{dataset_2_name}\", \"label\": f\"Dataset 2 URL ({dataset_2_name})\"},\n",
    "    \"saveDirectory\": {\"value\": save_directory, \"label\": \"Save Directory\"},\n",
    "    \"dataset1FileLocation\": {\"value\": f\"{save_directory}/{dataset_1_name}\", \"label\": f\"Dataset 1 File Location ({dataset_1_name})\"},\n",
    "    \"dataset2FileLocation\": {\"value\": f\"{save_directory}/{dataset_2_name}\", \"label\": f\"Dataset 2 File Location ({dataset_2_name})\"},\n",
    "    \"fileType\": {\"value\": \"csv\", \"label\": \"File Type\"},\n",
    "    \"inferSchema\": {\"value\": \"true\", \"label\": \"Infer Schema\"},\n",
    "    \"firstRowIsHeader\": {\"value\": \"true\", \"label\": \"First Row Is Header\"},\n",
    "    \"delimiter\": {\"value\": \",\", \"label\": \"Delimiter\"},\n",
    "    \"bronze\": {\"value\": \"db_bronze\", \"label\": \"Data Base Bronze\"},\n",
    "    \"silver\": {\"value\": \"db_silver\", \"label\": \"Data Base Silver\"},\n",
    "    \"gold\": {\"value\": \"db_gold\", \"label\": \"Data Base Gold\"},\n",
    "    \"dataset_1_table\": {\"value\": \"red_wine\", \"label\": \"Table Name Dataset 1\"},\n",
    "    \"dataset_2_table\": {\"value\": \"white_wine\", \"label\": \"Table Name Dataset 2\"},\n",
    "    \"partition_columns\": {\"value\": [\"quality\", \"free sulfur dioxide\"], \"label\": \"Columns to partitioning datasets\"}\n",
    "}\n",
    "\n",
    "# Llamar la función con el diccionario configurado\n",
    "assign_notebook_parameter_value_from_text_widget(widget_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e8e5c05-db73-4a60-aefe-5fdf48c9eca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# FUNCIONES PARA DESCARGAR, LISTAR Y ELIMINAR ARCHIVOS EN UN DIRECTORIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66ce439e-ecd9-4a44-94bb-f2c8a3aa7abc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# Función para descargar y guardar los archivos en databricks\n",
    "\n",
    "def download_dataset_and_store_in_dbfs(url: str, save_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Descarga un archivo desde una URL y lo guarda en el Databricks File System (DBFS).\n",
    "\n",
    "    Esta función descarga el contenido de un archivo desde una URL dada y lo guarda en una ruta dentro de DBFS.\n",
    "\n",
    "    Parámetros:\n",
    "    - url (str): URL desde donde se descargará el archivo.\n",
    "    - save_path (str): Ruta completa en DBFS donde se guardará el archivo descargado.\n",
    "\n",
    "    Excepciones:\n",
    "    - requests.exceptions.RequestException: Si ocurre un error durante la descarga del archivo.\n",
    "    - Exception: Para cualquier otro error inesperado, como problemas al guardar el archivo en DBFS.\n",
    "\n",
    "    Ejemplo de uso:\n",
    "    ```python\n",
    "    download_dataset_and_store_in_dbfs(\"https://example.com/file.csv\", \"/dbfs/tmp/data/file.csv\")\n",
    "    ```\n",
    "\n",
    "    El archivo será guardado en la ubicación especificada dentro de DBFS.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Descargar el contenido del archivo\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Verifica si la solicitud fue exitosa\n",
    "\n",
    "        # Usar dbutils.fs.put para guardar el contenido en DBFS\n",
    "        dbutils.fs.put(save_path, response.text, overwrite=True)\n",
    "\n",
    "        print(f\"Archivo guardado correctamente en {save_path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error al descargar el archivo desde {url}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error inesperado al guardar el archivo en DBFS: {e}\")\n",
    "\n",
    "\n",
    "def list_files_in_directory_path(directory_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Lista los archivos dentro de un directorio especificado. Si no hay archivos, imprime un mensaje indicando\n",
    "    que el directorio está vacío.\n",
    "\n",
    "    Parámetros:\n",
    "    - directory_path (str): Ruta completa del directorio cuyo contenido se desea listar.\n",
    "\n",
    "    Retorno:\n",
    "    - None: Esta función no retorna ningún valor, solo imprime el contenido del directorio o un mensaje de vacío.\n",
    "    \"\"\"\n",
    "    files = dbutils.fs.ls(directory_path)\n",
    "    if not files:\n",
    "        print(f\"No hay archivos en el directorio {directory_path}.\")\n",
    "    else:\n",
    "        print(f\"Archivos en el directorio {directory_path}:\")\n",
    "        for file in files:\n",
    "            print(file.path)\n",
    "\n",
    "def delete_all_files_in_directory_path(directory_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Elimina todos los archivos dentro de un directorio especificado. Si el directorio está vacío, imprime un mensaje \n",
    "    indicando que no hay archivos para eliminar.\n",
    "\n",
    "    Parámetros:\n",
    "    - directory_path (str): Ruta completa del directorio cuyo contenido se desea eliminar.\n",
    "\n",
    "    Retorno:\n",
    "    - None: Esta función no retorna ningún valor, solo imprime el estado de eliminación de los archivos.\n",
    "    \"\"\"\n",
    "    # Listar todos los archivos en el directorio\n",
    "    files = dbutils.fs.ls(directory_path)\n",
    "\n",
    "    if not files:\n",
    "        print(f\"No hay archivos para eliminar en el directorio {directory_path}.\\n\")\n",
    "    else:\n",
    "        # Eliminar cada archivo encontrado\n",
    "        print(f\"Eliminando archivos en el directorio {directory_path}...\\n\")\n",
    "        for file in files:\n",
    "            dbutils.fs.rm(file.path, recurse=True)\n",
    "            print(f\"Archivo eliminado: {file.path}\")\n",
    "        \n",
    "        print(f\"\\nTodos los archivos en el directorio {directory_path} han sido eliminados.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f01074d6-5d25-4886-a795-9a9b573d499f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# FUNCION PARA EJECUTAR LOS QUERIES EN SPARKSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33bb2af6-8277-488b-aaac-59a9efdc581f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_database(database_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Crea una base de datos en Databricks si no existe ya.\n",
    "\n",
    "    Parámetros:\n",
    "    - database_name (str): Nombre de la base de datos a crear.\n",
    "\n",
    "    Retorno:\n",
    "    - None\n",
    "\n",
    "    Ejemplo de uso:\n",
    "    ```python\n",
    "    create_database(\"nombre_de_la_base_de_datos\")\n",
    "    ```\n",
    "    \"\"\"\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "    print(f\"La base de datos '{database_name}' ha sido creada exitosamente (o ya existía).\")\n",
    "\n",
    "\n",
    "def drop_database(database_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Elimina una base de datos en Databricks si existe.\n",
    "\n",
    "    Parámetros:\n",
    "    - database_name (str): Nombre de la base de datos que se desea eliminar.\n",
    "\n",
    "    Retorno:\n",
    "    - None\n",
    "\n",
    "    Nota:\n",
    "    Esta función elimina la base de datos especificada únicamente si existe. Para eliminar una base de datos que contenga\n",
    "    tablas u objetos, utiliza la opción CASCADE dentro del comando SQL.\n",
    "\n",
    "    Ejemplo de uso:\n",
    "    ```python\n",
    "    drop_database(\"nombre_de_la_base_de_datos\")\n",
    "    ```\n",
    "    \"\"\"\n",
    "    spark.sql(f\"DROP DATABASE IF EXISTS {database_name} CASCADE\")\n",
    "    print(f\"La base de datos '{database_name}' ha sido eliminada exitosamente (si existía).\")\n",
    "\n",
    "def execute_spark_sql_query(query: str):\n",
    "    \"\"\"\n",
    "    Ejecuta una consulta SQL en Spark.\n",
    "\n",
    "    Parámetros:\n",
    "    - query (str): Consulta SQL a ejecutar.\n",
    "\n",
    "    Retorno:\n",
    "    - pyspark.sql.dataframe.DataFrame o None: \n",
    "        - Si la consulta produce un resultado (como SELECT), retorna un DataFrame.\n",
    "        - Si la consulta no produce un resultado (como UPDATE o DELETE), retorna None.\n",
    "\n",
    "    Nota:\n",
    "    La función asume que la consulta SQL está correctamente formada y que se puede ejecutar en el entorno de Spark.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Ejecutando consulta SQL:\\n{query}\")\n",
    "        # Ejecutar el query SQL\n",
    "        result = spark.sql(query)\n",
    "        \n",
    "        # Determinar si la consulta devuelve un DataFrame o no\n",
    "        if result.isStreaming or hasattr(result, \"count\"):  # Verifica si tiene un resultado tangible\n",
    "            print(f\"La consulta SQL se ejecutó exitosamente. El número de registros obtenidos es: {result.count()}\")\n",
    "            return result\n",
    "        else:\n",
    "            print(\"La consulta SQL se ejecutó exitosamente. No se generó un resultado para mostrar.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error al ejecutar la consulta SQL:\\n{query}\\nError: {e}\")\n",
    "        return None\n",
    "    \n",
    "def list_tables_in_database(database_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Lista y muestra todas las tablas en una base de datos específica en Databricks.\n",
    "\n",
    "    Parámetros:\n",
    "    - database_name (str): Nombre de la base de datos de la cual se quieren listar las tablas.\n",
    "\n",
    "    Retorno:\n",
    "    - None\n",
    "\n",
    "    Ejemplo de uso:\n",
    "    ```python\n",
    "    list_tables_in_database(\"mi_base_de_datos\")\n",
    "    ```\n",
    "    \"\"\"\n",
    "    tablas = spark.sql(f\"SHOW TABLES IN {database_name}\")\n",
    "    display(tablas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89b143c2-7996-459b-bc4d-9366e14975f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# FUNCIONES PARA GUARDAR LOS DATAFRAMES COMOL DELTA Y PARQUET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c15de1e-5389-4a81-8630-ef0b70656226",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def standardize_column_names_from_list(column_names: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Estandariza los nombres de las columnas reemplazando caracteres no alfanuméricos\n",
    "    por guiones bajos (\"_\"). Esta transformación ayuda a evitar problemas de formato\n",
    "    en el procesamiento de datos.\n",
    "\n",
    "    Parámetros:\n",
    "    - column_names (List[str]): Lista de nombres de columnas a estandarizar.\n",
    "\n",
    "    Retorno:\n",
    "    - List[str]: Nueva lista con los nombres de las columnas estandarizados.\n",
    "    \"\"\"\n",
    "    standardized_columns = []\n",
    "    \n",
    "    for col_name in column_names:\n",
    "        # Reemplazar caracteres no alfanuméricos por \"_\"\n",
    "        new_col_name = ''.join([char if char.isalnum() or char == '_' else '_' for char in col_name])\n",
    "        standardized_columns.append(new_col_name)\n",
    "    \n",
    "    print(f\"Los nombres de las columnas han sido estandarizados exitosamente. Las columnas ahora usan solo caracteres alfanuméricos y guiones bajos.\")\n",
    "    return standardized_columns\n",
    "\n",
    "def standardize_column_names_from_spark_dataframe(spark_dataframe: \"pyspark.sql.dataframe.DataFrame\") -> \"pyspark.sql.dataframe.DataFrame\":\n",
    "    \"\"\"\n",
    "    Estandariza los nombres de las columnas de un DataFrame de Spark reemplazando caracteres no alfanuméricos\n",
    "    por guiones bajos (\"_\"). Esta transformación ayuda a evitar problemas de formato en el procesamiento de datos.\n",
    "\n",
    "    Parámetros:\n",
    "    - spark_dataframe (pyspark.sql.dataframe.DataFrame): DataFrame de Spark con los nombres de columnas a estandarizar.\n",
    "\n",
    "    Retorno:\n",
    "    - pyspark.sql.dataframe.DataFrame: Nuevo DataFrame de Spark con los nombres de columnas estandarizados.\n",
    "    \"\"\"\n",
    "    for col_name in spark_dataframe.columns:\n",
    "        # Reemplazar caracteres no alfanuméricos por \"_\"\n",
    "        new_col_name = ''.join([char if char.isalnum() or char == '_' else '_' for char in col_name])\n",
    "        spark_dataframe = spark_dataframe.withColumnRenamed(col_name, new_col_name)\n",
    "    \n",
    "    print(f\"Los nombres de las columnas han sido estandarizados exitosamente. Las columnas ahora usan solo caracteres alfanuméricos y guiones bajos.\")\n",
    "    return spark_dataframe\n",
    "\n",
    "def save_dataframe_as_delta_format(spark_dataframe: \"pyspark.sql.dataframe.DataFrame\", path: str, partition_keys: List[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Guarda un DataFrame de Spark en formato Delta en una ubicación específica de almacenamiento,\n",
    "    opcionalmente particionado por una o más columnas.\n",
    "\n",
    "    Parámetros:\n",
    "    - spark_dataframe (DataFrame): DataFrame de Spark a guardar.\n",
    "    - path (str): Ruta en el sistema de archivos donde se guardará el DataFrame en formato Delta.\n",
    "    - partition_keys (List[str], opcional): Lista de nombres de columnas por las que se particionará el DataFrame.\n",
    "\n",
    "    Retorno:\n",
    "    - None\n",
    "\n",
    "    Nota:\n",
    "    Esta función sobrescribe cualquier archivo existente en la ruta especificada.\n",
    "    \"\"\"\n",
    "    # Habilitar la fusión automática de esquemas (si es necesario)\n",
    "    spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "    # Estandarizar nombres de columnas (opcional)\n",
    "    spark_dataframe = standardize_column_names_from_spark_dataframe(spark_dataframe)\n",
    "    partition_keys = standardize_column_names_from_list(partition_keys)\n",
    "    # Guardar el DataFrame en formato Delta, particionado si se especifica\n",
    "    if partition_keys:\n",
    "        spark_dataframe.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .partitionBy(partition_keys) \\\n",
    "            .save(path)\n",
    "        print(f\"El DataFrame ha sido guardado en formato Delta en '{path}', particionado por {partition_keys}\")\n",
    "        return partition_keys\n",
    "    else:\n",
    "        spark_dataframe.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(path)\n",
    "        print(f\"El DataFrame ha sido guardado exitosamente en formato Delta en la ruta: {path}\")\n",
    "\n",
    "def save_dataframe_as_delta_table(spark_dataframe: \"pyspark.sql.dataframe.DataFrame\", \n",
    "                                  database_name: str, \n",
    "                                  table_name: str, \n",
    "                                  partition_by: list = None) -> None:\n",
    "    \"\"\"\n",
    "    Guarda un DataFrame de Spark como una tabla Delta en una base de datos de Databricks.\n",
    "\n",
    "    Parámetros:\n",
    "    - spark_dataframe (pyspark.sql.dataframe.DataFrame): DataFrame de Spark a guardar.\n",
    "    - database_name (str): Nombre de la base de datos donde se creará la tabla.\n",
    "    - table_name (str): Nombre de la tabla Delta a crear en la base de datos.\n",
    "    - partition_by (list): Lista de nombres de columnas por las cuales se debe particionar la tabla. Si no se proporciona, la tabla no será particionada.\n",
    "\n",
    "    Retorno:\n",
    "    - None\n",
    "\n",
    "    Nota:\n",
    "    Esta función sobrescribe cualquier tabla existente con el mismo nombre en la base de datos especificada.\n",
    "    \"\"\"\n",
    "    spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "    full_table_name = f\"{database_name}.{table_name}\"\n",
    "\n",
    "    # Estandarizar los nombres de las columnas (si es necesario)\n",
    "    spark_dataframe = standardize_column_names_from_spark_dataframe(spark_dataframe)\n",
    "\n",
    "    if partition_by:\n",
    "        # Estandarizar los nombres de las columnas en partition_by (si es necesario)\n",
    "        partition_by = standardize_column_names_from_list(partition_by)\n",
    "\n",
    "        # Guardar la tabla particionada por las columnas especificadas\n",
    "        spark_dataframe.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .partitionBy(*partition_by) \\\n",
    "            .saveAsTable(full_table_name)\n",
    "        print(f\"El DataFrame ha sido guardado exitosamente como tabla Delta en la base de datos: {database_name}, \"\n",
    "              f\"tabla: {table_name}, particionada por {partition_by}\")\n",
    "    else:\n",
    "        # Guardar la tabla sin particionamiento\n",
    "        spark_dataframe.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(full_table_name)\n",
    "        print(f\"El DataFrame ha sido guardado exitosamente como tabla Delta en la base de datos: {database_name}, \"\n",
    "              f\"tabla: {table_name}\")\n",
    "\n",
    "def save_dataframe_as_parquet_format(spark_dataframe: \"pyspark.sql.dataframe.DataFrame\", \n",
    "                                     path: str, \n",
    "                                     partition_by: list = None) -> None:\n",
    "    \"\"\"\n",
    "    Guarda un DataFrame de Spark en formato Parquet en una ubicación específica de almacenamiento,\n",
    "    opcionalmente particionado por columnas específicas.\n",
    "\n",
    "    Parámetros:\n",
    "    - spark_dataframe (pyspark.sql.dataframe.DataFrame): DataFrame de Spark a guardar.\n",
    "    - path (str): Ruta en el sistema de archivos donde se guardará el DataFrame en formato Parquet.\n",
    "    - partition_by (list): Lista de nombres de columnas por las cuales se debe particionar el archivo Parquet.\n",
    "\n",
    "    Retorno:\n",
    "    - None\n",
    "\n",
    "    Nota:\n",
    "    Esta función sobrescribe cualquier archivo existente en la ruta especificada.\n",
    "    \"\"\"\n",
    "    spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "    if partition_by:\n",
    "        # Guardar con particionamiento\n",
    "        spark_dataframe.write.format(\"parquet\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .partitionBy(partition_by) \\\n",
    "            .save(path)\n",
    "        print(f\"El DataFrame ha sido guardado en formato Parquet en '{path}', particionado por {partition_by}\")\n",
    "        return partition_by\n",
    "    else:\n",
    "        # Guardar sin particionamiento\n",
    "        spark_dataframe.write.format(\"parquet\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(path)\n",
    "        print(f\"El DataFrame ha sido guardado exitosamente en formato Parquet en la ruta: {path}\")\n",
    "\n",
    "\n",
    "def save_dataframe_as_parquet_table(spark_dataframe: \"pyspark.sql.dataframe.DataFrame\", \n",
    "                                    database_name: str, \n",
    "                                    table_name: str, \n",
    "                                    partition_by: list = None) -> None:\n",
    "    \"\"\"\n",
    "    Guarda un DataFrame de Spark como una tabla Parquet en una base de datos de Databricks.\n",
    "\n",
    "    Parámetros:\n",
    "    - spark_dataframe (pyspark.sql.dataframe.DataFrame): DataFrame de Spark a guardar.\n",
    "    - database_name (str): Nombre de la base de datos donde se creará la tabla.\n",
    "    - table_name (str): Nombre de la tabla Parquet a crear en la base de datos.\n",
    "    - partition_by (list): Lista de nombres de columnas por las cuales se debe particionar la tabla. Si no se proporciona, la tabla no será particionada.\n",
    "\n",
    "    Retorno:\n",
    "    - None\n",
    "\n",
    "    Nota:\n",
    "    Esta función sobrescribe cualquier tabla existente con el mismo nombre en la base de datos especificada.\n",
    "    \"\"\"\n",
    "    spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "    full_table_name = f\"{database_name}.{table_name}\"\n",
    "\n",
    "    if partition_by:\n",
    "        # Guardar la tabla particionada por las columnas especificadas\n",
    "        spark_dataframe.write.format(\"parquet\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .partitionBy(partition_by) \\\n",
    "            .saveAsTable(full_table_name)\n",
    "        print(f\"El DataFrame ha sido guardado exitosamente como tabla Parquet en la base de datos: {database_name}, \"\n",
    "              f\"tabla: {table_name}, particionada por {partition_by}\")\n",
    "    else:\n",
    "        # Guardar la tabla sin particionamiento\n",
    "        spark_dataframe.write.format(\"parquet\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(full_table_name)\n",
    "        print(f\"El DataFrame ha sido guardado exitosamente como tabla Parquet en la base de datos: {database_name}, \"\n",
    "              f\"tabla: {table_name}\")\n",
    "\n",
    "def save_dataframe_as_temp_table(spark_dataframe: \"pyspark.sql.dataframe.DataFrame\", \n",
    "                                 table_name: str, \n",
    "                                 partition_by: list = None) -> None:\n",
    "    \"\"\"\n",
    "    Guarda un DataFrame de Spark como una tabla temporal en Databricks.\n",
    "\n",
    "    Parámetros:\n",
    "    - spark_dataframe (pyspark.sql.dataframe.DataFrame): DataFrame de Spark a guardar como tabla temporal.\n",
    "    - table_name (str): Nombre de la tabla temporal a crear.\n",
    "    - partition_by (list): Lista de columnas por las cuales particionar los datos antes de guardarlos como tabla temporal (opcional).\n",
    "\n",
    "    Retorno:\n",
    "    - None\n",
    "\n",
    "    Nota:\n",
    "    Esta función crea o reemplaza una tabla temporal, que existe solo durante la sesión activa de Spark.\n",
    "    \"\"\"\n",
    "    spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "    \n",
    "    # Si se especifica particionamiento, particionamos el DataFrame antes de crear la tabla temporal\n",
    "    if partition_by:\n",
    "        spark_dataframe = spark_dataframe.repartition(*partition_by)\n",
    "\n",
    "    # Crear la vista temporal\n",
    "    spark_dataframe.createOrReplaceTempView(table_name)\n",
    "    print(f\"El DataFrame ha sido guardado exitosamente como tabla temporal: {table_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b53a438-55e6-42d0-9f1b-2a2ecbd5206c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# FUNCION PARA LEER LOS ARCHIVOS DESCARGADOS COMO SPARK DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36efc388-ad6a-432e-8514-24c2586de499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_data_to_spark_dataframe(file_location: str, \n",
    "                                 file_type: str = \"csv\", \n",
    "                                 infer_schema: bool = True, \n",
    "                                 first_row_is_header: bool = True, \n",
    "                                 delimiter: str = \",\") -> \"pyspark.sql.dataframe.DataFrame\":\n",
    "    \"\"\"\n",
    "    Carga un archivo en un DataFrame de Spark, con opciones configurables para tipos de archivo, inferencia de esquema,\n",
    "    encabezado y delimitador. Compatible con archivos CSV, Parquet, Delta, JSON, ORC y Avro.\n",
    "\n",
    "    Parámetros:\n",
    "    - file_location (str): La ubicación del archivo que se desea cargar.\n",
    "    - file_type (str): El tipo de archivo. Valores permitidos: 'csv', 'parquet', 'delta', 'json', 'orc', 'avro'. Por defecto es 'csv'.\n",
    "    - infer_schema (bool): Si se debe inferir el esquema automáticamente (aplicable solo para CSV y JSON). Por defecto es True.\n",
    "    - first_row_is_header (bool): Si la primera fila debe ser usada como encabezado (aplicable solo para CSV). Por defecto es True.\n",
    "    - delimiter (str): El delimitador para los archivos CSV. Por defecto es ','.\n",
    "\n",
    "    Retorna:\n",
    "    - DataFrame: El DataFrame cargado con los datos del archivo.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Diccionario que mapea el tipo de archivo al método de carga correspondiente\n",
    "        format_options = {\n",
    "            \"csv\": {\n",
    "                \"format\": \"csv\",\n",
    "                \"options\": {\n",
    "                    \"inferSchema\": infer_schema,\n",
    "                    \"header\": first_row_is_header,\n",
    "                    \"sep\": delimiter\n",
    "                }\n",
    "            },\n",
    "            \"parquet\": {\"format\": \"parquet\", \"options\": {}},\n",
    "            \"delta\": {\"format\": \"delta\", \"options\": {}},\n",
    "            \"json\": {\n",
    "                \"format\": \"json\",\n",
    "                \"options\": {\"inferSchema\": infer_schema}\n",
    "            },\n",
    "            \"orc\": {\"format\": \"orc\", \"options\": {}},\n",
    "            \"avro\": {\"format\": \"avro\", \"options\": {}}\n",
    "        }\n",
    "\n",
    "        # paraleer avro se puede requerir\n",
    "        # spark.conf.set(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.4.0\")\n",
    "\n",
    "        # Verificar si el tipo de archivo es soportado\n",
    "        if file_type not in format_options:\n",
    "            raise ValueError(f\"Tipo de archivo no soportado: {file_type}\")\n",
    "\n",
    "        # Obtener el formato y las opciones correspondientes\n",
    "        file_format = format_options[file_type][\"format\"]\n",
    "        options = format_options[file_type][\"options\"]\n",
    "\n",
    "        # Cargar el archivo en un DataFrame\n",
    "        df = spark.read.format(file_format).options(**options).load(file_location)\n",
    "\n",
    "        print(f\"Archivo {file_type} cargado correctamente desde {file_location}\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar el archivo desde {file_location}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76cd56e2-4db5-477e-86d8-afc664e1d49d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# FUNCION PARA LISTAR LAS TABLAS DE LA BASE DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc8fc09e-7d6e-4f9c-a31b-e7a6f5e81359",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def list_temp_tables(show_global: bool = False) -> \"pyspark.sql.dataframe.DataFrame\":\n",
    "    \"\"\"\n",
    "    Lista las tablas temporales en la sesión de Spark y las devuelve como un DataFrame de Spark.\n",
    "\n",
    "    Parámetros:\n",
    "    - show_global (bool): Si es True, muestra tablas temporales globales (prefijadas con 'global_temp'). \n",
    "                          Si es False, muestra las tablas temporales locales de la sesión.\n",
    "\n",
    "    Retorno:\n",
    "    - pyspark.sql.dataframe.DataFrame: Un DataFrame con las tablas temporales disponibles.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if show_global:\n",
    "            # Listar tablas temporales globales\n",
    "            global_temp_tables = [\n",
    "                (f\"global_temp.{table.name}\", \"Global Temporary\")\n",
    "                for table in spark.catalog.listTables(\"global_temp\")\n",
    "            ]\n",
    "        else:\n",
    "            # Listar tablas temporales locales\n",
    "            temp_tables = [\n",
    "                (table.name, \"Local Temporary\")\n",
    "                for table in spark.catalog.listTables()\n",
    "                if table.isTemporary\n",
    "            ]\n",
    "        \n",
    "        # Combinar los resultados y convertirlos a un DataFrame de Spark\n",
    "        tables = global_temp_tables if show_global else temp_tables\n",
    "        if tables:\n",
    "            schema = [\"Table\", \"Type\"]\n",
    "            display(spark.createDataFrame(tables, schema=schema))\n",
    "        else:\n",
    "            print(\"No se encontraron tablas temporales.\")\n",
    "            schema = [\"Table\", \"Type\"]\n",
    "            display(spark.createDataFrame([], schema=schema))\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error al listar las tablas temporales: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11fa9fec-6c22-439d-be32-cf1769aedeec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DESCARGAR LOS DATASETS ORIGINALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11252095-238e-4b2e-8c17-22bc380a5ee8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 100951 bytes.\nArchivo guardado correctamente en /FileStore/tables/winequality-red.csv\nWrote 264426 bytes.\nArchivo guardado correctamente en /FileStore/tables/winequality-white.csv\nArchivos en el directorio /FileStore/tables:\ndbfs:/FileStore/tables/Delta_dataset1/\ndbfs:/FileStore/tables/winequality-red.csv\ndbfs:/FileStore/tables/winequality-white.csv\n"
     ]
    }
   ],
   "source": [
    "# Recuperar los valores de los widgets\n",
    "dataset1Url = dbutils.widgets.get(\"dataset1\")\n",
    "dataset2Url = dbutils.widgets.get(\"dataset2\")\n",
    "saveDirectory = dbutils.widgets.get(\"saveDirectory\")\n",
    "dataset_1_name = dbutils.widgets.get(\"dataset_1_name\")\n",
    "dataset_2_name = dbutils.widgets.get(\"dataset_2_name\")\n",
    "\n",
    "# Descargar y guardar los archivos de vino tinto y vino blanco en el almacenamiento local\n",
    "download_dataset_and_store_in_dbfs(dataset1Url, os.path.join(saveDirectory, dataset_1_name))\n",
    "download_dataset_and_store_in_dbfs(dataset2Url, os.path.join(saveDirectory, dataset_2_name))\n",
    "\n",
    "# listar los archivos del directorio creado\n",
    "list_files_in_directory_path(saveDirectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99589348-9dca-4f28-beb4-cec025ae8bdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## LEER LOS DATASETS DESCARGADOS COMO SPARK DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e58e1276-23a2-4385-b7c4-6f5c2722d670",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo csv cargado correctamente desde /FileStore/tables/winequality-red.csv\nArchivo csv cargado correctamente desde /FileStore/tables/winequality-white.csv\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>fixed acidity</th><th>volatile acidity</th><th>citric acid</th><th>residual sugar</th><th>chlorides</th><th>free sulfur dioxide</th><th>total sulfur dioxide</th><th>density</th><th>pH</th><th>sulphates</th><th>alcohol</th><th>quality</th></tr></thead><tbody><tr><td>7.4</td><td>0.7</td><td>0.0</td><td>1.9</td><td>0.076</td><td>11.0</td><td>34.0</td><td>0.9978</td><td>3.51</td><td>0.56</td><td>9.4</td><td>5</td></tr><tr><td>7.8</td><td>0.88</td><td>0.0</td><td>2.6</td><td>0.098</td><td>25.0</td><td>67.0</td><td>0.9968</td><td>3.2</td><td>0.68</td><td>9.8</td><td>5</td></tr><tr><td>7.8</td><td>0.76</td><td>0.04</td><td>2.3</td><td>0.092</td><td>15.0</td><td>54.0</td><td>0.997</td><td>3.26</td><td>0.65</td><td>9.8</td><td>5</td></tr><tr><td>11.2</td><td>0.28</td><td>0.56</td><td>1.9</td><td>0.075</td><td>17.0</td><td>60.0</td><td>0.998</td><td>3.16</td><td>0.58</td><td>9.8</td><td>6</td></tr><tr><td>7.4</td><td>0.7</td><td>0.0</td><td>1.9</td><td>0.076</td><td>11.0</td><td>34.0</td><td>0.9978</td><td>3.51</td><td>0.56</td><td>9.4</td><td>5</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         7.4,
         0.7,
         0.0,
         1.9,
         0.076,
         11.0,
         34.0,
         0.9978,
         3.51,
         0.56,
         9.4,
         5
        ],
        [
         7.8,
         0.88,
         0.0,
         2.6,
         0.098,
         25.0,
         67.0,
         0.9968,
         3.2,
         0.68,
         9.8,
         5
        ],
        [
         7.8,
         0.76,
         0.04,
         2.3,
         0.092,
         15.0,
         54.0,
         0.997,
         3.26,
         0.65,
         9.8,
         5
        ],
        [
         11.2,
         0.28,
         0.56,
         1.9,
         0.075,
         17.0,
         60.0,
         0.998,
         3.16,
         0.58,
         9.8,
         6
        ],
        [
         7.4,
         0.7,
         0.0,
         1.9,
         0.076,
         11.0,
         34.0,
         0.9978,
         3.51,
         0.56,
         9.4,
         5
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "fixed acidity",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "volatile acidity",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "citric acid",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "residual sugar",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "chlorides",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "free sulfur dioxide",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "total sulfur dioxide",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "density",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "pH",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "sulphates",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "alcohol",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "quality",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>fixed acidity</th><th>volatile acidity</th><th>citric acid</th><th>residual sugar</th><th>chlorides</th><th>free sulfur dioxide</th><th>total sulfur dioxide</th><th>density</th><th>pH</th><th>sulphates</th><th>alcohol</th><th>quality</th></tr></thead><tbody><tr><td>7.0</td><td>0.27</td><td>0.36</td><td>20.7</td><td>0.045</td><td>45.0</td><td>170.0</td><td>1.001</td><td>3.0</td><td>0.45</td><td>8.8</td><td>6</td></tr><tr><td>6.3</td><td>0.3</td><td>0.34</td><td>1.6</td><td>0.049</td><td>14.0</td><td>132.0</td><td>0.994</td><td>3.3</td><td>0.49</td><td>9.5</td><td>6</td></tr><tr><td>8.1</td><td>0.28</td><td>0.4</td><td>6.9</td><td>0.05</td><td>30.0</td><td>97.0</td><td>0.9951</td><td>3.26</td><td>0.44</td><td>10.1</td><td>6</td></tr><tr><td>7.2</td><td>0.23</td><td>0.32</td><td>8.5</td><td>0.058</td><td>47.0</td><td>186.0</td><td>0.9956</td><td>3.19</td><td>0.4</td><td>9.9</td><td>6</td></tr><tr><td>7.2</td><td>0.23</td><td>0.32</td><td>8.5</td><td>0.058</td><td>47.0</td><td>186.0</td><td>0.9956</td><td>3.19</td><td>0.4</td><td>9.9</td><td>6</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         7.0,
         0.27,
         0.36,
         20.7,
         0.045,
         45.0,
         170.0,
         1.001,
         3.0,
         0.45,
         8.8,
         6
        ],
        [
         6.3,
         0.3,
         0.34,
         1.6,
         0.049,
         14.0,
         132.0,
         0.994,
         3.3,
         0.49,
         9.5,
         6
        ],
        [
         8.1,
         0.28,
         0.4,
         6.9,
         0.05,
         30.0,
         97.0,
         0.9951,
         3.26,
         0.44,
         10.1,
         6
        ],
        [
         7.2,
         0.23,
         0.32,
         8.5,
         0.058,
         47.0,
         186.0,
         0.9956,
         3.19,
         0.4,
         9.9,
         6
        ],
        [
         7.2,
         0.23,
         0.32,
         8.5,
         0.058,
         47.0,
         186.0,
         0.9956,
         3.19,
         0.4,
         9.9,
         6
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "fixed acidity",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "volatile acidity",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "citric acid",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "residual sugar",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "chlorides",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "free sulfur dioxide",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "total sulfur dioxide",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "density",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "pH",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "sulphates",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "alcohol",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "quality",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# File locations and types\n",
    "# Recuperar los valores de los widgets\n",
    "fileType = dbutils.widgets.get(\"fileType\")\n",
    "inferSchema = dbutils.widgets.get(\"inferSchema\")\n",
    "firstRowIsHeader = dbutils.widgets.get(\"firstRowIsHeader\")\n",
    "delimiter = dbutils.widgets.get(\"delimiter\")\n",
    "\n",
    "dataset_1_name = dbutils.widgets.get(\"dataset_1_name\")\n",
    "dataset_2_name = dbutils.widgets.get(\"dataset_2_name\")\n",
    "saveDirectory = dbutils.widgets.get(\"saveDirectory\")\n",
    "\n",
    "dataset_1_location = f\"{saveDirectory}/{dataset_1_name}\"\n",
    "dataset_2_location = f\"{saveDirectory}/{dataset_2_name}\"\n",
    "\n",
    "red_wine_df = load_data_to_spark_dataframe(dataset_1_location, fileType, inferSchema, firstRowIsHeader, delimiter)\n",
    "white_wine_df = load_data_to_spark_dataframe(dataset_2_location, fileType, inferSchema, firstRowIsHeader, delimiter)\n",
    "\n",
    "display(red_wine_df.limit(5))\n",
    "display(white_wine_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0028549-134d-44fc-b60e-e38b81c0ac53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DEFINICION DE VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a73344e0-f9cb-416f-9849-0e7210f31a10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "partition_columns = get_widget_value_as_list(\"partition_columns\")\n",
    "saveDirectory = dbutils.widgets.get(\"saveDirectory\")\n",
    "database = dbutils.widgets.get(\"bronze\")\n",
    "\n",
    "partition_column_1_value = '3'\n",
    "partition_column_2_value = '5.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0a628ba-75e6-4f59-8f9d-8fb05d213fe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### GUARDA EL DATAFRAME EN FORMATO DELTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f3e59d0-2d2c-4fa3-8855-62f7ff5b0557",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los nombres de las columnas han sido estandarizados exitosamente. Las columnas ahora usan solo caracteres alfanuméricos y guiones bajos.\nLos nombres de las columnas han sido estandarizados exitosamente. Las columnas ahora usan solo caracteres alfanuméricos y guiones bajos.\nEl DataFrame ha sido guardado en formato Delta en '/FileStore/tables/Delta_dataset1', particionado por ['quality', 'free_sulfur_dioxide']\nArchivos en el directorio /FileStore/tables/Delta_dataset1:\ndbfs:/FileStore/tables/Delta_dataset1/_delta_log/\ndbfs:/FileStore/tables/Delta_dataset1/quality=3/\ndbfs:/FileStore/tables/Delta_dataset1/quality=4/\ndbfs:/FileStore/tables/Delta_dataset1/quality=5/\ndbfs:/FileStore/tables/Delta_dataset1/quality=6/\ndbfs:/FileStore/tables/Delta_dataset1/quality=7/\ndbfs:/FileStore/tables/Delta_dataset1/quality=8/\n\nLos nombres de las columnas han sido estandarizados exitosamente. Las columnas ahora usan solo caracteres alfanuméricos y guiones bajos.\nLos nombres de las columnas han sido estandarizados exitosamente. Las columnas ahora usan solo caracteres alfanuméricos y guiones bajos.\nEl DataFrame ha sido guardado en formato Delta en '/FileStore/tables/Delta_dataset2', particionado por ['quality', 'free_sulfur_dioxide']\nArchivos en el directorio /FileStore/tables/Delta_dataset2:\ndbfs:/FileStore/tables/Delta_dataset2/_delta_log/\ndbfs:/FileStore/tables/Delta_dataset2/quality=3/\ndbfs:/FileStore/tables/Delta_dataset2/quality=4/\ndbfs:/FileStore/tables/Delta_dataset2/quality=5/\ndbfs:/FileStore/tables/Delta_dataset2/quality=6/\ndbfs:/FileStore/tables/Delta_dataset2/quality=7/\ndbfs:/FileStore/tables/Delta_dataset2/quality=8/\ndbfs:/FileStore/tables/Delta_dataset2/quality=9/\n\nArchivos en el directorio /FileStore/tables/Delta_dataset1/quality=3/free_sulfur_dioxide=5.0:\ndbfs:/FileStore/tables/Delta_dataset1/quality=3/free_sulfur_dioxide=5.0/part-00000-32b4895c-92c9-4cc7-805a-66cdff61d591.c000.snappy.parquet\ndbfs:/FileStore/tables/Delta_dataset1/quality=3/free_sulfur_dioxide=5.0/part-00000-8ad5ca53-aa7f-4ad8-b16b-0ba28fc8c9cf.c000.snappy.parquet\n\nArchivos en el directorio /FileStore/tables/Delta_dataset2/quality=3/free_sulfur_dioxide=5.0:\ndbfs:/FileStore/tables/Delta_dataset2/quality=3/free_sulfur_dioxide=5.0/part-00000-1d2de44f-f2ae-40ee-8d4c-bdbc95a1b4e4.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "deltaPath1 = f\"{saveDirectory}/Delta_dataset1\"\n",
    "deltaPath2 = f\"{saveDirectory}/Delta_dataset2\"\n",
    "\n",
    "partition_columns_delta_1 = save_dataframe_as_delta_format(red_wine_df, deltaPath1, partition_columns)\n",
    "# listar los archivos del directorio creado\n",
    "list_files_in_directory_path(deltaPath1)\n",
    "print()\n",
    "\n",
    "partition_columns_delta_2 = save_dataframe_as_delta_format(white_wine_df, deltaPath2, partition_columns)\n",
    "list_files_in_directory_path(deltaPath2)\n",
    "print()\n",
    "\n",
    "list_files_in_directory_path(f'{deltaPath1}/{partition_columns_delta_1[0]}={partition_column_1_value}/{partition_columns_delta_1[1]}={partition_column_2_value}')\n",
    "\n",
    "print()\n",
    "list_files_in_directory_path(f'{deltaPath2}/{partition_columns_delta_2[0]}={partition_column_1_value}/{partition_columns_delta_2[1]}={partition_column_2_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c73788f0-00ed-410f-8ba8-b9987cfd0347",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### GUARDA EL DATAFRAME EN FORMATO PARQUET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "459100b1-2569-4119-ab28-8e099aed147b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El DataFrame ha sido guardado en formato Parquet en '/FileStore/tables/Parquet_dataset1', particionado por ['quality', 'free sulfur dioxide']\nArchivos en el directorio /FileStore/tables/Parquet_dataset1:\ndbfs:/FileStore/tables/Parquet_dataset1/_SUCCESS\ndbfs:/FileStore/tables/Parquet_dataset1/quality=3/\ndbfs:/FileStore/tables/Parquet_dataset1/quality=4/\ndbfs:/FileStore/tables/Parquet_dataset1/quality=5/\ndbfs:/FileStore/tables/Parquet_dataset1/quality=6/\ndbfs:/FileStore/tables/Parquet_dataset1/quality=7/\ndbfs:/FileStore/tables/Parquet_dataset1/quality=8/\n\nEl DataFrame ha sido guardado en formato Parquet en '/FileStore/tables/Parquet_dataset2', particionado por ['quality', 'free sulfur dioxide']\nArchivos en el directorio /FileStore/tables/Parquet_dataset2:\ndbfs:/FileStore/tables/Parquet_dataset2/_SUCCESS\ndbfs:/FileStore/tables/Parquet_dataset2/quality=3/\ndbfs:/FileStore/tables/Parquet_dataset2/quality=4/\ndbfs:/FileStore/tables/Parquet_dataset2/quality=5/\ndbfs:/FileStore/tables/Parquet_dataset2/quality=6/\ndbfs:/FileStore/tables/Parquet_dataset2/quality=7/\ndbfs:/FileStore/tables/Parquet_dataset2/quality=8/\ndbfs:/FileStore/tables/Parquet_dataset2/quality=9/\n\nArchivos en el directorio /FileStore/tables/Parquet_dataset1/quality=3/free sulfur dioxide=5.0:\ndbfs:/FileStore/tables/Parquet_dataset1/quality=3/free sulfur dioxide=5.0/_SUCCESS\ndbfs:/FileStore/tables/Parquet_dataset1/quality=3/free sulfur dioxide=5.0/_committed_6696874080369009984\ndbfs:/FileStore/tables/Parquet_dataset1/quality=3/free sulfur dioxide=5.0/_started_6696874080369009984\ndbfs:/FileStore/tables/Parquet_dataset1/quality=3/free sulfur dioxide=5.0/part-00000-tid-6696874080369009984-489cec7c-59a8-43a4-98f2-cf0217c0815d-1585-2.c000.snappy.parquet\n\nArchivos en el directorio /FileStore/tables/Parquet_dataset2/quality=3/free sulfur dioxide=5.0:\ndbfs:/FileStore/tables/Parquet_dataset2/quality=3/free sulfur dioxide=5.0/_SUCCESS\ndbfs:/FileStore/tables/Parquet_dataset2/quality=3/free sulfur dioxide=5.0/_committed_9189345811591723571\ndbfs:/FileStore/tables/Parquet_dataset2/quality=3/free sulfur dioxide=5.0/_started_9189345811591723571\ndbfs:/FileStore/tables/Parquet_dataset2/quality=3/free sulfur dioxide=5.0/part-00000-tid-9189345811591723571-997f133d-dfe0-41ae-a486-846c44b977cd-1586-1.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "parquetPath1  = f\"{saveDirectory}/Parquet_dataset1\"\n",
    "parquetPath2  = f\"{saveDirectory}/Parquet_dataset2\"\n",
    "\n",
    "partition_columns_parquet_1 = save_dataframe_as_parquet_format(red_wine_df, parquetPath1, partition_columns)\n",
    "# listar los archivos del directorio creado\n",
    "list_files_in_directory_path(parquetPath1)\n",
    "print()\n",
    "\n",
    "partition_columns_parquet_2 = save_dataframe_as_parquet_format(white_wine_df, parquetPath2, partition_columns)\n",
    "list_files_in_directory_path(parquetPath2)\n",
    "print()\n",
    "\n",
    "list_files_in_directory_path(f'{parquetPath1}/{partition_columns_parquet_1[0]}={partition_column_1_value}/{partition_columns_parquet_1[1]}={partition_column_2_value}')\n",
    "\n",
    "print()\n",
    "list_files_in_directory_path(f'{parquetPath2}/{partition_columns_parquet_2[0]}={partition_column_1_value}/{partition_columns_parquet_2[1]}={partition_column_2_value}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee5659b2-05e4-4f07-a024-53b849577c3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### GUARDA EL DATAFRAME COMO TABLA EN FORMATO DELTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "864d2cc9-9864-4bd3-b5d6-6cc4a3e1ff22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La base de datos 'db_bronze' ha sido creada exitosamente (o ya existía).\n"
     ]
    }
   ],
   "source": [
    "create_database(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dd2597a-062e-44ae-a2a8-64b6552e21c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los nombres de las columnas han sido estandarizados exitosamente. Las columnas ahora usan solo caracteres alfanuméricos y guiones bajos.\nLos nombres de las columnas han sido estandarizados exitosamente. Las columnas ahora usan solo caracteres alfanuméricos y guiones bajos.\nEl DataFrame ha sido guardado exitosamente como tabla Delta en la base de datos: db_bronze, tabla: dataset1_delta, particionada por ['quality', 'free_sulfur_dioxide']\n\nLos nombres de las columnas han sido estandarizados exitosamente. Las columnas ahora usan solo caracteres alfanuméricos y guiones bajos.\nLos nombres de las columnas han sido estandarizados exitosamente. Las columnas ahora usan solo caracteres alfanuméricos y guiones bajos.\nEl DataFrame ha sido guardado exitosamente como tabla Delta en la base de datos: db_bronze, tabla: dataset2_delta, particionada por ['quality', 'free_sulfur_dioxide']\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>database</th><th>tableName</th><th>isTemporary</th></tr></thead><tbody><tr><td>db_bronze</td><td>dataset1_delta</td><td>false</td></tr><tr><td>db_bronze</td><td>dataset2_delta</td><td>false</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "db_bronze",
         "dataset1_delta",
         false
        ],
        [
         "db_bronze",
         "dataset2_delta",
         false
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "database",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tableName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isTemporary",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tabla_dataset1 = f'dataset1_delta'\n",
    "tableParquetPath1  = f\"{saveDirectory}/{tabla_dataset1}\"\n",
    "\n",
    "tabla_dataset2 = f'dataset2_delta'\n",
    "tableParquetPath2  = f\"{saveDirectory}/{tabla_dataset2}\"\n",
    "\n",
    "save_dataframe_as_delta_table(red_wine_df, database, tabla_dataset1, partition_columns)\n",
    "print()\n",
    "save_dataframe_as_delta_table(white_wine_df, database, tabla_dataset2, partition_columns)\n",
    "\n",
    "print()\n",
    "list_tables_in_database(database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f07c50b-91ba-4b09-a15e-93d7723d17ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### GUARDA EL DATAFRAME COMO TABLA EN FORMATO PARQUET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50fc2bc9-3e8b-4b46-b67d-5ddce3a10b64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El DataFrame ha sido guardado exitosamente como tabla Parquet en la base de datos: db_bronze, tabla: dataset1_parquet, particionada por ['quality', 'free sulfur dioxide']\n\nEl DataFrame ha sido guardado exitosamente como tabla Parquet en la base de datos: db_bronze, tabla: dataset2_parquet, particionada por ['quality', 'free sulfur dioxide']\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>database</th><th>tableName</th><th>isTemporary</th></tr></thead><tbody><tr><td>db_bronze</td><td>dataset1_delta</td><td>false</td></tr><tr><td>db_bronze</td><td>dataset1_parquet</td><td>false</td></tr><tr><td>db_bronze</td><td>dataset2_delta</td><td>false</td></tr><tr><td>db_bronze</td><td>dataset2_parquet</td><td>false</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "db_bronze",
         "dataset1_delta",
         false
        ],
        [
         "db_bronze",
         "dataset1_parquet",
         false
        ],
        [
         "db_bronze",
         "dataset2_delta",
         false
        ],
        [
         "db_bronze",
         "dataset2_parquet",
         false
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "database",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tableName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isTemporary",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tabla_dataset1 = f'dataset1_parquet'\n",
    "tableParquetPath1  = f\"{saveDirectory}/{tabla_dataset1}\"\n",
    "\n",
    "tabla_dataset2 = f'dataset2_parquet'\n",
    "tableParquetPath2  = f\"{saveDirectory}/{tabla_dataset2}\"\n",
    "\n",
    "save_dataframe_as_parquet_table(red_wine_df, database, tabla_dataset1, partition_columns)\n",
    "print()\n",
    "save_dataframe_as_parquet_table(white_wine_df, database, tabla_dataset2, partition_columns)\n",
    "\n",
    "print()\n",
    "list_tables_in_database(database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1fd3e08-128b-48b7-99d7-e27c02a28410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## GUARDAR EL DATAFRAME COMO TABLAS TEMPORALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a8e20f6-a646-491b-bf16-8bbbfc0c55cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El DataFrame ha sido guardado exitosamente como tabla temporal: dataset1_temp\n\nEl DataFrame ha sido guardado exitosamente como tabla temporal: dataset2_temp\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Table</th><th>Type</th></tr></thead><tbody><tr><td>dataset1_temp</td><td>Local Temporary</td></tr><tr><td>dataset2_temp</td><td>Local Temporary</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dataset1_temp",
         "Local Temporary"
        ],
        [
         "dataset2_temp",
         "Local Temporary"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Table",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Type",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Table</th><th>Type</th></tr></thead><tbody><tr><td>global_temp.dataset1_temp</td><td>Global Temporary</td></tr><tr><td>global_temp.dataset2_temp</td><td>Global Temporary</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "global_temp.dataset1_temp",
         "Global Temporary"
        ],
        [
         "global_temp.dataset2_temp",
         "Global Temporary"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Table",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Type",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>database</th><th>tableName</th><th>isTemporary</th></tr></thead><tbody><tr><td>db_bronze</td><td>dataset1_delta</td><td>false</td></tr><tr><td>db_bronze</td><td>dataset1_parquet</td><td>false</td></tr><tr><td>db_bronze</td><td>dataset2_delta</td><td>false</td></tr><tr><td>db_bronze</td><td>dataset2_parquet</td><td>false</td></tr><tr><td></td><td>dataset1_temp</td><td>true</td></tr><tr><td></td><td>dataset2_temp</td><td>true</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "db_bronze",
         "dataset1_delta",
         false
        ],
        [
         "db_bronze",
         "dataset1_parquet",
         false
        ],
        [
         "db_bronze",
         "dataset2_delta",
         false
        ],
        [
         "db_bronze",
         "dataset2_parquet",
         false
        ],
        [
         "",
         "dataset1_temp",
         true
        ],
        [
         "",
         "dataset2_temp",
         true
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "database",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tableName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isTemporary",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tabla_dataset1 = f'dataset1_temp'\n",
    "tableParquetPath1  = f\"{saveDirectory}/{tabla_dataset1}\"\n",
    "\n",
    "tabla_dataset2 = f'dataset2_temp'\n",
    "tableParquetPath2  = f\"{saveDirectory}/{tabla_dataset2}\"\n",
    "\n",
    "save_dataframe_as_temp_table(red_wine_df, tabla_dataset1, partition_columns)\n",
    "print()\n",
    "save_dataframe_as_temp_table(white_wine_df, tabla_dataset2, partition_columns)\n",
    "\n",
    "print()\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# Listar tablas temporales locales\n",
    "list_temp_tables()\n",
    "\n",
    "# Listar tablas temporales globales\n",
    "list_temp_tables(show_global=True)\n",
    "\n",
    "list_tables_in_database(database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "851c25b4-8df2-4a99-acf1-399e6e433cd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ELIMINAS LOS PARAMETROS DEL NOTEBOOK Y LOS ARCHIVOS CREADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5acbe92-1244-4fcd-8e03-ce89fcb46061",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La base de datos 'db_bronze' ha sido eliminada exitosamente (si existía).\nEliminando archivos en el directorio /FileStore/tables...\n\nArchivo eliminado: dbfs:/FileStore/tables/Delta_dataset1/\nArchivo eliminado: dbfs:/FileStore/tables/Delta_dataset2/\nArchivo eliminado: dbfs:/FileStore/tables/Parquet_dataset1/\nArchivo eliminado: dbfs:/FileStore/tables/Parquet_dataset2/\nArchivo eliminado: dbfs:/FileStore/tables/winequality-red.csv\nArchivo eliminado: dbfs:/FileStore/tables/winequality-white.csv\n\nTodos los archivos en el directorio /FileStore/tables han sido eliminados.\n\nTodos los widgets han sido eliminados.\n"
     ]
    }
   ],
   "source": [
    "database = dbutils.widgets.get(\"bronze\")\n",
    "drop_database(database)\n",
    "\n",
    "delete_all_files_in_directory_path(saveDirectory)\n",
    "remove_assign_notebook_parameter_value_from_widget()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "1_Bronze",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
